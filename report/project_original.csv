,LFB-AVG-test-CosineSimilarity,LFB-AVG-test-CosineSimilarity/dataloader_idx_1,LFB-AVG-test-MeanAbsoluteError,LFB-AVG-test-MeanAbsoluteError/dataloader_idx_1,LFB-AVG-test-MeanSquaredError,LFB-AVG-test-MeanSquaredError/dataloader_idx_1,LFB-AVG-test-behavior-MulticlassAUROC,LFB-AVG-test-behavior-MulticlassAccuracy,LFB-AVG-test-behavior-MulticlassF1Score,LFB-AVG-test-label-CosineSimilarity,LFB-AVG-test-label-MeanAbsoluteError,LFB-AVG-test-label-MeanSquaredError,LFB-LAST-test-CosineSimilarity,LFB-LAST-test-CosineSimilarity/dataloader_idx_1,LFB-LAST-test-MeanAbsoluteError,LFB-LAST-test-MeanAbsoluteError/dataloader_idx_1,LFB-LAST-test-MeanSquaredError,LFB-LAST-test-MeanSquaredError/dataloader_idx_1,LFB-LAST-test-behavior-MulticlassAUROC,LFB-LAST-test-behavior-MulticlassAccuracy,LFB-LAST-test-behavior-MulticlassF1Score,LFB-LAST-test-label-CosineSimilarity,LFB-LAST-test-label-MeanAbsoluteError,LFB-LAST-test-label-MeanSquaredError,LFB-LAST-validation-MeanSquaredError,LFB-SENTENCES-test-behavior-MulticlassAUROC,LFB-SENTENCES-test-behavior-MulticlassAccuracy,LFB-SENTENCES-test-behavior-MulticlassF1Score,LFB-SENTENCES-test-label-CosineSimilarity,LFB-SENTENCES-test-label-MeanAbsoluteError,LFB-SENTENCES-test-label-MeanSquaredError,_runtime,_step,_timestamp,_wandb,artifactspath,batch_size,batches_per_epoch,bs_encoder.layer.10.output.dense,bs_encoder.layer.11.output.dense,bs_encoder.layer.12.output.dense,bs_encoder.layer.13.output.dense,bs_encoder.layer.14.output.dense,bs_encoder.layer.15.output.dense,bs_encoder.layer.16.output.dense,bs_encoder.layer.17.output.dense,bs_encoder.layer.18.output.dense,bs_encoder.layer.19.output.dense,bs_encoder.layer.2.output.dense,bs_encoder.layer.20.output.dense,bs_encoder.layer.21.output.dense,bs_encoder.layer.22.output.dense,bs_encoder.layer.23.output.dense,bs_encoder.layer.3.output.dense,bs_encoder.layer.4.output.dense,bs_encoder.layer.5.output.dense,bs_encoder.layer.6.output.dense,bs_encoder.layer.7.output.dense,bs_encoder.layer.8.output.dense,bs_encoder.layer.9.output.dense,bs_hidden_state_0,bs_hidden_state_1,bs_hidden_state_10,bs_hidden_state_11,bs_hidden_state_12,bs_hidden_state_2,bs_hidden_state_3,bs_hidden_state_4,bs_hidden_state_5,bs_hidden_state_6,bs_hidden_state_7,bs_hidden_state_8,bs_hidden_state_9,bs_hs_0,bs_hs_1,bs_hs_10,bs_hs_11,bs_hs_12,bs_hs_13,bs_hs_14,bs_hs_15,bs_hs_16,bs_hs_17,bs_hs_18,bs_hs_19,bs_hs_2,bs_hs_20,bs_hs_21,bs_hs_22,bs_hs_23,bs_hs_24,bs_hs_3,bs_hs_4,bs_hs_5,bs_hs_6,bs_hs_7,bs_hs_8,bs_hs_9,checkpoint,checkpoint_path,checkpointing,cod_hidden_state_0,cod_hidden_state_1,cod_hidden_state_10,cod_hidden_state_11,cod_hidden_state_12,cod_hidden_state_2,cod_hidden_state_3,cod_hidden_state_4,cod_hidden_state_5,cod_hidden_state_6,cod_hidden_state_7,cod_hidden_state_8,cod_hidden_state_9,cod_hs_0,cod_hs_1,cod_hs_10,cod_hs_11,cod_hs_12,cod_hs_13,cod_hs_14,cod_hs_15,cod_hs_16,cod_hs_17,cod_hs_18,cod_hs_19,cod_hs_2,cod_hs_20,cod_hs_21,cod_hs_22,cod_hs_23,cod_hs_24,cod_hs_3,cod_hs_4,cod_hs_5,cod_hs_6,cod_hs_7,cod_hs_8,cod_hs_9,commonsense-test-MulticlassAUROC,commonsense-test-MulticlassAUROC/dataloader_idx_0,commonsense-test-MulticlassAccuracy,commonsense-test-MulticlassAccuracy/dataloader_idx_0,commonsense-test-MulticlassF1Score,commonsense-test-label-MulticlassAUROC,commonsense-test-label-MulticlassAccuracy,commonsense-test-label-MulticlassF1Score,commonsense-validation-MulticlassAUROC,commonsense-validation-MulticlassAccuracy,commonsense-validation-MulticlassF1Score,commonsense-validation-label-MulticlassAUROC,commonsense-validation-label-MulticlassAccuracy,commonsense-validation-label-MulticlassF1Score,datapath,debug,ds1,ds1/enable,ds1/input_col,ds1/label_col,ds1/loss_fn,ds1/name,ds1/path,ds1/revision,ds1/test/batch_size,ds1/test/shuffle,ds1/test/slicing,ds1/train/batch_size,ds1/train/shuffle,ds1/train/slicing,ds1/validation/batch_size,ds1/validation/shuffle,ds1/validation/slicing,ds2,ds2/enable,ds2/input_col,ds2/label_col,ds2/loss_fn,ds2/name,ds2/path,ds2/revision,ds2/sampling_method,ds2/test,ds2/train/batch_size,ds2/train/shuffle,ds2/train/slicing,ds2/validation,early_stop_threshold,epoch,find_bs,find_learning_rate,find_lr,finetuned_path,last_checkpoint_path,loss_names,loss_weights,lr,lr-AdamW,lr-AdamW/pg1,lr-AdamW/pg2,lr-AdamW/pg3,lr-AdamW/pg4,model_path,name,num_epochs,num_samples_test,num_samples_train,num_workers,only_train_head,plc,plc/adamw/betas,plc/adamw/eps,plc/adamw/lr,plc/adamw/weight_decay,plc/before_lr_decay_warm_up_steps,plc/has_ReduceLROnPlateau,plc/has_learning_rate_decay,plc/lr_scheduler_frequency,plc/lr_scheduler_interval,plc/reduceLROnPlateau_config/cooldown,plc/reduceLROnPlateau_config/eps,plc/reduceLROnPlateau_config/factor,plc/reduceLROnPlateau_config/min_lr,plc/reduceLROnPlateau_config/patience,plc/reduceLROnPlateau_config/verbose,plc/regularization_coef,plc/regularize_from_init,plc/stepLR_gamma,plc/stepLR_step_size,plc/token_location,plc/train_all,pltc,pltc/accumulate_grad_batches,pltc/check_val_every_n_epoch,pltc/enable_checkpointing,pltc/limit_test_batches,pltc/limit_train_batches,pltc/limit_val_batches,pltc/log_every_n_steps,pltc/max_epochs,pltc/max_steps,pltc/max_time,pltc/min_epochs,pltc/min_steps,pltc/num_sanity_val_steps,pltc/overfit_batches,pltc/precision,pltc/val_check_interval,profiler,regularization_coef,regularize_from_init,sampling_method,shuffle_test,shuffle_train,test-LFB-AVG-mse/dataloader_idx_1,test-LFB-LAST-mse/dataloader_idx_1,test-MeanSquaredError,test-MeanSquaredError/dataloader_idx_1,test-MulticlassAccuracy,test-MulticlassAccuracy/dataloader_idx_0,test-commonsense-acc,test-commonsense-acc/dataloader_idx_0,test_acc,to_save_model,torch_float32_matmul_precision,train_datasets,train_loss,trainer/global_step,val-commonsense-acc,val_acc,validation-MeanSquaredError,validation-MulticlassAccuracy,validation-commonsense-acc
0,,,,,,,,,,,,,,,,,,,0.3940171301364898,0.4202279448509216,0.3790190815925598,0.1079355776309967,0.7586880922317505,0.9210708737373352,,,,,,,,10006.022001743317,335,1702411660.4963658,{'runtime': 10005},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.2421130097670904,0.25639066946675504,0.2529851014972261,0.3105193006297641,0.269989537864194,0.2899217557071819,0.28907349623860557,0.3305163181379219,0.34842437726676845,0.3485064814838209,0.3558675409833041,0.36108508260869904,0.17024452499841009,0.35012081176888576,0.33597508889206135,0.3426485803333757,0.33387508408614436,0.26588581089019286,0.09243843699342734,0.07551135963136693,0.03396754984722596,0.03752111127334352,0.0788466692838245,0.14512443198592398,0.1587318588482134,,,,,,,,,,,,,,,,,-0.3950316574988877,-0.3256548751331654,-2.543717865987167,-0.7342483679009875,-0.1401350095788212,-0.18651668970682425,-0.4298580956668336,0.3403161767135491,-0.7298951653397703,-0.33664837174990825,-0.0396967613260788,-1.0949866732650797,-0.5664583648050228,-0.2931657939898813,-0.3285664223268463,-0.4014332005559198,-0.6684495440062896,-0.493275911056557,0.05985693062250452,-0.024748684460486725,0.20098074144474964,-0.2546090747838603,-0.498817333246482,-0.22529299790450177,0.02116952344947032,,,,,,0.700333297252655,0.6184999942779541,0.5576567053794861,,,,0.9250667095184326,0.8542667627334595,0.8496127724647522,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/bert-large-cased_fmri_and_ethics_hm_23-12-12_1720.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,bert-large-cased,bert-large-cased_fmri_and_ethics_hm_23-12-12_1720,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 7, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.019003815948963165,0.0,,,,,
1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5858.236626148224,2,1702400482.0149765,{'runtime': 5857},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.24279931329387,0.2591161357850304,0.2568881953811933,0.33114626356779664,0.2789337080456613,0.28375964655779995,0.27173450238597013,0.31222664052522736,0.2887635536885729,0.30410371538379066,0.2351865853075281,0.1923342366840926,0.17336376883301466,0.14218485599529068,0.20112641109036575,0.28315164641780227,0.3153952448008379,0.038970951122187514,0.12177981003509142,0.07823958011788648,0.03776198909713527,0.04668976987273687,0.07429197524347217,0.14410159728374194,0.17559633167066363,,,,,,,,,,,,,,,,,-0.39748883615383934,-0.31679234038291826,-2.514962440248587,-0.7260417557551391,-0.1448579931122307,-0.1863774204786723,-0.42868443478503426,0.33964135753110347,-0.7141308569305342,-0.3350002617634773,-0.0648198164909426,-1.0992254699907198,-0.5560083548964507,-0.2840333701928739,-0.33835711483989983,-0.39609443006976663,-0.6662824594773027,-0.4614021005325244,0.07113933410721474,-0.003797876020063873,0.20346346102944235,-0.2554459141877159,-0.5009545147742331,-0.2104741737944007,0.022219698021617384,,,,,,0.4886668622493744,0.5,0.3397129774093628,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/bert-large-cased_no_F_T_23-12-12_1523.ckpt,,,,,,,,,bert-large-cased,bert-large-cased_no_F_T_23-12-12_1523,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 0, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,,0.0,,,,,
2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4489.985455274582,170,1701451516.4453044,{'runtime': 4488},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.024955477046882647,-0.09124090995901755,0.13442647815302666,0.26917771724122547,0.27629254105172174,0.289027724587929,0.2559081726043197,0.1599662246549829,0.2402230804747393,0.2431764233954629,0.2204798860321432,0.3181671135333892,-0.05587824056965319,0.147456277539744,0.20483079137694393,0.18166263104249103,0.2197784401709918,0.10932080836698764,0.032078535413566166,0.03802156969860561,0.06560271228324119,0.12082659305807524,0.018222424876267095,-0.0734666589287589,0.043017488556069544,,'artifacts/roberta-large_fmri-hm_then_ethics-hm_23-12-01_1542.ckpt',,,,,,,,,,,,,,,-1.615658966710653,-2.2081395290679677,-1.1619817550438745,-1.308580228900031,-0.7514451691985655,-0.08396842592302489,0.20127294757046033,-0.22835438907330283,-1.1463726489614945,-0.7294714885735973,-0.587060805720143,-1.4021788121795682,-1.112564281680947,-0.7764115988285483,-0.04473019594039651,-0.31213736905716827,-0.7628998776467213,-0.5816572503554724,-0.5203527599604985,-0.3502813412553132,-0.42223017524204454,-0.37124398211999754,-1.7294433950155137,-4.115644149108448,-0.7129851137768122,,,,,,0.8342000246047974,0.6956475973129272,0.6695106029510498,,,,0.9756000638008118,0.898933470249176,0.8955197334289551,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-SENTENCES', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-12-01_1610.ckpt,,,,,1e-05,1e-05,,,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-12-01_1610,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.04220223426818848,0.0,,,,,
3,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,0.47983869910240173,0.4924730956554413,0.060477521270513535,0.7902217507362366,1.003330111503601,1636.7356843948364,86,1701447007.5333154,{'runtime': 1636},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.023732299877007857,-0.09279933674212258,0.10489518917857012,0.20594650227135708,0.1941915730178515,0.21657663126472235,0.20273252065749545,0.07969642145074173,0.1382358837988911,0.20657756180773973,0.20370876098627744,0.20857693182835496,-0.07932609209620568,0.20446131207447824,0.23833821184698356,0.2974762125773903,0.1887007800104284,0.19809346365552735,0.020324369951413936,0.022034259678772223,0.04081684794301686,0.06566431346094656,0.07370898523506875,0.04258838561310942,0.08335291194925763,,'',,,,,,,,,,,,,,,-1.6142067254268555,-2.2087372654566244,-1.160225966882595,-1.309122917517599,-0.7513719502664689,-0.08098155921032313,0.20684847675928095,-0.2252516375373239,-1.146922344698628,-0.7303114141515865,-0.5824315895803349,-1.4047781673833248,-1.11514838300806,-0.7757507339873393,-0.04533261508981057,-0.3125809718799375,-0.762521720630823,-0.578723560651033,-0.519530819031093,-0.3507109897175902,-0.4212969705079135,-0.3717597481388175,-1.7297857024231271,-4.117548847622819,-0.7104473407881409,,,,,,0.5156666040420532,0.5011667013168335,0.42678070068359375,,,,0.5196333527565002,0.510200023651123,0.48447924852371216,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-SENTENCES', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-12-01_1542.ckpt,,,,,9.70299e-06,9.70299e-06,9.70299e-06,9.70299e-06,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-12-01_1542,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,2.3126907348632812,0.0,,,,,
4,,,,,,,,,,,,,,,,,,,,,,,,,,0.20801274478435516,0.354166716337204,0.3642039895057678,0.05449853092432022,0.7944301962852478,1.013938069343567,1508.1090154647827,42,1701439331.0709984,{'runtime': 1507},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.02375475514082896,-0.0967884338261282,0.11621408556743516,0.2262160131725716,0.18664173827310773,0.2285471950491215,0.17725455306823548,0.10408417590285976,0.09332949724527374,0.10082935314311628,0.1464177950826804,0.213947425359324,-0.07417677893230389,0.20212348677004577,0.24723587956872264,0.3317453928845777,0.29628544567255743,0.19793863142157192,0.029925516704578676,0.07874804547489078,0.033772767847635096,0.06040796898103269,0.017696047753393118,0.04576358971592141,0.06234909754144157,,'',,,,,,,,,,,,,,,-1.6141922926612584,-2.208675137943916,-1.1602203304647882,-1.3091162509412797,-0.7513269964043743,-0.0809451397435379,0.2068336231442788,-0.2252515702895861,-1.1468785389094929,-0.7302673601023266,-0.5823837489767203,-1.4046290577690923,-1.115081262006846,-0.7757650550873245,-0.04538808371218006,-0.31259323106661463,-0.7625378875008781,-0.5787328883980505,-0.5195336447331318,-0.3507110799839379,-0.4212973451829194,-0.37169066512535576,-1.7297272346983052,-4.117488192709129,-0.7103917510161382,,,,,,0.5262476801872253,0.5,0.3387535810470581,,,,0.4862410128116608,0.5,0.346112459897995,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-SENTENCES', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-12-01_1336.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-12-01_1336,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,1.9734681844711304,0.0,,,,,
5,,,,,,,,,,,,,,,,,,,,,,,,,,0.22868584096431732,0.5798609852790833,0.5624273419380188,0.08425646275281906,0.7808749675750732,0.9804503321647644,4050.0633239746094,149,1701437800.06374,{'runtime': 4049},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.023718289485303704,-0.0943992096675951,0.06596098992867198,0.16816928547821616,0.125452682986317,0.1932837580083407,0.20910997297535092,0.10913823432567232,0.13746513107493566,0.16010930118987207,0.1545684948924479,0.1811734316452531,-0.08865191920675682,0.14434567662172468,0.12480889781089656,0.2811728287989788,0.2890125837548907,0.21476807080614804,0.023388748782851063,0.04413151771585705,0.07837121445378904,0.08027478642777632,-0.0376493427321856,-0.011369522946206672,0.011103552470859548,,'artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-12-01_1228.ckpt',,,,,,,,,,,,,,,-1.6141675059403284,-2.2087217966197357,-1.1601979891953116,-1.3090968091950406,-0.7513468938081718,-0.08097872795398287,0.2068882292566515,-0.22526129206408463,-1.1468785347086916,-0.7302237784430969,-0.5823781601310205,-1.4046945447225836,-1.1150627406319864,-0.7757227884486539,-0.04534445473641746,-0.31258778268954357,-0.7625092028897482,-0.5787081418222024,-0.5195310805232756,-0.3506869719422112,-0.421316476252213,-0.3717639743681431,-1.7297219567486115,-4.117443635055862,-0.7103849560345559,,,,,,0.46695712208747864,0.5,0.3387535810470581,,,,0.4242134988307953,0.5,0.346112459897995,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-SENTENCES', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-12-01_1229.ckpt,,,,,9.6059601e-06,9.6059601e-06,9.6059601e-06,9.6059601e-06,roberta-large,roberta-large_Ethics-HM_then_fmri-HM_23-12-01_1229,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,1.217678785324097,0.0,,,,,
6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,305.50096011161804,2,1701433644.428719,{'runtime': 304},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.023758446749979144,-0.0954509363345054,0.1861494716945706,0.27794434244195015,0.27507519023290794,0.27701097169859296,0.2359074933725226,0.15480247335095712,0.19991137058671055,0.2570323425134069,0.2648972220008895,0.2944609910639708,-0.06901272136797167,0.3005602127117843,0.28332534263067644,0.34768008834066916,0.3271764818520339,0.3156183588298693,0.026667596239173228,0.07397003013826574,0.042414461951431746,0.11427161470365484,0.07195046040141001,0.06782567885513649,0.09012823198020198,,'',,,,,,,,,,,,,,,-1.6141762967155375,-2.208688496349758,-1.1602012447167152,-1.3090860316451036,-0.7513279270933373,-0.0809850981946234,0.20688986248458377,-0.2252446926848939,-1.1468742216467225,-0.7302642215528072,-0.5824303283417003,-1.4046725130044866,-1.115065318033353,-0.7757431227743188,-0.04536919351488433,-0.3125869076469019,-0.7625283894815571,-0.5787265270701494,-0.5195292217875807,-0.3506918205372336,-0.4212630514226912,-0.3717076862235946,-1.7297101765735916,-4.117401606343931,-0.7103503548380625,,,,,,0.5089325904846191,0.5,0.3397129774093628,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-SENTENCES', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_no_F_T_23-12-01_1221.ckpt,,,,,,,,,roberta-large,roberta-large_no_F_T_23-12-01_1221,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 0, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,,0.0,,,,,
7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9445.414831638336,128,1701347746.934543,{'runtime': 9444},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.025038223193403614,-0.0867198072157645,0.1446783596310283,0.2154342792328944,0.20823507690407517,0.2735256294185125,0.21125893005286303,0.12173028911708168,0.2374113503693337,0.1508494586052872,0.17928958430528907,0.18537081112795872,-0.08555370211162744,0.2979069260918492,0.3049253581310251,0.29773525765134373,0.2316619578630318,0.24313347236890348,0.014454555561513163,0.07802486793655368,0.05357495651080854,0.08499757144016848,0.04048629646270024,-0.004119297948414415,0.11068230669973653,,'roberta-large_fmri-hm_then_ethics-hm_23-11-30_0803.ckpt',,,,,,,,,,,,,,,-1.6137991588252998,-2.2105649152519997,-1.1633221132527267,-1.3072053124469178,-0.7506934059274173,-0.0815336978452752,0.2105698956702281,-0.22562826031213112,-1.1461605762462814,-0.7287246698846321,-0.5795339992775199,-1.4004159655351804,-1.112912378374705,-0.7763086551006577,-0.0460053623878367,-0.31442186978772213,-0.7641950031672471,-0.5793314433395569,-0.5196612267409917,-0.34995338113089325,-0.42473818568167365,-0.37116118433337153,-1.7296387779181883,-4.119884264687217,-0.7106302108553775,,,,,,0.8093427419662476,0.7096120119094849,0.6986552476882935,,,,0.960145890712738,0.8971213698387146,0.8980017900466919,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-11-30_0958.ckpt,,,,,9.135172474836406e-06,9.135172474836406e-06,,,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-11-30_0958,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.13886567950248718,0.0,,,,,
8,,,,,,,0.6299339532852173,0.5115512013435364,0.4786321222782135,0.07419271767139435,0.3034901022911072,0.14858759939670563,,,,,,,,,,,,,,,,,,,,4172.8150017261505,31,1701335630.8362486,{'runtime': 4160},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.02352620947650728,-0.09504693167066004,0.1655072889598296,0.26045412633798076,0.2269781857070123,0.2733871259116047,0.1577627197395428,0.08411267893749663,0.1070385499194652,0.1253318977776656,0.07079533948476298,0.017123023539625557,-0.06484257711746427,0.13458719972829955,0.15971645938856915,0.2952054711169297,0.2261271562136516,0.2502059370923975,0.03616403186101172,0.1034501894400452,0.05529496190146898,0.09563250752036354,0.02229194951062375,0.0863857067401084,0.13451223966539394,,'',,,,,,,,,,,,,,,-1.6137802404984374,-2.20833515683897,-1.1603179118278075,-1.3095085627854872,-0.7515084976530655,-0.08100675296461302,0.20697315633029323,-0.2256744429387385,-1.146582269425226,-0.7296770510683059,-0.5819965268357385,-1.4068147315883115,-1.115535919591374,-0.7756267054271369,-0.04533295230157064,-0.31234885670115964,-0.7625363712608697,-0.5773782812030805,-0.5194183838390651,-0.3504062631339222,-0.42067051265956934,-0.3717247901156804,-1.7303657949811293,-4.118246524094325,-0.7100748940296528,,,,,,0.5097213983535767,0.5039839744567871,0.359251469373703,,,,0.5904950499534607,0.5053257942199707,0.3617262244224548,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 16}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 32}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 16}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-11-30_0803.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-11-30_0803,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 32, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.6856201887130737,0.0,,,,,
9,,,,,,,0.4188034236431122,0.5548432469367981,0.5344933271408081,0.10145725309848784,0.2896444797515869,0.13569477200508118,,,,,,,,,,,,,,,,,,,,2362.0007631778717,75,1701286427.059199,{'runtime': 2360},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.024281461302850256,-0.08824523565191723,0.2002760982836444,0.3068059793228878,0.2876850804818012,0.3198607867890674,0.2172281744637117,0.14249317512700604,0.2808706959676554,0.2475832446720299,0.24631435148126163,0.2427104153808241,-0.06877545608654281,0.27161889077246393,0.2060871872195903,0.3012864545825892,0.33234437360214564,0.32853395236675553,0.0333955904388126,0.06804497618096415,0.01471530315702599,0.05528542216095774,0.013314563194789467,0.12159078983968909,0.17315875970829078,,'artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-11-29_1732.ckpt',,,,,,,,,,,,,,,-1.6143861748239066,-2.2085950186123076,-1.1606810783705956,-1.3085354596013483,-0.7517561022985153,-0.07959611977555037,0.20458677153524896,-0.22498512397188364,-1.1467335609092713,-0.7300724234024094,-0.5792006564870336,-1.405755193173816,-1.1150911586265773,-0.7754793551153032,-0.04391322968914135,-0.3128079714588017,-0.7623232537716356,-0.5799600176364685,-0.5205976351938844,-0.34982182169956455,-0.4223737132426766,-0.3720393355012283,-1.7306245041081143,-4.118143446440789,-0.7121649108138774,,,,,,0.5071666836738586,0.49799999594688416,0.3368763029575348,,,,0.5181666612625122,0.5,0.34515097737312317,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-11-29_1854.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_Ethics-HM_then_fmri-HM_23-11-29_1854,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.957387149333954,0.0,,,,,
10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4875.8650941848755,249,1701284027.5457172,{'runtime': 4874},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0243776829025623,-0.09416781091321816,0.15198321342615745,0.23041547786900377,0.23519397085239271,0.24459977737969452,0.2335372939411562,0.17434281297122492,0.22171779815439344,0.2552266838430074,0.23588095913631893,0.14363909121786442,-0.04022025430790278,0.2778576856804031,0.1694104767354765,0.17791045244616355,0.16578163644614813,0.06340963803055748,0.014268139643372311,0.0341438185067964,0.06188403845196909,0.11461318904293497,0.07093851214105033,0.08091726318020818,0.1367950572173882,,'',,,,,,,,,,,,,,,-1.6173550153022809,-2.211073157740737,-1.1613764141758267,-1.308460050112357,-0.7518046285605742,-0.07889315197850011,0.20693330654462772,-0.22376788559361804,-1.1456253616791194,-0.7283621284226525,-0.589274095622321,-1.4033884515831918,-1.1139072196363524,-0.7760608971443157,-0.04607941413062333,-0.31251890243351976,-0.7635214418456013,-0.5813652700193612,-0.5209220966723991,-0.34974343695468435,-0.4240074724305314,-0.37343583729661933,-1.727313775526671,-4.115557859472046,-0.7178036027814216,,,,,,0.8348143100738525,0.7160476446151733,0.6955580115318298,,,,0.9630333185195924,0.9016667604446412,0.8989951014518738,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-11-29_1732.ckpt,,,,,9.135172474836406e-06,9.135172474836406e-06,,,roberta-large,roberta-large_Ethics-HM_then_fmri-HM_23-11-29_1732,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.0016911027487367392,0.0,,,,,
11,,,,,,,0.4051282107830048,0.5292022824287415,0.5033169984817505,0.0890001580119133,0.30421948432922363,0.14927127957344055,,,,,,,,,,,,,,,,,,,,6967.401952505112,732,1701279112.8575604,{'runtime': 6965},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.020950190329292927,-0.1115541129128846,0.0433387138217243,0.16756754981493624,0.19908738535609172,0.20767550077921657,0.1983059020096092,0.04646698138393907,0.20745424305441337,0.21473821139744065,0.1980126259795521,0.08272869352343665,-0.024265919330811275,-0.000830714846609126,0.06925288570654166,0.08557168005031275,0.06092701889146541,0.05647480652623666,0.05574203886397855,0.026836258180000436,0.045729711369094685,0.05236464908915249,-0.031494441218406444,-0.053272648209996845,-0.004813362228477063,,'',,,,,,,,,,,,,,,-1.6104584821998205,-2.2089846048937014,-1.1612251585290831,-1.3123751044498273,-0.7531842589485851,-0.08662014130382167,0.2204520402015253,-0.23115461993276895,-1.1481447364272772,-0.7302826573732193,-0.5841165247200053,-1.4087903055358404,-1.1208688381429543,-0.7768601752615243,-0.04633601155847522,-0.3146052209449355,-0.7661061464602039,-0.5828347268093184,-0.5200516678539286,-0.3506675610854373,-0.4158261747634444,-0.37015052699950424,-1.726324595654321,-4.1145600645453,-0.70337596895935,,,,,,0.8622499704360962,0.7246666550636292,0.6832950711250305,,,,0.9723333716392516,0.9092668294906616,0.9053016901016236,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri_and_ethics_hm_23-11-29_1535.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_fmri_and_ethics_hm_23-11-29_1535,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,3.0278701160568744e-05,0.0,,,,,
12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5951.545321464539,372,1701266977.6834176,{'runtime': 5950},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.021488378645435652,-0.09811683205821407,0.04666208831968772,0.19662081275962,0.21617764721879829,0.2547328928076123,0.24814078626258235,0.2550280908312474,0.28179615806640157,0.3015488081751784,0.29786544992698694,0.269105875295053,-0.06494837646050869,0.3297045951553961,0.22583735430034524,0.1766832706410186,0.16394080215214551,0.10558558476120523,0.02634290936035265,0.06345023614613805,0.02033798957688949,0.08293686367219075,0.03521048646768797,0.021841740348029597,0.07754479624506379,,'artifacts/roberta-large_fmri-hm_then_ethics-hm_23-11-29_1153.ckpt',,,,,,,,,,,,,,,-1.6172510076228144,-2.2136238556576293,-1.1596428166286374,-1.3079343793215237,-0.7523706018750638,-0.08095425164645875,0.200345986747127,-0.22488901503646885,-1.1471598786563195,-0.7371514952043976,-0.5769477781631829,-1.4052358009639314,-1.1195920517507965,-0.7744830250698356,-0.04497050918969059,-0.3128136322406425,-0.7637735760922815,-0.5793409162958827,-0.5167464154223023,-0.35192145640988537,-0.4248372380257648,-0.3734972200166493,-1.7309636263980233,-4.125314387073975,-0.7144061816525586,,,,,,0.8368416428565979,0.7325142025947571,0.7138497829437256,,,,0.9667999744415284,0.9054000377655028,0.9029794931411744,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-11-29_1230.ckpt,,,,,7.856781408072185e-06,7.856781408072185e-06,,,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-11-29_1230,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.0001280225842492655,0.0,,,,,
13,,,,,,,,,,,,,,,,,,,0.4136752486228943,0.5160256624221802,0.4998779296875,0.14215008914470673,0.7474824786186218,0.8954382538795471,,,,,,,,2082.5503146648407,98,1701260990.8589876,{'runtime': 2081},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.023882352498994203,-0.098668734018178,0.15865198081778548,0.2261936398603926,0.22069145912771415,0.27346877075595116,0.21902832825797336,0.15297026123857296,0.15523935087785218,0.23353462601989905,0.23380412634518688,0.246876948880657,-0.10097785450893114,0.2040663074350334,0.22054699997737265,0.30150208106514265,0.27447716830011565,0.117690233741433,0.019824792238479123,0.0980062616181542,0.03951183376173551,0.03988372456641581,0.016760944600272422,0.1298310938952289,0.16375112202983105,,'',,,,,,,,,,,,,,,-1.6130164885800302,-2.205312849014727,-1.1604888977094228,-1.3102145516306964,-0.7512392566381334,-0.07961317103756382,0.20746362172011457,-0.2255777396477554,-1.147131040203889,-0.7303602279292778,-0.5822100584210286,-1.4047000780915735,-1.1124566315116242,-0.775699161830214,-0.047283272513382046,-0.31406733195821857,-0.7622602527440436,-0.5775951903311802,-0.518604445769165,-0.3495723982287726,-0.42130002512485154,-0.3694955601507166,-1.73093864921968,-4.11364668996964,-0.705561542667791,,,,,,0.5223333835601807,0.4943332970142365,0.35413339734077454,,,,0.5380333065986633,0.4980667233467102,0.3741626441478729,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-11-29_1153.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-11-29_1153,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,1.343380331993103,0.0,,,,,
14,,,,,,,,,,,,,,,,,,,0.3999999761581421,0.47970086336135864,0.46286123991012573,0.13848745822906494,0.7500361204147339,0.9012256264686584,,,,,,,,1753.4649102687836,62,1701193974.9931812,{'runtime': 1753},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.025508958214229032,-0.09977916790642256,0.017062934891813354,0.2042447263650054,0.1828313356083502,0.22442145758264845,0.17879064794817234,0.1246341401044158,0.23209196898016757,0.27628291224987445,0.24410833874799603,0.2397099323847767,-0.07321454953592475,0.13789705235665684,0.07427598475301656,0.06596001395229271,0.01238488574829646,-0.01294345584888037,0.04464189682742304,0.07412695926413786,0.05988835700496105,0.03762295138406101,-0.10245749817356868,-0.02400700871258467,0.023421585777995132,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-11-28_1428.ckpt,,,,,,,,,,,,,,,-1.6136909515282647,-2.2109154759703338,-1.160421390472642,-1.3076289598687558,-0.7522969950223013,-0.0846838198843003,0.2114304242181999,-0.2280584762061344,-1.148281367569417,-0.7304341308541,-0.5890671678801258,-1.404511713904374,-1.113336285452,-0.7759070020953989,-0.04448279166856817,-0.3116280901214421,-0.7619934030259188,-0.5753083900616469,-0.5193969870245436,-0.35050936379341735,-0.42288869111808447,-0.37167713179699646,-1.729472972470612,-4.1183002854858985,-0.7056562401582704,,,,,,0.8480000495910645,0.7219999432563782,0.6737710237503052,,,,0.9706334471702576,0.851933479309082,0.8364242315292358,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-11-28_1723.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_Ethics-HM_then_fmri-HM_23-11-28_1723,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,1.9866490364074707,0.0,,,,,
15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8600.898707151413,282,1701190313.977652,{'runtime': 8600},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.025467245357828783,-0.09518259033689389,0.12224088606057044,0.2436812521171982,0.2493295995734161,0.2717253461726208,0.23718800271884916,0.13235613087268097,0.1949348882833324,0.2354591631468431,0.2093721088419898,0.26825515411476214,-0.06591452003904376,0.28516840214233863,0.19979326487242144,0.12612736779746192,0.10082271165050262,0.0984862513303337,0.04849198115490762,0.061591936163328184,0.04418316518429724,0.10758079468864588,0.041806169001162886,0.02051943065394328,0.13577052245183469,,'',,,,,,,,,,,,,,,-1.6155015012268057,-2.2133631372750853,-1.1607037618987115,-1.307172720296823,-0.7520348264140908,-0.0826749618874818,0.20982245697127863,-0.2270517457683685,-1.147173461652316,-0.7314136151264623,-0.5885013893553237,-1.403764660757572,-1.1132318700606527,-0.7754123229140395,-0.044852623188067486,-0.311901221366899,-0.7624054733403076,-0.5749790172170537,-0.5192084945835354,-0.3503567013342337,-0.42401067557358463,-0.3728128510891018,-1.7301222285541495,-4.119954793412913,-0.7077971391832869,,,,,,0.8364237546920776,0.7298855781555176,0.712294340133667,,,,0.9649999737739564,0.895466685295105,0.8918060064315796,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-11-28_1428.ckpt,,,,,8.86384871716129e-06,8.86384871716129e-06,,,roberta-large,roberta-large_Ethics-HM_then_fmri-HM_23-11-28_1428,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.0007071543368510902,0.0,,,,,
16,,,,,,,,,,,,,,,,,,,0.3970085680484772,0.4323361814022064,0.40704110264778137,0.09761830419301988,0.7632268071174622,0.9326249957084656,,,,,,,,6375.3203864097595,334,1701181680.8938644,{'runtime': 6374},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.02502078535482918,-0.08566842137677523,0.08792420210584188,0.18912825832482744,0.16497976077927878,0.17139197926287966,0.14830849313800692,0.17471895202983312,0.2610122692646254,0.18842470201641168,0.21339006558669,0.16888102352650278,-0.04786505170669018,0.19758687010114917,0.11629694537457776,0.07374540110740498,0.07986966881686446,0.1120793711438615,0.008920540778861246,0.05785549500487057,0.013620481915811632,0.02888941064933737,0.05415210251210791,0.007294215147884375,0.05921127970281653,,'',,,,,,,,,,,,,,,-1.6130375710893592,-2.208668459547233,-1.1607950409564345,-1.307300322476974,-0.7509513433778949,-0.08086614834938066,0.2024798032026952,-0.221756999095446,-1.1455942233398675,-0.7282291656037747,-0.5830061523398768,-1.4094657839185132,-1.1100675136687044,-0.7748610303696106,-0.04407442240909765,-0.3103236559181124,-0.7639360631121435,-0.5820542995471323,-0.5208692158067181,-0.35080534696458665,-0.4218128560348857,-0.3729211430119399,-1.7341944722011775,-4.121432537536837,-0.7053681253956579,,,,,,0.8383333683013916,0.6988332867622375,0.6524568200111389,,,,0.9744666814804076,0.9144001603126526,0.9114811420440674,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri_and_ethics_hm_23-11-28_1241.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_fmri_and_ethics_hm_23-11-28_1241,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 7, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.01904110051691532,0.0,,,,,
17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,604.2761371135712,2,1701175282.5862691,{'runtime': 603},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.023758445275169415,-0.09545093040937663,0.186149460139318,0.2779443251885177,0.2750751731575779,0.2770109545030996,0.2359074787285325,0.15480246374157394,0.19991135817718497,0.2570323265580894,0.26489720555735874,0.2944609727852645,-0.06901271708399653,0.3005601940544693,0.28332532504321867,0.3476800667583798,0.3271764615425109,0.3156183392378175,0.0266675945837807,0.07397002554656786,0.04241445931854974,0.11427160761022805,0.07195045593507461,0.06782567464484678,0.0901282263854783,,'',,,,,,,,,,,,,,,-1.6141762967154598,-2.208688496349857,-1.160201244716688,-1.309086031645109,-0.7513279270933013,-0.08098509819446242,0.20688986248455063,-0.2252446926848448,-1.1468742216467387,-0.7302642215528952,-0.5824303283417898,-1.404672513004657,-1.1150653180333774,-0.7757431227743457,-0.04536919351484081,-0.31258690764688146,-0.7625283894815937,-0.5787265270702067,-0.5195292217876233,-0.35069182053735903,-0.4212630514227238,-0.37170768622356865,-1.7297101765735905,-4.1174016063439,-0.7103503548380463,,,,,,0.4983928203582764,0.5,0.32459110021591187,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_no_F_T_23-11-28_1231.ckpt,,,,,,,,,roberta-large,roberta-large_no_F_T_23-11-28_1231,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 0, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,,0.0,,,,,
18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8135.478876590729,282,1700933924.4364617,{'runtime': 8134},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.024058389910354517,-0.09028035053609204,-0.03283621209750213,0.11482473364537292,0.16773216819483622,0.13407802585119502,-0.05194345824438363,0.04687387221627356,0.09691519834780064,0.14106199174996567,0.15225425731800568,0.11792431693676236,-0.06634241153059067,0.1453475300993094,0.13814509647632717,0.17242500073209752,0.18226070140729944,0.1954638013688874,0.021107864260545995,0.017221895455204198,-0.04900994362545656,-0.06640046859531264,-0.1478176876518308,-0.01625515443457992,0.02902333658043395,,artifacts/RoBERTa-transfer.ckpt,,,,,,,,,,,,,,,-1.6132857715972753,-2.2073069003353667,-1.161742461169344,-1.3087781507626457,-0.7516037631552006,-0.08203044556342332,0.20956446650777785,-0.22664270216385973,-1.150409320358671,-0.7298369130760376,-0.5812432920916797,-1.4052369228495762,-1.1125681317638,-0.7754066511785034,-0.046486589543339285,-0.31560065314968266,-0.7609915063767896,-0.5782169517801228,-0.5208718385311983,-0.3508902422563547,-0.4225533905964509,-0.36918955449196456,-1.7293299932384647,-4.1187762545934214,-0.7073981886329572,,,,,,0.8214333653450012,0.6837714314460754,0.6523687839508057,,,,0.9674666523933412,0.8958001732826233,0.8934198021888733,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,9.70299e-06,9.70299e-06,,,roberta-large,"roberta-large, fmri-HM then Ethics-HM 23-11-25 15:22",,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.01602882146835327,0.0,,,,,
19,,,,,,,0.40299150347709656,0.5477208495140076,0.5134513974189758,0.11061050742864607,0.2831067442893982,0.13001157343387604,,,,,,,,,,,,,,,,,,,,5389.620526790619,326,1700925766.4449048,{'runtime': 5388},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.02287329713663415,-0.08831058302006806,-0.061714181562403846,0.02058864620885983,0.03444357217860477,-0.006322623349518033,-0.0800567443527398,0.1177363467222041,0.15576138865567413,0.17621760569082046,0.17570220080118096,0.1685738481738062,-0.08198783743753003,0.07664303335641336,0.15074342787710074,0.2908523197005816,0.2973224635964698,0.31170707678231985,0.018101568000769844,0.012289820993159334,-0.02853187643853678,-0.12258081500488129,-0.17729098400930066,-0.008326755015720068,-0.10276867060315496,,,,,,,,,,,,,,,,,-1.615552080166088,-2.2094106782532212,-1.1608521021115386,-1.3091487521262395,-0.751651270493807,-0.07946677267053404,0.2071447289756971,-0.22582199439357753,-1.1481959958911157,-0.7294141202364635,-0.5830035017288631,-1.4054833204155948,-1.1162571428901198,-0.7757546783800533,-0.046063380903562434,-0.3133187356378979,-0.7616114828070866,-0.5776585780082504,-0.519890874263133,-0.3515981277759299,-0.4218771386918636,-0.37246875424640313,-1.730282483675914,-4.1206274204759445,-0.7113912338361752,,,,,,0.4923332929611206,0.5019999742507935,0.3244191110134125,,,,0.4635332524776459,0.5,0.3182210922241211,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/RoBERTa-transfer.ckpt,,,,,8.86384871716129e-06,8.86384871716129e-06,8.86384871716129e-06,8.86384871716129e-06,roberta-large,"roberta-large, fmri-HM then Ethics-HM 23-11-25 13:52",,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.7352885007858276,0.0,,,,,
20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4958.03945016861,170,1700920355.8528512,{'runtime': 4957},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.024546746200272147,-0.10217779714053402,0.15074687246049442,0.2141917500613097,0.19144290831503524,0.2308737300845273,0.08661897091608292,0.026282655945393137,0.06785860844164791,0.10960694329905032,0.03303102598560095,0.17927511896573003,-0.07653496967981643,0.17903558716333107,0.12162387477490431,0.2221822084101524,0.18507721928595852,0.23324106320314764,0.021022138799494917,0.05819692853879436,0.07274025661271417,0.06724307440536904,0.038218079447010674,0.004613331970680033,0.0957661801783622,,,,,,,,,,,,,,,,,-1.614143898441394,-2.207513932494668,-1.1603093632114767,-1.3090574288074337,-0.7512958992446606,-0.07988634330643518,0.206112457727751,-0.224394572300481,-1.146716432418336,-0.7316438603472295,-0.5775176680137653,-1.404650631538471,-1.1138963852751838,-0.7764377288059487,-0.043620826024139525,-0.31214666760617504,-0.7639888378919815,-0.5751825445504615,-0.5188402948456254,-0.3500535865498826,-0.4225979311047199,-0.3708369651766074,-1.7289388273139816,-4.119926664283553,-0.711271444204703,,,,,,0.8188997507095337,0.7050762176513672,0.6772086024284363,,,,0.9723333716392516,0.9055335521697998,0.9006666541099548,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1e-05,1e-05,,,roberta-large,"roberta-large, (fmri and Ethics)-HM 23-11-25 12:29",,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 7, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.010548795573413372,0.0,,,,,
21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,205.4416182041168,2,1700915379.8111825,{'runtime': 204},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.023758445275184444,-0.09545093040937572,0.18614946013931816,0.27794432518851725,0.27507517315757907,0.2770109545030995,0.235907478728533,0.15480246374157247,0.19991135817718553,0.25703232655808944,0.264897205557359,0.29446097278526395,-0.06901271708399585,0.30056019405446877,0.2833253250432164,0.347680066758381,0.32717646154251084,0.3156183392378172,0.026667594583783098,0.07397002554656824,0.042414459318546704,0.11427160761022456,0.07195045593507368,0.067825674644848,0.09012822638547938,,,,,,,,,,,,,,,,,-1.6141762967154722,-2.208688496349763,-1.1602012447169447,-1.3090860316451225,-0.7513279270934035,-0.08098509819456079,0.20688986248455976,-0.2252446926848404,-1.1468742216467045,-0.7302642215528798,-0.5824303283418766,-1.4046725130046558,-1.1150653180334968,-0.7757431227744456,-0.04536919351483659,-0.3125869076469667,-0.7625283894816008,-0.5787265270702058,-0.5195292217877676,-0.3506918205373417,-0.42126305142276754,-0.37170768622358286,-1.7297101765733576,-4.117401606344154,-0.7103503548380332,,,,,,0.4998047649860382,0.5,0.3387535810470581,,,,,,,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,,,,,roberta-large,"roberta-large, no f.t. 23-11-25 12:26",,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 0, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,,0.0,,,,,
22,,,,,,,,,,,,,,,,,,,0.30946823954582214,0.17722223699092865,0.05727272853255272,0.0062933932058513165,0.7821601629257202,0.9784178137779236,,,,,,,,88.39630174636841,2,1700913654.0220118,{'runtime': 87},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,0.16426980294569565,0.16919894980091035,0.3188825677051471,0.35143764129737803,0.06344482397926342,0.25342712156962294,0.30998930455871376,0.22631567782022244,0.24965002660572025,0.27220693974207855,0.3006321614167136,0.30443059698424374,0.30876336200523335,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-0.5076392920407058,-0.15572638993482668,-2.4042098405622947,-0.6706278910700758,-0.11118443429235292,-0.22474439189999407,-0.5676480698044735,0.09694556899872298,0.008549803385132115,-0.3058246719716593,-0.36004027561867025,-0.14560062154992104,-0.1556597675301945,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4823715090751648,0.48477381467819214,0.4626286029815674,,,,,,,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 20}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,,,,,bert-base-cased,scarlet-sponge-1047,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 20, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 0, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,
23,,,,,,,,,,,,,,,,,,,0.3782055974006653,0.04500000178813934,0.032499998807907104,1.4161840226734055e-05,0.7865431308746338,0.9879252910614014,,,,,,,,88.80608224868774,1,1700913457.924379,{'runtime': 87},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,0.16426980294569565,0.16919894980091035,0.3188825677051471,0.35143764129737803,0.06344482397926342,0.25342712156962294,0.30998930455871376,0.22631567782022244,0.24965002660572025,0.27220693974207855,0.3006321614167136,0.30443059698424374,0.30876336200523335,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4783024489879608,0.5,0.33936309814453125,,,,,,,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 20}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,,,,,bert-base-cased,stoic-totem-1046,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 20, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 0, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,
24,,,,,,,,,,,,,,,,,,,0.31396982073783875,0.2811904847621918,0.234062060713768,0.004305749665945768,0.7847902774810791,0.9849279522895812,,,,,,,,38.907726526260376,0,1700913319.7416644,{'runtime': 38},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.45722854137420654,0.4893214404582978,0.36561498045921326,,,,,,,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 20}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,,,,,bert-base-cased,treasured-music-1045,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 20, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 0, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,
25,,,,,,,,,,,,,,,,,,,0.5933674573898315,0.5063095092773438,0.45583027601242065,0.1190103441476822,0.7358659505844116,0.8705196380615234,,,,,,,,148.2829713821411,10,1700911962.5454245,{'runtime': 148},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5499786734580994,0.5228332877159119,0.4159770905971527,,,,0.7622441649436951,0.6032092571258545,0.5370250344276428,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 20}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,bert-base-cased,proud-mountain-1044,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 20, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 2, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0839014053344727,0.0,,,,,
26,,,,,,,,,,,,,,,,,,,0.3422069251537323,0.23782053589820865,0.21033114194869995,0.05533262714743614,0.7852998375892639,0.9855875372886658,,,,,,,,78.75860667228699,31,1700911759.0529256,{'runtime': 78},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5083190202713013,0.4895428717136383,0.4445077180862427,,,,0.4014137089252472,0.4166666865348816,0.3823953866958618,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,2e-05,2e-05,2e-05,2e-05,bert-base-cased,lively-field-1043,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.9551817178726196,0.0,,,,,
27,,,,,,,,,,,,,,,,,,,0.4640086889266968,0.30000001192092896,0.22908498346805573,0.15642298758029938,0.7275493144989014,0.8520244359970093,,,,,,,,304.15432596206665,50,1700911304.038074,{'runtime': 303},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5146931409835815,0.5,0.3401784896850586,,,,0.5985357165336609,0.5,0.3461896479129791,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,bert-base-cased,classic-voice-1039,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.3362205028533936,0.0,,,,,
28,,,,,,,,,,,,,,,,,,,0.32766982913017273,0.28333333134651184,0.20091620087623596,0.0698113664984703,0.756767988204956,0.9166369438171388,,,,,,,,2839.4242367744446,5,1698088059.8852549,{'runtime': 2839},/Users/ajmeek/PycharmProjects/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.46602457761764526,0.5,0.3401784896850586,,,,0.4753170907497406,0.5,0.3461896479129791,/Users/ajmeek/PycharmProjects/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,bert-base-cased,ancient-pond-1036,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.337463855743408,0.0,,,,,
29,,,,,,,,,,,,,,,,,,,0.5916516184806824,0.5816559195518494,0.5365456938743591,0.15183675289154053,0.7329474091529846,0.8621099591255188,,,,,,,,350.0743124485016,34,1697290535.6947825,{'runtime': 349},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5585340857505798,0.5348985195159912,0.49746301770210266,,,,0.7947016358375549,0.6993557810783386,0.6893270611763,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 16}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 32}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 16}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,True,,False,,,,,,,2e-05,2e-05,2e-05,2e-05,bert-base-cased,snowy-smoke-1035,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.05515718460083,0.0,,,,,
30,,,,,,,,,,,,,,,,,,,0.6095253825187683,0.564047634601593,0.5187015533447266,0.13867104053497314,0.7313676476478577,0.8605422973632812,,,,,,,,498.7980980873108,50,1697203255.86921,{'runtime': 498},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5579070448875427,0.5289405584335327,0.4789683222770691,,,,0.8022892475128174,0.6998220682144165,0.6864106059074402,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 20}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,bert-base-cased,fresh-snow-1026,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 20, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.8474228382110596,0.0,,,,,
31,,,,,,,,,,,,,,,,,,,0.4076556861400604,0.26517096161842346,0.22800663113594055,0.06357927620410919,0.7846009135246277,0.9845790266990662,,,,,,,,106.80224704742432,32,1697202629.877124,{'runtime': 106},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5404333472251892,0.5356476902961731,0.4899471700191498,,,,0.504092276096344,0.5595238208770752,0.5595238208770752,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,2e-05,2e-05,2e-05,2e-05,bert-base-cased,wise-fog-1024,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.9406831860542296,0.0,,,,,
32,,,,,,,,,,,,,,,,,,,0.45096665620803833,0.30000001192092896,0.22908498346805573,0.15546473860740662,0.7275786399841309,0.8523438572883606,,,,,,,,269.8547613620758,50,1697201493.3543394,{'runtime': 269},artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.499413400888443,0.5,0.3401784896850586,,,,0.594759464263916,0.5,0.3461896479129791,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,bert-base-cased,dauntless-vortex-1019,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.325981616973877,0.0,,,,,
33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5404.60195016861,168,1697051065.817655,{'runtime': 5404},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8312333226203918,,0.7154094576835632,,0.6913905143737793,,,,0.970566749572754,0.9126667976379396,0.91079843044281,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'RAW', 'path': 'data/ds000212/ds000212_raw', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,False,,,,,,,1e-05,1e-05,,,roberta-large,skilled-terrain-1006,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 2, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 7, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.004022954497486353,0.0,,,,,
34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,679.0230314731598,1,1696612689.5787654,{'runtime': 678},/workspace/brainbias/artifacts,,,0.33822836750724217,0.32224700381312604,0.329399054297205,0.30441479405720895,0.3263145761914081,0.32470209244488746,0.30206704083787933,0.33183122757720296,0.3467327519625064,0.32009392762316957,0.08614520224647636,0.29331375535833654,0.2773057406110295,0.14261947959324436,0.27320633329412286,0.12915958715515788,0.04091418683803475,0.26041771559123267,0.26274545910891606,0.1877742413171337,0.28789157357533973,0.2329861158216615,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5157905220985413,,0.5,,0.3387535810470581,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': None, 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': None, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,,,,,roberta-large,woven-violet-1004,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 0, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,,0.0,,,,,
35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,924.8469824790956,1,1696611976.0299523,{'runtime': 924},/workspace/brainbias/artifacts,,,0.35202212546345607,0.342565731588197,0.35327802285758836,0.3341860004006267,0.2820083932190051,0.3168689997873132,0.24805342478786951,0.19995898792124456,0.14559180127915175,-0.06489202047035979,0.028581542151774216,-0.011577390399807907,-0.09917863119188042,-0.04750174698921806,0.17873393015637923,0.06325171678151258,0.05439995261149916,0.06570203811546252,0.15336087071337917,0.2397298063186176,0.31115716072737293,0.26544655656284355,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5115381479263306,,0.5034667253494263,,0.37450265884399414,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': None, 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': None, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,,,,,bert-large-cased,fast-eon-1003,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 0, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,,0.0,,,,,
36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,271.4138514995575,1,1696611016.6895936,{'runtime': 270},/workspace/brainbias/artifacts,,,0.13014037858640976,0.13816996540377355,,,,,,,,,0.15866121999254723,,,,,0.3042837371344377,0.18674915292652283,0.28411062512730045,0.298900962581425,0.3318969753853771,0.22853201751213145,0.1307728334870761,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4859666526317597,,0.5,,0.32459110021591187,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': None, 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': None, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,,,,,bert-base-cased,crimson-hill-1002,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 0, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,,0.0,,,,,
37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2085.7484085559845,59,1696609613.9296725,{'runtime': 2085},/workspace/brainbias/artifacts,,,0.2959424330565122,0.31906242244699085,0.2777876938481649,0.26796907573200424,0.2333917744825067,0.20544696526653924,0.26394844085852565,0.13608101230659972,0.1915934778638068,0.10406957732537278,0.042772118724419,0.17668586652633334,0.14243814070862856,0.002378186799944243,0.07063149905871212,0.1260027156250806,0.04555667276742976,0.16211053203792505,0.17401584676792312,0.1414245415647379,0.24601742821513775,0.20652298086896417,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8399333953857422,,0.7414857149124146,,0.7256808280944824,,,,0.9554563164711,0.9003307819366455,0.8922266960144043,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1e-05,1e-05,,,roberta-large,robust-star-1001,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 7, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.1694328337907791,0.0,,,,,
38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6096.261821269989,221,1696607212.414961,{'runtime': 6123},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.960350513458252,0.9149471521377563,0.9054303169250488,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,False,,False,,,,,,,9.3206534790699e-06,9.3206534790699e-06,,,roberta-large,quiet-firebrand-1000,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 40, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.0005962108843959868,961.0,,,,,
39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5468.635464429855,165,1696601082.4493434,{'runtime': 5468},/workspace/brainbias/artifacts,,,0.3267608084970115,0.28853560171651915,0.2836891353904222,0.29359071179861496,0.2506708461833131,0.3398903126493725,0.3262187267830805,0.30645755610904496,0.33215347635575354,0.30872474080174533,0.11070740105544666,0.1484748459863028,0.28252656234873685,0.2458787542697849,0.370071213184112,0.10793322745080318,0.09717238065201876,0.16176464299533558,0.1238251552760398,0.16926125002446898,0.2780039425475231,0.20694082058002036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8282334208488464,,0.7071143388748169,,0.685374915599823,,,,0.958465576171875,0.9177249670028688,0.9107012152671814,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,7.936142836436552e-06,7.936142836436552e-06,,,roberta-large,stellar-pine-999,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.01164982095360756,0.0,,,,,
40,0.1749057024717331,,0.271648108959198,,0.12014944851398468,,,,,,,,,,,,,,,,,,,,,,,,,,,2212.9589269161224,43,1696594957.5931609,{'runtime': 2212},/workspace/brainbias/artifacts,,,0.2963387962410311,0.306218407266386,0.33855401998616685,0.33060548263559486,0.3699867384096016,0.31503496626595473,0.3100114056812877,0.33435091314410514,0.3438555691042052,0.34612033913146745,0.098044570331895,0.3365535006128693,0.33825789008958945,0.33055666561340863,0.2079217652416968,0.09043830507247316,0.10968564312804097,0.18099600301324245,0.13967470304737478,0.21334871083149676,0.2625312171921344,0.2564223198880576,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4833285808563232,,0.5,,0.3387535810470581,,,,0.4959941506385803,0.5,0.34444940090179443,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,8.016305895390456e-06,8.016305895390456e-06,8.016305895390456e-06,,roberta-large,restful-sound-997,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.1319766640663147,0.0,,,,,
41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,300.02967834472656,3,1696592601.2311304,{'runtime': 333},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5266925096511841,0.5,0.31724387407302856,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,7.0,False,,False,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,,,,,roberta-large,lyric-bee-996,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,,39.0,,,,,
42,0.17515461146831512,,0.27170315384864807,,0.12019044160842896,,,,,,,,,,,,,,,,,,,,,,,,,,,1965.5090670585632,42,1696590770.975379,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.497223824262619,,0.5043714046478271,,0.3806816041469574,,,,0.5025173425674438,0.4913079440593719,0.3689289391040802,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,8.016305895390456e-06,8.016305895390456e-06,8.016305895390456e-06,,roberta-large,golden-bush-995,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.13148613274097443,0.0,,,,,
43,,,,,,,,,,,,,0.17123468220233917,,0.7402589321136475,,0.8785380125045776,,,,,,,,,,,,,,,1892.7275557518003,42,1696588695.8756657,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.49540475010871887,,0.4970000088214874,,0.32982754707336426,,,,0.4682038128376007,0.49555936455726624,0.32108941674232483,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,8.016305895390456e-06,8.016305895390456e-06,8.016305895390456e-06,,roberta-large,cool-glade-994,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.9808571934700012,0.0,,,,,
44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2360.1767814159393,63,1696586761.2142663,{'runtime': 2359},/workspace/brainbias/artifacts,,,0.33822836750724217,0.32224700381312604,0.329399054297205,0.30441479405720895,0.3263145761914081,0.32470209244488746,0.30206704083787933,0.33183122757720296,0.3467327519625064,0.32009392762316957,0.08614520224647636,0.29331375535833654,0.2773057406110295,0.14261947959324436,0.27320633329412286,0.12915958715515788,0.04091418683803475,0.26041771559123267,0.26274545910891606,0.1877742413171337,0.28789157357533973,0.2329861158216615,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.49141862988471985,,0.5,,0.325551301240921,,,,0.5355622172355652,0.5,0.31724387407302856,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,2.2301757365478138e-07,2.2301757365478138e-07,,,roberta-large,still-puddle-993,,,,0.0,,"{'adamw': {'lr': 2.754228703338167e-07, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.7309180498123169,0.0,,,,,
45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2096.3066215515137,63,1696582887.2947435,{'runtime': 2095},/workspace/brainbias/artifacts,,,0.3239755309657212,0.3692998355566265,0.3573745036637867,0.32967389224782423,0.3806043471073729,0.3616381784992513,0.3313528722980261,0.39436673007685336,0.3988046960815988,0.3953116658082224,0.08449480929380224,0.3704708011160635,0.3765410736222029,0.2632185085020519,0.34134423282671017,0.08318318423182779,0.04388478249498421,0.2248889155768547,0.2734221091709338,0.22574666506641217,0.31927630966771636,0.23780886963912215,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5167930126190186,,0.4968096315860748,,0.36232990026474,,,,0.48889046907424927,0.4950557351112366,0.36861708760261536,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,8.097278682212583e-06,8.097278682212583e-06,,,roberta-large,fallen-smoke-991,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.714719295501709,0.0,,,,,
46,0.16780826449394226,,0.27287042140960693,,0.12115226686000824,,,,,,,,,,,,,,,,,,,,,,,,,,,2228.957775115967,43,1696580733.4583762,{'runtime': 2228},/workspace/brainbias/artifacts,,,0.3239755309657212,0.3692998355566265,0.3573745036637867,0.32967389224782423,0.3806043471073729,0.3616381784992513,0.3313528722980261,0.39436673007685336,0.3988046960815988,0.3953116658082224,0.08449480929380224,0.3704708011160635,0.3765410736222029,0.2632185085020519,0.34134423282671017,0.08318318423182779,0.04388478249498421,0.2248889155768547,0.2734221091709338,0.22574666506641217,0.31927630966771636,0.23780886963912215,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5276714563369751,,0.5,,0.32459110021591187,,,,0.5038683414459229,0.5,0.31724387407302856,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,8.016305895390456e-06,8.016305895390456e-06,8.016305895390456e-06,,roberta-large,giddy-hill-990,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.13182534277439115,0.0,,,,,
47,,,,,,,,,,,,,0.16414174437522888,,0.7418690919876099,,0.8821674585342407,,,,,,,,,,,,,,,2185.7181589603424,43,1696578460.20206,{'runtime': 2185},/workspace/brainbias/artifacts,,,0.36655263315699993,0.3386283415030718,0.28583819465285804,0.31357722941113453,0.29761967307748144,0.24520556425275755,0.29428439691271874,0.3061815217734468,0.24970850141375975,0.3012060817619921,0.1059875593305192,0.3050718669593882,0.36807423157078045,0.3495367031548565,0.37424000421556475,0.14629488425744344,0.13219608575740333,0.3154483033814802,0.3162109760278601,0.26149472387723505,0.23567051225651087,0.2292200561550285,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/RoBERTa_3hours_on_ethics_only.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8206333518028259,,0.7023620009422302,,0.6800127029418945,,,,0.939642071723938,0.8662060499191284,0.8599669337272644,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,8.016305895390456e-06,8.016305895390456e-06,8.016305895390456e-06,,roberta-large,restful-feather-989,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.978830873966217,0.0,,,,,
48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4256.626519441605,184,1696534434.5756226,{'runtime': 4256},/workspace/brainbias/artifacts,,,0.34730434224549994,0.2967526012938548,0.30920039061047727,0.24801381274072548,0.181580413700863,0.20249233526441476,0.31261993155280937,0.2026862641359042,0.16097378868439707,0.2391122223201802,0.11509147651672932,0.24194479721031703,0.1859235061207848,0.28272080286015405,0.13426290168063654,0.11550945318783956,0.04796946100792085,0.27110547229145876,0.2703136777095577,0.13979175745183353,0.19573118218009863,0.19154503617916424,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8096543550491333,,0.7204018831253052,,0.7092084884643555,,,,0.9650220274925232,0.901766836643219,0.9031136631965636,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,7.856781408072185e-06,7.856781408072185e-06,,,roberta-large,splendid-bird-988,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.0071256328374147415,0.0,,,,,
49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10295.013877630234,184,1696530125.8126576,{'runtime': 10294},/workspace/brainbias/artifacts,,,0.34730434224549994,0.2967526012938548,0.30920039061047727,0.24801381274072548,0.181580413700863,0.20249233526441476,0.31261993155280937,0.2026862641359042,0.16097378868439707,0.2391122223201802,0.11509147651672932,0.24194479721031703,0.1859235061207848,0.28272080286015405,0.13426290168063654,0.11550945318783956,0.04796946100792085,0.27110547229145876,0.2703136777095577,0.13979175745183353,0.19573118218009863,0.19154503617916424,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8067980408668518,,0.7215041518211365,,0.7104514837265015,,,,0.966066837310791,0.8968977332115173,0.8957172632217407,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,7.856781408072185e-06,7.856781408072185e-06,,,roberta-large,woven-night-987,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.0031903174240142107,0.0,,,,,
50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4225.638116598129,184,1696519799.6168425,{'runtime': 4225},/workspace/brainbias/artifacts,,,0.33822838850281534,0.322247023816653,0.32939907474469676,0.3044148129537992,0.3263145964474306,0.32470211260081444,0.30206705958873314,0.33183124817567244,0.34673277348599013,0.3200939474930443,0.08614520759395064,0.29331377356583155,0.2773057578248856,0.14261948844630648,0.2732063502534416,0.12915959517276035,0.04091418937778722,0.260417731756697,0.26274547541887594,0.18777425297324535,0.28789159144624993,0.23298613028430737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.492218554019928,,0.5002519488334656,,0.3428170382976532,,,,0.48655757308006287,0.4990079402923584,0.3485328257083893,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,7.856781408072185e-06,7.856781408072185e-06,,,roberta-large,fresh-mountain-986,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.8289176821708679,0.0,,,,,
51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,917.357797384262,30,1696515509.9806144,{'runtime': 916},/workspace/brainbias/artifacts,,,0.33822838850281534,0.322247023816653,0.32939907474469676,0.3044148129537992,0.3263145964474306,0.32470211260081444,0.30206705958873314,0.33183124817567244,0.34673277348599013,0.3200939474930443,0.08614520759395064,0.29331377356583155,0.2773057578248856,0.14261948844630648,0.2732063502534416,0.12915959517276035,0.04091418937778722,0.260417731756697,0.26274547541887594,0.18777425297324535,0.28789159144624993,0.23298613028430737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5052210092544556,,0.4998582899570465,,0.3324834406375885,,,,0.5532541275024414,0.4998582899570465,0.32285791635513306,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,2.3808428509309656e-05,2.3808428509309656e-05,,,roberta-large,laced-shape-985,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.8610630631446838,0.0,,,,,
52,,,,,,,,,,,,,0.1299133449792862,,0.7351474165916443,,0.870998203754425,,,,,,,,,,,,,,,2243.750134944916,60,1696448663.096457,{'runtime': 2243},/workspace/brainbias/artifacts,,,0.3126806229773663,0.3385966943558591,0.3502626769158152,0.3176381259404611,0.33614446787756147,0.3400949560685249,0.295280796908554,0.2815801808243228,0.29202769358811487,0.3100267528224743,0.038614221873627515,0.2849374368478668,0.33086801853710357,0.24395212460898716,-0.013760723605959908,0.18863576871595744,0.09700925224247008,0.28234334957892904,0.15284357880167265,0.11916396835604053,0.19407631137311224,0.24804228712922416,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.49847835302352905,,0.5,,0.327863872051239,,,,0.4601143300533294,0.5,0.316671758890152,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,2.3808428509309656e-05,2.3808428509309656e-05,2.3808428509309656e-05,,roberta-large,earthy-wave-984,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.9170128703117372,0.0,,,,,
53,0.16991810500621796,,0.27741605043411255,,0.124864399433136,,,,,,,,,,,,,,,,,,,,,,,,,,,2243.6332664489746,60,1696446397.5908206,{'runtime': 2243},/workspace/brainbias/artifacts,,,0.37494534054008993,0.3503474697309174,0.366210042509004,0.3179936698606042,0.33911197158573053,0.3496581781153095,0.338086277683165,0.3628964085493228,0.3799770943385098,0.34276034374385606,0.11130823458289628,0.3524193055027754,0.3914846604353941,0.3320115671251123,0.31227368341888007,0.1373491990601795,0.039767682024466615,0.25226793310556855,0.26798208857359695,0.3108785625845842,0.3343887661520009,0.20689916943482853,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_roberta-large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5426263809204102,,0.5,,0.3334672152996063,,,,0.6863453388214111,0.5,0.34722059965133667,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,2.3808428509309656e-05,2.3808428509309656e-05,2.3808428509309656e-05,,roberta-large,colorful-wind-983,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.10189230740070344,0.0,,,,,
54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4594.457991838455,12,1696444128.019307,{'runtime': 4594},/workspace/brainbias/artifacts,,,0.33822838850281534,0.322247023816653,0.32939907474469676,0.3044148129537992,0.3263145964474306,0.32470211260081444,0.30206705958873314,0.33183124817567244,0.34673277348599013,0.3200939474930443,0.08614520759395064,0.29331377356583155,0.2773057578248856,0.14261948844630648,0.2732063502534416,0.12915959517276035,0.04091418937778722,0.260417731756697,0.26274547541887594,0.18777425297324535,0.28789159144624993,0.23298613028430737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5622308254241943,,0.4990706443786621,,0.3903071880340576,,,,0.7235457301139832,0.5,0.34823742508888245,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 512}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 512}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 512}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,True,,artifacts/train_head_on_ethics_roberta-large.ckpt,,,,,0.0018926429760766125,0.0018926429760766125,,,roberta-large,clear-star-982,,,,0.0,,"{'adamw': {'lr': 0.0022908676527677745, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.6627026200294495,0.0,,,,,
55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2254.48078417778,2,1696439051.401467,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6528870463371277,0.5,0.34823742508888245,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 512}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 512}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 512}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,2.0,True,,True,,artifacts/train_head_on_ethics_roberta-large.ckpt,,,,,0.01445439770745928,0.01445439770745928,,,roberta-large,giddy-waterfall-981,,,,0.0,,"{'adamw': {'lr': 0.01445439770745928, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.6626973748207092,49.0,,,,,
56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,741.2517783641815,33,1696436385.4048374,{'runtime': 938},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5073241591453552,0.5,0.34722059965133667,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,2.0,False,,False,,artifacts/train_head_on_ethics_roberta-large.ckpt,,,,,1e-05,1e-05,,,roberta-large,glorious-shape-979,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.7078518867492676,149.0,,,,,
57,,,,,,,,,,,,,0.14017722010612488,,0.7299309372901917,,0.8586615324020386,,,,,,,,,,,,,,,2226.0437762737274,60,1696430354.8933172,{'runtime': 2225},/workspace/brainbias/artifacts,,,0.33560684343004776,0.3354080194672112,0.3018440276506819,0.29152339317025666,0.2999658806248197,0.32498424982748036,0.3311252951824707,0.30737234259327006,0.35877597291621804,0.309773165029189,0.1212940347270256,0.3393505127126982,0.3054835750390689,0.37961224813630146,0.3521212743820887,0.12365346178262264,0.05340530843138517,0.2280153016071826,0.04677389139002431,0.21912716468594504,0.25747160796882207,0.2365359269930809,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.49641185998916626,,0.5006285905838013,,,,,,0.47972455620765686,0.5010448694229126,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,7.105532272722918e-06,,,,,roberta-large,polar-planet-976,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.9302008152008056,0.0,,,,,
58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7028.423704385757,127,1696428106.9987323,{'runtime': 7028},/workspace/brainbias/artifacts,,,0.3633453409332048,0.396574067069308,0.3402714568544661,0.3171869262434517,0.3133927454666199,0.3324968648506184,0.34742798515411444,0.1858913707835642,0.20575776812651167,0.24196650484400423,0.05330614393727405,0.20487139203481136,0.2544697021225219,0.1076171707209396,0.21670649941120368,0.1041994805908133,0.02913710603515905,0.1687186534919845,0.15428236996932498,0.16605510046121966,0.28088259918662456,0.24392263855529783,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8173218369483948,,0.7316893339157104,,,,,,0.9593263864517212,0.8956918716430664,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,7.105532272722918e-06,,,,,roberta-large,azure-dust-975,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.10758067667484283,0.0,,,,,
59,0.17090657353401184,,0.27732497453689575,,0.12476351112127304,,,,,,,,,,,,,,,,,,,,,,,,,,,2215.8219594955444,60,1696421056.5767975,{'runtime': 2215},/workspace/brainbias/artifacts,,,0.29404264620717735,0.3010681456378557,0.25276640652339444,0.30209644911535805,0.2819139757937158,0.3597311887632455,0.2717897745153098,0.3194927527040357,0.32511402144521134,0.3014204946508654,0.07939078905270905,0.35482456672940227,0.31938570047960624,0.30606319656774056,0.2893203541854603,0.12111551306200798,0.10120672620140345,0.1444026645247293,0.07567735471305828,0.12638007937563206,0.25364234742659036,0.24818670303037507,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_roberta-large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7502238154411316,,0.5,,,,,,0.8310742378234863,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,7.105532272722918e-06,,,,,roberta-large,super-planet-974,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.11165616661310196,0.0,,,,,
60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7031.11376452446,127,1696418812.3009963,{'runtime': 7030},/workspace/brainbias/artifacts,,,0.34034084923091024,0.33308686838527063,0.33077252878548385,0.2635545226793689,0.196471986980213,0.2616025025526643,0.24619730941920975,0.20244613783237567,0.2699253797716014,0.1159434026542636,0.08191821147101915,0.14805327272325233,0.08699587112870397,-0.002875976088477108,0.15900933712084714,0.14098385129189908,0.07772412125853381,0.22321933903477473,0.18343332822506653,0.1180952584398044,0.2514520595071947,0.2241878718130952,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8267990946769714,,0.734114408493042,,,,,,0.9647541642189026,0.9020938277244568,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/train_head_on_ethics_roberta-large.ckpt,,,,7.105532272722918e-06,,,,,roberta-large,noble-smoke-973,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.0004474793386179954,0.0,,,,,
61,0.17283880710601807,,0.27713027596473694,,0.12462146580219267,,,,,,,,,,,,,,,,,,,,,,,,,,,2222.5290319919586,60,1696411760.196316,{'runtime': 2222},/workspace/brainbias/artifacts,,,0.3034749991550626,0.2954450308742321,0.2661656951439136,0.268037785983634,0.2920392604175698,0.29276883580553975,0.2870797197664294,0.2770957024420944,0.3313659839097765,0.3410581857129908,0.13190870941607435,0.3337793427892308,0.3168599505028585,0.22138558333085245,0.329209227395555,0.13123545793915342,0.07658398439672387,0.2514370403473745,0.2189451763545365,0.21399004259074125,0.18904990123440288,0.2109263086638237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4714189767837525,,0.5,,,,,,0.4455546438694,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,7.105532272722918e-06,,,,,roberta-large,expert-water-972,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.1318855732679367,0.0,,,,,
62,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,466.7319097518921,15,1696409249.6221538,{'runtime': 487},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_DEBERTA.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5043888092041016,0.5013333559036255,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 2}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 4}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,1.0,False,,False,,,,,,6e-06,,,,,microsoft/deberta-v2-xlarge,legendary-wave-970,,,,0.0,,"{'adamw': {'lr': 6e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 4, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 2, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.1312551200389862,279.0,,,,,
63,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13518.855049610138,405,1696408727.4811575,{'runtime': 13518},/workspace/brainbias/artifacts,,,0.20514825390781224,0.2805008846670336,0.25141891024841045,0.3172963271021132,0.21508238026313156,0.3348086783298541,0.3385739205452974,0.336782196896906,0.3321632458214854,0.3350256968717293,0.08254292420317931,0.33169316069220856,0.3632626208115044,0.3636464835507143,0.36294120755906817,0.11263463718941838,0.2825612537296941,-0.037188359787476206,0.060224225907266914,0.1272471352312148,0.11740521076632476,0.21559569449438995,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5173333287239075,,0.49799999594688416,,,,,,0.5127777457237244,0.5013333559036255,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 4}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/train_head_on_ethics_DEBERTA.ckpt,,,,4.714068844843314e-06,,,,,microsoft/deberta-v2-xlarge,ancient-glitter-969,,,,0.0,,"{'adamw': {'lr': 6e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 4, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 2, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.6854463219642639,0.0,,,,,
64,0.17429454624652865,,0.2773292362689972,,0.12477093189954758,,,,,,,,,,,,,,,,,,,,,,,,,,,6646.211628198624,220,1696377542.0770931,{'runtime': 6645},/workspace/brainbias/artifacts,,,0.05706971606460968,-0.10920203674454536,0.15194028044806362,0.10420635115287288,0.11375472457454344,0.0881360770753032,0.2018201565485832,0.22719276538317348,0.30708201963041704,0.33297827425108234,-0.0002674415277525938,0.3777265936889092,0.3715757486845242,0.376358065882721,0.3766978902489813,-0.062143263403704865,0.3075390453179245,0.03255426042119116,0.16237119135258007,0.19420589820441783,0.1822284861042115,0.013417158614573285,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_DEBERTA.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.41850000619888306,,0.484499990940094,,,,,,0.479333370923996,0.4986666738986969,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 2}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 4}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,4.263319363633753e-06,,,,,microsoft/deberta-v2-xlarge,devoted-puddle-968,,,,0.0,,"{'adamw': {'lr': 6e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 4, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 4, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.10723619163036346,0.0,,,,,
65,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13131.000771045685,336,1696370837.9160771,{'runtime': 13130},/workspace/brainbias/artifacts,,,0.3094017343668406,0.15018780442899743,0.19860612983820536,0.3233033312572045,-0.006328276031755959,-0.13137599396116462,0.18389882933656423,0.20891239744595813,0.16425068022402364,0.11608781943500808,-0.010606050812273345,0.1600793983860926,0.1769017541832257,0.12495296455825664,0.1857908322332964,-0.1276971976453607,0.25135080802214443,0.13372223502855363,0.2154528183437962,0.2377881377292907,0.21903424759429416,0.06854704365269279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5046666860580444,,0.49900001287460327,,,,,,0.5132222175598145,0.49844446778297424,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 4}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/train_head_on_ethics_DEBERTA.ckpt,,,,4.263319363633753e-06,,,,,microsoft/deberta-v2-xlarge,icy-cosmos-967,,,,0.0,,"{'adamw': {'lr': 6e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 4, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 4, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.7565827369689941,0.0,,,,,
66,,,,,,,,,,,,,0.07551824301481247,,0.7818760871887207,,0.9810325503349304,,,,,,,,,,,,,,,2958.903335094452,115,1696357329.0932171,{'runtime': 2958},/workspace/brainbias/artifacts,,,0.35012002091192146,0.2977970876263915,0.2673998678371528,0.2885522882321249,0.30633728106319164,0.25788110002598963,0.12442009278224549,0.3928667448979578,0.18913134933600328,0.1913196740276152,-0.001377942526367125,0.14208332120428951,0.08391797956524591,0.04197702062695876,0.3153586808620735,0.20189420981994124,0.23905267207925496,0.09841376243925322,0.2047477930157834,0.254687344858374,0.3347865226059612,0.3608287449889741,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4621666967868805,,0.48773330450057983,,,,,,0.4653632342815399,0.49751773476600647,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,2.131659681816876e-05,,,,,bert-large-cased,absurd-snowflake-964,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.7217020988464355,0.0,,,,,
67,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4717.6433227062225,141,1696354349.5769117,{'runtime': 4717},/workspace/brainbias/artifacts,,,0.34126627032840273,0.3385376042561437,0.32287843151627166,0.35449570742138226,0.275891749017739,0.29163975789339713,0.2554567286435677,0.35833549891523014,0.1196652409629754,0.1136912885649252,0.07771093558114169,0.10607988086702576,0.19959168769728225,0.1564377413774746,0.2050112562179964,0.07854619945954483,0.18278455804408997,0.12554637963329723,0.19124328189326337,0.23310660230003985,0.3065106766259628,0.3254495269586635,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.628201961517334,,0.5872511267662048,,,,,,0.8822034597396851,0.7927094101905823,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,2.131659681816876e-05,,,,,bert-large-cased,curious-night-963,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.0002602000313345343,0.0,,,,,
68,0.0979049950838089,,0.2960107624530792,,0.14090068638324738,,,,,,,,,,,,,,,,,,,,,,,,,,,2956.4278090000153,115,1696349612.602773,{'runtime': 2955},/workspace/brainbias/artifacts,,,0.3509641146722456,0.3253619602435479,0.29049199251384555,0.2882629400111552,0.17614652401061262,0.1684878783121999,0.17603537864308447,0.30313235491112733,0.26596734491592566,0.32544762199002547,0.183525745562304,0.15081320022661257,0.106135458719898,0.08333169215960529,0.25818886994167173,0.28719084251020816,0.2643015896927895,0.14646789345172784,0.20007756031561852,0.22735221281400544,0.29440025705784656,0.2486431767198624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6858237981796265,,0.5959523916244507,,,,,,0.8713372349739075,0.7773470878601074,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,2.131659681816876e-05,,,,,bert-large-cased,hopeful-frog-962,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.10933154076337814,0.0,,,,,
69,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4719.336962938309,141,1696346628.482982,{'runtime': 4718},/workspace/brainbias/artifacts,,,0.3220507952193136,0.352178004574483,0.2901739962598569,0.26990958073549853,0.2999266883139463,0.2790225563473503,0.27843400621067255,0.17472822219393375,0.14442472949743715,0.1585719523936925,0.175464556795264,-0.0015882501317977364,-0.03949147284022735,0.08393898766252246,0.08860702071294721,0.17317123967904463,0.1820815713783398,0.10717556214157192,0.1709976069296683,0.2157167334551117,0.2818484034540034,0.22616537125778985,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6522274613380432,,0.5882715582847595,,,,,,0.875968873500824,0.7802029848098755,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,2.131659681816876e-05,,,,,bert-large-cased,cosmic-glade-959,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.00530822342261672,0.0,,,,,
70,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1482.2194278240204,82,1696341619.59508,{'runtime': 1488},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5004377365112305,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,3.0,False,,False,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,0.0003,,,,,bert-large-cased,vibrant-waterfall-952,,,,0.0,,"{'adamw': {'lr': 0.0003, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.646603524684906,1999.0,,,,,
71,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,960.6575095653534,16,1696339364.4777205,{'runtime': 960},artifacts,,,0.1042449089470374,0.17855737995550663,,,,,,,,,0.11349399725788611,,,,,0.2672098204632186,0.2324855959114379,0.2795970066083814,0.27211341159532626,0.3058449064804556,0.2333402474260197,0.1802243640974078,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5685949325561523,,0.5444515347480774,,,,,,0.8106157183647156,0.6678712368011475,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 20}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 20}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 20}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,2e-05,,,,,bert-base-cased,true-snowball-951,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 20, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.5583480000495911,0.0,,,,,
72,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1431.892289876938,0,1696336899.0814388,,artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5009110569953918,0.5,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:100]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,,False,,,,,,,,,,,,bert-base-cased,likely-terrain-947,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': False, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,
73,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,207.1023421287537,0,1696327739.921005,,artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5124695301055908,0.5,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:100]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,,False,,,,,,,,,,,,bert-base-cased,rich-sea-946,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': False, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,
74,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1809.006688594818,8,1696326953.2640183,,artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5163238048553467,0.5,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:100]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,6.0,,False,,,,,,,0.05,,,,,bert-base-cased,jumping-frost-943,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': False, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0133631229400635,69.0,,,,,
75,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,463.9216029644013,11,1696323301.528449,,artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 15}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 15}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,0.05,,,,,bert-base-cased,trim-tree-934,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.7024229764938354,299.0,,,,,
76,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,773.6532227993011,1,1696266528.2410178,{'runtime': 772},artifacts,,,0.38023763631092394,0.3800893326214198,,,,,,,,,0.34951545775095844,,,,,0.3766162822069544,0.380991853212456,0.3802474159881062,0.38017094681297714,0.38026066211649223,0.3804614835533136,0.3802079266584236,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5005083680152893,,0.5,,,,,,,,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 15}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 15}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,,,,,bert-base-cased,young-planet-932,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,,0.0,,,,,
77,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,572.0016977787018,1,1696265357.4877548,{'runtime': 570},artifacts,,,0.3802118553360058,0.3803125093985857,,,,,,,,,0.33103710666881775,,,,,0.37648990234034624,0.3739526302144206,0.374457417744315,0.3802166804784645,0.3802351321377888,0.3802382729767057,0.3803734769584992,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4844002425670624,,0.5,,,,,,,,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 15}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 15}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,,,,,bert-base-cased,faithful-snow-931,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,,0.0,,,,,
78,,,,,,,,,,,,,0.04268528521060944,,2.0720767974853516,,5.936777114868164,,,,,,,,,,,,,,,622.9897091388702,1,1696264249.2137852,{'runtime': 621},artifacts,,,0.09767950893340302,0.12065021276211914,,,,,,,,,0.21037009959487377,,,,,0.1628953353068621,0.24344361119637123,0.2846963759028824,0.319497175520607,0.2871647190379742,0.29300549212792026,-0.053407397388914826,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.51739102602005,,0.5032753944396973,,,,,,,,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,,,,,bert-base-cased,hardy-donkey-930,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,
79,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2016.7818207740784,23,1696249348.1587217,,artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5024834275245667,0.5,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,4.0,False,,False,,,,,,0.04567586237418205,,,,,bert-base-cased,major-dragon-920,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 50, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.702164888381958,499.0,,,,,
80,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,991.8274314403534,28,1696247091.5418124,,artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4984446167945862,0.5,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 15}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 15}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,1.0,False,,False,,,,,,0.05,,,,,bert-base-cased,autumn-butterfly-918,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.6853033900260925,699.0,,,,,
81,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,147.84179615974426,11,1696074700.883439,,artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 15}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 15}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,0.05,,,,,bert-base-cased,rich-wildflower-914,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,0.8326610326766968,299.0,,,,,
82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,985.1379437446594,19,1696072844.6988597,{'runtime': 989},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4979964196681976,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 64}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,5.0,True,,True,,artifacts/train_head_on_ethics_bert-base-cased.ckpt,,,,0.04365158322401657,,,,,bert-base-cased,astral-wind-910,,,,0.0,,"{'adamw': {'lr': 0.04365158322401657, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,highest,,0.6849092245101929,491.0,,,,,
83,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,249.58392310142517,3,1696071476.561213,,artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,0.05,,,,,bert-base-cased,laced-microwave-908,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.2188972234725952,99.0,,,,,
84,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2704.7518393993378,133,1696070725.9860003,{'runtime': 2986},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5,,0.5,,,,,,0.5014163255691528,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,True,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,0.23528628701974832,,,,,bert-large-cased,breezy-silence-906,,,,0.0,,"{'adamw': {'lr': 0.3311311214825908, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 10, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,high,,0.3713749051094055,0.0,,,,,
85,,,,,,,,,,,,,0.08309121429920197,,0.8017702102661133,,1.0300626754760742,,,,,,,,,,,,,,,687.6243345737457,21,1696067982.4763737,{'runtime': 686},/workspace/brainbias/artifacts,,,0.1826117850897664,0.195808343220449,,,,,,,,,0.24880390067433353,,,,,0.2664140111515024,0.3651099563846942,0.3042149957634867,0.34040520883523484,0.09781555565857002,0.06915538621558764,0.2950620442591432,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5223536491394043,,0.5042048692703247,,,,,,0.5177371501922607,0.5025123953819275,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 128}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,True,,,,,,0.0002089296130854041,,,,,bert-base-cased,sunny-monkey-905,,,,0.0,,"{'adamw': {'lr': 0.0002089296130854041, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,high,,0.7510178089141846,0.0,,,,,
86,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1130.2561600208282,36,1696067276.350118,{'runtime': 1129},/workspace/brainbias/artifacts,,,0.3572854661840421,0.36227684544957367,,,,,,,,,0.3211743741772153,,,,,0.3488703938517769,0.3360901339728745,0.3404096823486418,0.3659394658523781,0.3705621819972194,0.3662707920212426,0.35959553382014375,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5003742575645447,,0.5,,,,,,0.49057623744010925,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 64}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,True,,,,,,0.0003019951720402019,,,,,bert-base-cased,fearless-plasma-904,,,,0.0,,"{'adamw': {'lr': 0.0003019951720402019, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,high,,0.7014466524124146,0.0,,,,,
87,0.18341776728630063,,0.27828681468963623,,0.12548914551734924,,,,,,,,,,,,,,,,,,,,,,,,,,,694.0684797763824,21,1696066127.1673317,{'runtime': 693},/workspace/brainbias/artifacts,,,0.3621582967443248,0.36219272262878505,,,,,,,,,0.05227342217608725,,,,,0.15044705318823545,0.25954463049685267,0.3254220493158258,0.3356383072686813,0.33012278316714316,0.33512337653177937,0.3563839272535652,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert-base-cased.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.498903751373291,,0.5,,,,,,0.4921068847179413,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 128}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,True,,,,,,0.0002089296130854041,,,,,bert-base-cased,exalted-darkness-903,,,,0.0,,"{'adamw': {'lr': 0.0002089296130854041, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,high,,0.12514176964759827,0.0,,,,,
88,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1131.6209411621094,36,1696065413.054603,{'runtime': 1130},/workspace/brainbias/artifacts,,,0.36194469591096107,0.36223292194320145,,,,,,,,,0.3217022839447047,,,,,0.2688051093185034,0.33965812903507064,0.36191724349957954,0.36435463883205504,0.3607066694416313,0.3609384490240858,0.36101113443645855,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.49394261837005615,,0.5,,,,,,0.5192644000053406,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 64}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,True,,artifacts/train_head_on_ethics_bert-base-cased.ckpt,,,,0.0003019951720402019,,,,,bert-base-cased,dutiful-wind-902,,,,0.0,,"{'adamw': {'lr': 0.0003019951720402019, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,high,,0.714978039264679,0.0,,,,,
89,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,464.1264181137085,14,1696063807.2241511,{'runtime': 475},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.49854618310928345,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 64}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,4.0,True,,True,,artifacts/train_head_on_ethics_bert-base-cased.ckpt,,,,0.2290867652767775,,,,,bert-base-cased,dainty-water-900,,,,0.0,,"{'adamw': {'lr': 0.2290867652767775, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,high,,0.7469649910926819,349.0,,,,,
90,,,,,,,,,,,,,0.12272857874631882,,0.7341504693031311,,0.8669653534889221,,,,,,,,,,,,,,,1440.0082774162292,63,1696011613.8472843,{'runtime': 1439},/workspace/brainbias/artifacts,,,0.31586105160994127,0.30407839854224744,0.33456305594839636,0.3117920459457952,0.3211071804554736,0.26468456671804336,0.2233512919903557,0.09341242202196905,0.1644008752852461,0.2834976607538661,-0.033697229824231886,0.0575112865425629,0.12992870411290575,0.08642372262359822,0.24303525892020225,0.0036523395773746113,-0.04641468260657874,-0.05090413184197535,0.10455193068599929,0.12692417812506704,0.3077643618426925,0.298164966004429,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4079999923706054,,0.4925000071525574,,,,,,0.4518055617809296,0.4740000069141388,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 2}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 3}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 3}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,3.311311214825911e-05,,,,,bert-large-cased,firm-firefly-891,,,,0.0,,"{'adamw': {'lr': 3.311311214825911e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 5, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 30, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.7531280517578125,0.0,,,,,
91,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2394.5915253162384,87,1696010149.6494863,{'runtime': 2393},/workspace/brainbias/artifacts,,,0.3522547526752202,0.3443323238705982,0.3534948883940038,0.3344616657115687,0.28258035669553516,0.31654265048624775,0.24879502158448208,0.19890389203045453,0.14233077398746374,-0.061472178670048394,0.02848921660811646,-0.004245944167388389,-0.0965083073930848,-0.051433520068162944,0.17950328147095929,0.06296782395791893,0.05474473342321338,0.06511660678275885,0.1550333879011282,0.23767458109358036,0.3102357165358096,0.26526376155218345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.458333283662796,,0.4879167675971985,,,,,,0.5205556154251099,0.517916738986969,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,7.585775750291837e-08,,,,,bert-large-cased,different-valley-890,,,,0.0,,"{'adamw': {'lr': 7.585775750291837e-08, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 5, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 30, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.6755181550979614,0.0,,,,,
92,0.17471802234649658,,0.276954710483551,,0.12443149834871292,,,,,,,,,,,,,,,,,,,,,,,,,,,1424.4357006549835,63,1696007715.2175117,{'runtime': 1423},/workspace/brainbias/artifacts,,,0.3513770175487146,0.36723858984778573,0.32123119536342604,0.3137051040952623,0.3283668039624416,0.3664323286541217,0.3277139126967679,0.29647230025655874,0.24086557668607692,0.25113337740185876,-0.0003070261900794726,0.342678021697284,0.2510618530472403,0.028003500316962863,0.29732833873225156,0.09782503040655456,0.05137127468375361,0.01940278447022005,0.12303070442533892,0.15561994370296547,0.3018735080175085,0.30850768377641286,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4020000100135803,,0.4794999957084656,,,,,,0.4369444251060486,0.4939444661140442,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 2}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 3}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 3}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,3.311311214825911e-05,,,,,bert-large-cased,fluent-planet-889,,,,0.0,,"{'adamw': {'lr': 3.311311214825911e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 5, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 30, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.129416823387146,0.0,,,,,
93,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2275.6702258586884,87,1696006256.4599595,{'runtime': 2275},/workspace/brainbias/artifacts,,,0.3519743289575086,0.34508167255977573,0.3533663781999606,0.33479883033741564,0.28271220168939826,0.317057012557654,0.2480367882013365,0.200745610728555,0.14688784218588022,-0.06026623346569492,0.028413538596662253,-0.010039500653164024,-0.09453658936655462,-0.05864979361735801,0.1552406733996675,0.06471993765819911,0.056100291083841305,0.06590745360678818,0.15434969191090384,0.2397963264852351,0.311099015000709,0.2657317384120917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4681251645088196,,0.49791669845581055,,,,,,0.5106944441795349,0.5182222127914429,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,7.585775750291837e-08,,,,,bert-large-cased,prime-dust-888,,,,0.0,,"{'adamw': {'lr': 7.585775750291837e-08, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 5, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 30, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.656735360622406,0.0,,,,,
94,0.15350741147994995,,0.27818477153778076,,0.1253318190574646,,,,,,,,,,,,,,,,,,,,,,,,,,,3765.083158016205,61,1696002390.879321,{'runtime': 3764},/workspace/brainbias/artifacts,,,0.3802136853905397,0.3637951907320965,0.3462523177539016,0.3443812255160895,0.27884218579184034,0.348541727724944,0.2855519752903995,0.2474739340241455,0.15471755505698248,0.1884982432301489,0.02915650241568992,0.20766204726723095,0.2617978745752624,0.04627166372883739,0.14106719425499342,0.0678715173819958,0.05520514567131743,0.08865944387059002,0.20332238404376363,0.25806247453458386,0.33486022928962544,0.3328380046833936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5404167771339417,,0.5272916555404663,,,,,,0.5173822641372681,0.5298148393630981,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 10}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 5}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,3.0199517204020163e-06,,,,,bert-large-cased,iconic-lion-882,,,,0.0,,"{'adamw': {'lr': 3.0199517204020163e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 10, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.10964985191822052,0.0,,,,,
95,,,,,,,,,,,,,-0.0010731964139267802,,0.7606505155563354,,0.9253008961677552,,,,,,,,,,,,,,,1384.9263854026794,47,1695998488.7736604,{'runtime': 1384},/workspace/brainbias/artifacts,,,0.13470922930208062,0.15483395156807078,,,,,,,,,0.16327209307637433,,,,,0.3066490575835245,0.17794125255352045,0.28328397923369747,0.2982680268620956,0.3318781509305676,0.23115144543800975,0.13111453458687325,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.49948322772979736,,0.4944084584712982,,,,,,0.4704594612121582,0.5057781338691711,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 7}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 7}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,7.585775750291837e-08,,,,,bert-base-cased,exalted-universe-880,,,,0.0,,"{'adamw': {'lr': 7.585775750291837e-08, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 3, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.9907834529876708,0.0,,,,,
96,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1787.7064092159271,57,1695997079.0674062,{'runtime': 1787},/workspace/brainbias/artifacts,,,0.34555934382829734,0.35413699484079153,,,,,,,,,0.25659849911134097,,,,,0.32454188036990667,0.32363067119469957,0.3629231323489557,0.36316629220960495,0.3626866920571588,0.365442162252031,0.35874817260135505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.502358615398407,,0.5,,,,,,0.45126432180404663,0.5149999856948853,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 15}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 15}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,0.036307805477010104,,,,,bert-base-cased,winter-cosmos-879,,,,0.0,,"{'adamw': {'lr': 0.036307805477010104, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 3, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.6900049448013306,0.0,,,,,
97,-0.005741635803133249,,0.3692806363105774,,0.21644584834575653,,,,,,,,,,,,,,,,,,,,,,,,,,,1265.169862985611,47,1695995271.244234,{'runtime': 1264},/workspace/brainbias/artifacts,,,0.22263775254492912,0.07482160943709959,,,,,,,,,0.18274314321455595,,,,,0.17830441129351907,0.13758176833672495,0.2885089148791047,0.26103824951151966,0.31824581247162054,0.13155108412183955,0.05889042409290612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert-base-cased.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.604866623878479,,0.5554250478744507,,,,,,0.7814788222312927,0.736579418182373,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 7}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 7}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,7.585775750291837e-08,,,,,bert-base-cased,fancy-pond-878,,,,0.0,,"{'adamw': {'lr': 7.585775750291837e-08, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 3, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.2226415723562241,0.0,,,,,
98,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1625.167465686798,57,1695993977.9726996,{'runtime': 1624},/workspace/brainbias/artifacts,,,0.202870712685991,0.11268483081359504,,,,,,,,,0.1817576225622072,,,,,0.17873367384824024,0.13815469979668063,0.28596368046058934,0.26104841911444726,0.31751659190719544,0.13601874311663106,0.06012968277718592,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6034502983093262,,0.5539613366127014,,,,,,0.770356297492981,0.7173922657966614,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 15}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 15}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,artifacts/train_head_on_ethics_bert-base-cased.ckpt,,,,5.7543993733715664e-05,,,,,bert-base-cased,logical-hill-877,,,,0.0,,"{'adamw': {'lr': 5.7543993733715664e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 3, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.003663677722215653,0.0,,,,,
99,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,172.29491090774536,9,1695989482.124059,{'runtime': 170},/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,0.09767950893340302,0.12065021276211914,,,,,,,,,0.21037009959487377,,,,,0.1628953353068621,0.24344361119637123,0.2846963759028824,0.319497175520607,0.2871647190379742,0.29300549212792026,-0.053407397388914826,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,,False,,False,,,,,,,,,,,bert-base-cased,ethereal-sea-873,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 0, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 0, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
100,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,47.7519052028656,1,1695988885.0939765,{'runtime': 46},/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,0.09767950893340302,0.12065021276211914,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,,False,,False,,,,,,,,,,,bert-base-cased,copper-spaceship-871,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 0, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 0, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
101,,,,,,,,,,,,,0.06015564501285553,,0.9034355878829956,,1.2954225540161133,,,,,,,,,,,,,,,293.5558943748474,30,1695976803.3028843,{'runtime': 465},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5474438071250916,,0.5151380300521851,,,,,,0.7380544543266296,0.6642106771469116,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,0.05,,,,,bert-base-cased,fearless-bush-856,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.298783302307129,0.0,,,,,
102,0.15920928120613098,,0.2743273675441742,,0.12248945981264114,,,,,,,,,,,,,,,,,,,,,,,,,,,2630.904547929764,52,1695973703.189195,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5840162038803101,,0.5525725483894348,,,,,,0.7428044676780701,0.6499056816101074,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 11}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 11}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 12}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 12}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,1.9054607179632464e-05,,,,,bert-large-cased,lemon-firebrand-851,,,,0.0,,"{'adamw': {'lr': 1.9054607179632464e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.1044418066740036,0.0,,,,,
103,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3551.4800577163696,92,1695944874.9687057,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4540276527404785,0.4655402302742005,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 11}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 11}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 12}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 12}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,99.0,False,,True,,,,,,4.691053534854179e-07,,,,,bert-large-cased,rich-star-849,,,,0.0,,"{'adamw': {'lr': 4.786300923226383e-07, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.819086492061615,999.0,,,,,
104,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3083.5333409309387,57,1695940734.9976878,{'runtime': 3630},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5004764199256897,,0.5,,,,,,0.500124454498291,0.4996379315853119,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 23}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 23}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,0.003981071705534969,,,,,bert-large-cased,serene-breeze-848,,,,0.0,,"{'adamw': {'lr': 0.003981071705534969, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.6848539113998413,0.0,,,,,
105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3623.6009180545807,89,1695937586.782319,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5173621773719788,0.505025327205658,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 11}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 11}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 12}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 12}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,98.0,False,,True,,,,,,3.278198102677652e-07,,,,,bert-large-cased,fresh-eon-847,,,,0.0,,"{'adamw': {'lr': 3.311311214825911e-07, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.7859818935394287,989.0,,,,,
106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1337.416684627533,57,1695933523.6130016,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5692427158355713,,0.5298568606376648,,,,,,0.8016980886459351,0.6453668475151062,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 23}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 23}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,0.025118864315095826,,,,,bert-large-cased,ruby-wood-846,,,,0.0,,"{'adamw': {'lr': 0.025118864315095826, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.9489009380340576,0.0,,,,,
107,0.18391674757003784,,0.2717154026031494,,0.12031222879886629,,,,,,,,,,,,,,,,,,,,,,,,,,,3954.974606990814,93,1695931567.244118,{'runtime': 4533},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4773915410041809,,0.4997403919696808,,,,,,0.4869998097419739,0.5003620386123657,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 11}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 11}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 12}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 12}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,2.959854681166016e-06,,,,,bert-large-cased,dashing-bush-845,,,,0.0,,"{'adamw': {'lr': 3.0199517204020163e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.11975033581256866,0.0,,,,,
108,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2857.7242069244385,56,1695927367.875444,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4969413578510285,0.4996379315853119,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 23}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 23}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,9.0,False,,True,,,,,,0.0001,,,,,bert-large-cased,wise-rain-844,,,,0.0,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.6968822479248047,449.0,,,,,
109,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4814.1206974983215,89,1695924449.1679964,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7588318586349487,0.6646527051925659,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 11}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 11}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 12}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 12}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,98.0,False,,True,,,,,,2.989752203197996e-06,,,,,bert-large-cased,pious-feather-843,,,,0.0,,"{'adamw': {'lr': 3.0199517204020163e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.10362353175878523,989.0,,,,,
110,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1314.2497627735138,57,1695917901.7924058,{'runtime': 2982},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5655314922332764,,0.5423464775085449,,,,,,0.7988473773002625,0.6546140909194946,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 23}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 23}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,0.02089296130854041,,,,,bert-large-cased,comic-pond-842,,,,0.0,,"{'adamw': {'lr': 0.02089296130854041, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.9426332712173462,0.0,,,,,
111,,,,,,,,,,,,,0.06322985887527466,,0.8346121907234192,,1.109493374824524,,,,,,,,,,,,,,,148.3109631538391,5,1695916257.2009692,{'runtime': 399},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5384550094604492,,0.529077410697937,,,,,,0.697196364402771,0.612122654914856,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,0.05,,,,,bert-base-cased,worthy-dew-840,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0988130569458008,0.0,,,,,
112,,,,,,,,,,,,,-0.04450731724500656,,5.785463333129883,,36.80185317993164,,,,,,,,,,,,,,,616.0114815235138,0,1695915614.0803325,{'runtime': 539},/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5192338824272156,,0.5,,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,,,,,bert-base-cased,fine-resonance-833,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,
113,0.18346676230430603,,0.27167752385139465,,0.12030363082885742,,,,,,,,,,,,,,,,,,,,,,,,,,,1582.567322254181,27,1695915007.5696852,{'runtime': 1582},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.48935794830322266,,0.5002595782279968,,,,,,0.5229164361953735,0.4996379315853119,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 11}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 11}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 12}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 12}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,1.9054607179632464e-05,,,,,bert-large-cased,brisk-night-832,,,,0.0,,"{'adamw': {'lr': 1.9054607179632464e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.14243894815444946,0.0,,,,,
114,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2909.6216175556183,56,1695913164.6330445,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5023490190505981,0.4996379315853119,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 23}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 23}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,9.0,False,,True,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,0.05248074602497723,,,,,bert-large-cased,eternal-spaceship-831,,,,0.0,,"{'adamw': {'lr': 0.05248074602497723, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.6752214431762695,449.0,,,,,
115,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,348.30588936805725,8,1695908289.6282885,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5057923197746277,0.5003620386123657,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 15}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 15}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 30}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 15}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 15}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,8.0,False,,False,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,0.006918309709189364,,,,,bert-large-cased,lyric-glade-825,,,,0.0,,"{'adamw': {'lr': 0.006918309709189364, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 30, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.15905052423477173,71.0,,,,,
116,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,903.5377411842346,4,1695907911.376351,{'runtime': 903},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5653040409088135,,0.5338348746299744,,,,,,0.7707415223121643,0.6738196611404419,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 500}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 500}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 500}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,8.0,False,,False,,,,,,0.006918309709189364,,,,,bert-large-cased,zesty-butterfly-824,,,,0.0,,"{'adamw': {'lr': 0.006918309709189364, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 500, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,high,,0.6976571083068848,97.0,,,,,
117,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,325.2444648742676,4,1695905918.2316968,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7226423025131226,0.5498076677322388,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 75}, 'train': {'shuffle': True, 'slicing': '[:5000]', 'batch_size': 75}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 75}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,3.0,False,,False,,,,,,0.006918309709189364,,,,,bert-large-cased,efficient-tree-818,,,,0.0,,"{'adamw': {'lr': 0.006918309709189364, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 75, 'token_location': 0, 'lr_warm_up_steps': 500, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6626976728439331,99.0,,,,,
118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1650.5821251869202,22,1695904842.9935572,{'runtime': 1650},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5709611773490906,,0.5400350093841553,,,,,,0.746946394443512,0.665782630443573,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 100}, 'train': {'shuffle': True, 'slicing': '[:2000]', 'batch_size': 100}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 100}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,30.0,False,,True,,,,,,0.006918309709189364,,,,,bert-large-cased,stoic-bird-816,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 100, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.3938957750797272,300.0,,,,,
119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,676.3929615020752,8,1695903140.9158094,{'runtime': 675},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5700711607933044,,0.5406560301780701,,,,,,0.7241909503936768,0.5758435130119324,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:2000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': True, 'slicing': '[:3000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,8.0,False,,True,,,,,,0.030199517204020192,,,,,bert-large-cased,fanciful-dream-815,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 50, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7060462236404419,173.0,,,,,
120,,,,,,,,,,,,,,0.0504433736205101,,0.8687589168548584,,1.196888446807861,,,,,,,,,,,,,,627.6940457820892,25,1695900015.9641268,{'runtime': 627},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5167199969291687,,0.5205366015434265,,,,,0.6034426093101501,0.5473920702934265,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,24.0,False,,True,,,,,,0.00017378008287493763,,,,,bert-base-cased,morning-sky-813,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,,,,,,49.0,,,,,
121,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3687.020716190338,0,1695812287.8543513,{'runtime': 3686},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5474254488945007,,0.5351073741912842,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': True, 'slicing': '[:3000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,1.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,bert-large-cased,tough-armadillo-794,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 40, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,45.0,,,,,
122,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,233.8168678283691,2,1695807819.112634,{'runtime': 301},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6825017333030701,0.5628098249435425,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,,False,,,,,,,0.05,,,,,bert-base-cased,silvery-sea-792,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.3583414554595947,49.0,,,,,
123,,,,,,,,,,,,,,-0.03781667351722717,,2.1649656295776367,,6.30921745300293,,,,,,,,,,,,,,886.7482504844666,0,1695733962.6445844,{'runtime': 884},/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5214177370071411,,0.5051379203796387,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,,False,,,,,,,,,,,,bert-base-cased,still-totem-789,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,,,,,
124,,,,,,,,,,,,,,-0.03882410749793053,,4.179725170135498,,19.250049591064453,,,,,,,,,,,,,,525.3575637340546,0,1695732774.3778398,{'runtime': 523},/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5135076642036438,,0.5034927725791931,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,,False,,,,,,,,,,,,bert-base-cased,vital-wildflower-788,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,
125,,0.13109958171844482,,0.279828280210495,,0.12720514833927157,,,,,,,,,,,,,,,,,,,,,,,,,,31771.094570159912,1545,1695712748.4413905,{'runtime': 31772},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5764083862304688,,0.5119072198867798,,,,,0.6168159246444702,0.5186913013458252,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': True, 'slicing': '[:3000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,279.0,,True,,,artifacts/train_head_on_ethics_bert_large_end.ckpt,,,,9.120108393559096e-06,,,,,bert-large-cased,sparkling-cherry-787,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 300, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 50, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.11895427107810974,1674.0,,,,,
126,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14880.22339630127,1283,1695680949.2685442,{'runtime': 14880},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5666467547416687,,0.5451452732086182,,,,,,0.6822502017021179,0.6563328504562378,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': True, 'slicing': '[:3000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,100.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,0.2754228703338169,,,,,bert-large-cased,dulcet-durian-786,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 50, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.000676998752169311,1500.0,,,,,
127,,0.18111491203308103,,0.27191808819770813,,0.1204804927110672,,,,,,,,,,,,,,,,,,,,,,,,,,11291.633395910265,645,1695664746.915395,{'runtime': 11292},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5621246695518494,,0.5304581522941589,,,,,0.6690197587013245,0.5695752501487732,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': True, 'slicing': '[:3000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,100.0,,True,,,artifacts/train_head_on_ethics_bert_large_end.ckpt,,,,1.947360525085198e-06,,,,,bert-large-cased,elated-glitter-785,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.11874288320541382,2800.0,,,,,
128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5881.73924446106,651,1695653431.9753654,{'runtime': 5881},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5680338740348816,,0.527925431728363,,,,,,0.7191347479820251,0.5907252430915833,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': True, 'slicing': '[:3000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,40.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,0.007675056068023484,,,,,bert-large-cased,curious-night-784,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 40, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5061703324317932,2920.0,,,,,
129,,0.16580268740653992,,0.2736184597015381,,0.12191083282232285,,,,,,,,,,,,,,,,,,,,,,,,,,10514.687022686005,701,1695644760.0831127,{'runtime': 10515},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5824858546257019,,0.5400397181510925,,,,,0.8027081489562988,0.6545830965042114,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,100.0,,True,,,,,,,5.2932430267575966e-08,,,,,bert-large-cased,lyric-monkey-782,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.08276480436325073,5600.0,,,,,
130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4170.205071687698,523,1695634215.1426027,{'runtime': 4169},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.579323410987854,,0.529625415802002,,,,,,0.8337500095367432,0.6786456108093262,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,30.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,8.115635054534986e-05,,,,,bert-large-cased,usual-dream-781,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.056493084877729416,4350.0,,,,,
131,,,,,,,,,,,,,,-0.021779296919703484,,1.1638058423995972,,2.147585153579712,,,,,,,,,,,,,,132.76133966445923,20,1695629915.0566278,{'runtime': 132},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5098560452461243,,0.49983611702919006,,,,,0.5234035849571228,0.49791303277015686,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,4.0,,False,,,,,,,0.05,,,,,bert-base-cased,winter-bee-780,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.964114785194397,439.0,,,,,
132,,,,,,,,,,,,,,0.0362970232963562,,1.092170476913452,,1.8966537714004517,,,,,,,,,,,,,,144.43823981285095,15,1695626934.6623738,{'runtime': 143},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5349443554878235,,0.5168223977088928,,,,,0.7317860722541809,0.5325327515602112,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,3.0,,False,,,,,,,0.05,,,,,bert-base-cased,floral-shape-777,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.741474986076355,303.0,,,,,
133,,,,,,,,,,,,,,,,1.0751209259033203,,1.8467315435409544,,,,,,,,,,,,,,180.44248795509336,20,1695626584.307257,{'runtime': 169},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5379254221916199,,0.5046797394752502,,,,,0.7342519164085388,0.6491926312446594,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,4.0,,False,,,,,,,0.05,,,,,bert-base-cased,generous-gorge-776,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.8797569274902344,407.0,,,,,
134,,,,,,,,,,,,,,,,,NaN,1.9120043516159055,,,,,,,NaN,,,,,,,199.76384830474856,27,1695623454.6324582,{'runtime': 199},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,0.5199999809265137,,,,,,0.5929999947547913,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,5.0,,False,,,,,,,0.05,,,,,bert-base-cased,dauntless-breeze-774,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.9747997522354128,576.0,,,,,
135,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,154.69078946113586,19,1695623132.3601174,{'runtime': 153},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,3.0,,False,,,,,,,0.05,,,,,bert-base-cased,splendid-wind-773,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,NaN,1.6387945413589478,0.0,0.5419999957084656,,,,,,,1.9276965856552124,399.0,,,NaN,0.6060000061988831,
136,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,222.38172507286072,32,1695622923.4622371,{'runtime': 221},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,6.0,,False,,,,,,,0.05,,,,,bert-base-cased,fancy-dust-772,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,NaN,1.822555899620056,0.0,0.5339999794960022,,,,,,,1.780839920043945,672.0,,,NaN,0.6800000071525574,
137,,,,,,0.1206127032637596,,,,,,,,,,,,,,,,,,,,,,,,,,5695.118129253387,202,1695466441.5549312,{'runtime': 5695},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4839999973773956,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,40.0,,True,,,,,,,6.3095734448019305e-06,,,,,bert-large-cased,tough-firefly-764,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 100, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.12560279667377472,122.0,,,,,
138,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2843.2788376808167,325,1695460721.0735986,{'runtime': 2842},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4970000088214874,,,,,,,0.4704999923706054,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,20.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,1.799351772668518e-07,,,,,bert-large-cased,easy-sunset-763,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6846752166748047,1460.0,,,,,
139,,,,,,0.1316504180431366,,,,,,,,,,,,,,,,,,,,,,,,,,2629.571589708328,101,1695457674.8792715,{'runtime': 2629},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.48500001430511475,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,30.0,,False,,,,,,,1.20226443461741e-06,,,,,bert-large-cased,whole-fog-762,,,,0.0,,"{'adamw': {'lr': 1.20226443461741e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 100, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1274394392967224,90.0,,,,,
140,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2676.0028533935547,325,1695455018.1165605,{'runtime': 2675},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4790000021457672,,,,,,,0.4194999933242798,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,20.0,,False,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,3.126915001754998e-07,,,,,bert-large-cased,balmy-silence-761,,,,0.0,,"{'adamw': {'lr': 1.20226443461741e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7651711106300354,1460.0,,,,,
141,,,,,,,,,,,,,,,,,,1.780733585357666,,,,,,,,,,,,,,165.69549560546875,20,1695452113.0425267,{'runtime': 165},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5220000147819519,,,,,,0.6060000061988831,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,4.0,,False,,,,,,,0.05,,,,,bert-base-cased,solar-oath-760,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.9197285175323489,409.0,,,,,
142,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,906.7631061077118,3,1695413279.587012,{'runtime': 919},/Users/ajmeek/PycharmProjects/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/Users/ajmeek/PycharmProjects/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,,False,,,,,,,0.05,,,,,bert-base-cased,lemon-cloud-757,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.368671178817749,99.0,,,,,
143,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8255.37435722351,563,1695415095.6260562,{'runtime': 8315},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,99.0,,False,,,,,,,9.932731534343352e-07,,,,,bert-large-cased,dazzling-water-756,,,,0.0,,"{'adamw': {'lr': 1.20226443461741e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 100, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.13831156492233276,299.0,,,,,
144,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2325.5034594535828,318,1695406775.0883515,{'runtime': 2358},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,19.0,,False,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,3.126915001754998e-07,,,,,bert-large-cased,worldly-spaceship-755,,,,0.0,,"{'adamw': {'lr': 1.20226443461741e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6883053779602051,1449.0,,,,,
145,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13471.061417102814,248,1695359959.047482,{'runtime': 13470},/Users/ajmeek/PycharmProjects/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/Users/ajmeek/PycharmProjects/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,False,,True,input,label,cross_entropy,commonsense,hendrycks/ethics,refs/pr/3,50.0,False,[:1000],50.0,True,[:1000],50.0,False,[:1000],,True,input,label,mse_loss,LFB-LAST,data/ds000212/ds000212_lfb,,,,2.0,False,,,,10.0,,False,,,,,,,0.05,,,,,bert-base-cased,comfy-firefly-747,,,,,,,"[0.9, 0.999]",1e-08,0.05,0.01,10000.0,,True,,,,,,,,,0.1,False,0.99,500.0,0.0,False,,1.0,1.0,False,1.0,1.0,1.0,,10.0,-1.0,,,,,0.0,32-true,1.0,,,,,,,,,,,,,,,0.531000018119812,False,,,5.9371538162231445,5950.0,,0.5960000157356262,,,
146,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11062.482272863388,564,1695324626.3379989,{'runtime': 11062},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,100.0,,True,,,,,,,6.871771301281917e-05,,,,,bert-large-cased,distinctive-wildflower-746,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 100, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,0.12159088253974916,,,,,,,0.48500001430511475,,,,,0.09228354692459106,300.0,,,,,
147,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3041.7673218250275,325,1695313528.302616,{'runtime': 3041},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,20.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,0.05958213704254145,,,,,bert-large-cased,zesty-voice-745,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5490000247955322,,,,,,1.5244423151016235,1460.0,,,,,0.6840000152587891
148,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1220.8585183620453,44,1695310261.9620864,{'runtime': 1220},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,8.0,,True,,,,,,,1.2042502672828112e-05,,,,,bert-large-cased,lilac-terrain-744,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,0.12439057976007462,,,,,,,0.5189999938011169,,,,,0.12430745363235474,205.0,,,,,
149,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,927.5932185649872,78,1695309019.4552195,{'runtime': 925},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,5.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,2.601624838898515e-07,,,,,bert-large-cased,efficient-shadow-743,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 5, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,,,,,,0.5858181118965149,365.0,,,,,0.534500002861023
150,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,282.06272315979004,22,1695307833.85839,{'runtime': 281},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,5.0,,True,,,,,,,0.00017378008287493763,,,,,bert-base-cased,sleek-plant-742,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 5, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,0.12454412132501602,,,,,,,0.515999972820282,,,,,0.09089798480272292,125.0,,,,,
151,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,317.1926975250244,78,1695306645.5492623,{'runtime': 316},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,5.0,,True,,,artifacts/train_head_on_ethics.ckpt,,,,0.006534986132707475,,,,,bert-base-cased,lemon-planet-741,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 5, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5289999842643738,,,,,,0.5807023048400879,365.0,,,,,0.6294999718666077
152,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,319.08488965034485,78,1695306114.2577317,{'runtime': 318},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,5.0,,True,,,artifacts/train_head_on_ethics.ckpt,,,,0.07165469806611827,,,,,bert-base-cased,jolly-durian-739,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 5, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5320000052452087,,,,,,1.7917693853378296,365.0,,,,,0.5504999756813049
153,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,118.01417231559752,12,1695304525.0364134,{'runtime': 117},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,2.0,,False,,,artifacts/checkpoint.ckpt,,,,0.05,,,,,bert-base-cased,breezy-moon-735,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,1.6752569675445557,,,,,,0.5199999809265137,,,,,1.7462605237960815,296.0,,,,,0.609000027179718
154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,66.42742347717285,12,1695303731.3560965,{'runtime': 65},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,,False,,,,,,,0.05,,,,,bert-base-cased,lively-planet-734,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,5.138019561767578,,,,,,0.5230000019073486,,False,,,6.112844467163086,345.0,,,,,
155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,79.23234558105469,23,1695303579.3034346,{'runtime': 78},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,1.0,,False,,,,,,,0.05,,,,,bert-base-cased,royal-voice-733,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5299999713897705,,,False,,,5.07367467880249,574.0,,,,,0.6290000081062317
156,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,106.0059552192688,50,1695298921.7891562,{'runtime': 105},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,2.0,,False,,,,,,,0.05,,,,,bert-base-cased,polished-bush-732,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,,,False,,,7.864725112915039,1229.0,0.6349999904632568,,,,
157,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,113.37168598175047,23,1695226221.102116,{'runtime': 112},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,1.0,,False,,,,,,,0.05,,,,,bert-base-cased,floral-capybara-720,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5230000019073486,True,,,6.108151912689209,595.0,,0.5360000133514404,,,
158,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12975.603532075882,410,1695146075.702577,{'runtime': 12975},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-SENTENCES', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:4000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,20.0,,False,,,,,,,0.0009135172474836408,,,,,bert-large-cased,kind-voice-719,,,,,,"{'adamw': {'lr': 0.001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 100, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,0.515999972820282,False,,,0.8469237089157104,200.0,,0.5339999794960022,,,
159,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,983.3234541416168,296,1695131975.8865912,{'runtime': 982},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,274.0,,True,,,,,,,0.0001445439770745928,,,,,bert-base-cased,resilient-wind-716,,,,,,"{'adamw': {'lr': 0.0005, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,,,,,,,,,0.5189999938011169,False,,,0.007556550204753876,549.0,,0.5299999713897705,,,
160,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16023.36880993843,585,1695128322.472525,{'runtime': 16023},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,100.0,,False,,,,,,,1.0764333400476943e-06,,,,,bert-large-cased,bumbling-waterfall-715,,,,,,"{'adamw': {'lr': 1.20226443461741e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 8, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5730000138282776,False,,,0.1220652163028717,3200.0,,0.7540000081062317,,,
161,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2231.163437604904,133,1695103131.6274097,{'runtime': 2230},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,0.15848931924611143,,,,,bert-large-cased,serene-rain-713,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 4, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,False,,,0.3919190168380738,1250.0,,0.5299999713897705,,,
162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4563.24374127388,146,1695100723.9822452,{'runtime': 4563},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,1.2022644346174132e-06,,,,,bert-large-cased,cosmic-cosmos-709,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 8, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5600000023841858,False,,,0.08935826271772385,800.0,,0.7350000143051147,,,
163,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4317.194122552872,163,1695096142.310567,{'runtime': 4316},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,0.00012022644346174132,,,,,bert-large-cased,rose-silence-708,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 4, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,False,,,0.818365216255188,1575.0,,0.5299999713897705,,,
164,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4264.367608785629,194,1695091804.8772118,{'runtime': 4264},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,2.2908676527677725e-05,,,,,bert-large-cased,youthful-deluge-707,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 2, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5649999976158142,False,,,0.11295507103204729,3125.0,,0.7360000014305115,,,
165,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4564.374855518341,146,1695087519.7458274,{'runtime': 4564},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-MIDDLE', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,3.0199517204020163e-06,,,,,bert-large-cased,worldly-capybara-706,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 8, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5699999928474426,False,,,0.7993297576904297,800.0,,0.7450000047683716,,,
166,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4325.295979976654,163,1695082938.052607,{'runtime': 4325},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-MIDDLE', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,0.00012022644346174132,,,,,bert-large-cased,expert-plasma-705,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 4, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,False,,,1.7454555034637451,1575.0,,0.5299999713897705,,,
167,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4266.319978713989,194,1695078594.1973836,{'runtime': 4266},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-MIDDLE', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,3.311311214825911e-05,,,,,bert-large-cased,dazzling-plasma-704,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 2, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,False,,,1.6050455570220947,3125.0,,0.5299999713897705,,,
168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4579.343895196915,146,1695074303.6230133,{'runtime': 4578},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,1.2022644346174132e-06,,,,,bert-large-cased,brisk-voice-703,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 8, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5759999752044678,False,,,0.7256357073783875,800.0,,0.7400000095367432,,,
169,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4325.839015722275,163,1695069702.9339907,{'runtime': 4325},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,0.00017378008287493763,,,,,bert-large-cased,breezy-sunset-702,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 4, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,False,,,1.4152942895889282,1575.0,,0.5299999713897705,,,
170,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4295.264527082443,194,1695065355.8873382,{'runtime': 4294},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,2.2908676527677725e-05,,,,,bert-large-cased,unique-aardvark-701,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 2, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5559999942779541,False,,,0.9122304916381836,3125.0,,0.7120000123977661,,,
171,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,359.32848930358887,8,1695060684.1732702,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,1.0,,True,,,,,,,0.000630957344480193,,,,,bert-large-cased,fanciful-butterfly-699,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 2, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.3002092838287354,149.0,,,,,
172,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8847.339977025986,543,1695056872.209184,{'runtime': 8857},artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': False, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,99.0,,True,,,,,,,1.9498445997580452e-05,,,,,bert-large-cased,light-dawn-698,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 7500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 7, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,0.7847325801849365,3399.0,,,,,
173,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,399.51161313056946,7,1694871283.9945242,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': False, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,1.0,,True,,,,,,,0.07585775750291836,,,,,bert-large-cased,major-shape-694,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 7500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 7, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,0.5849629640579224,49.0,,,,,
174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4329.159880876541,127,1694870863.72655,{'runtime': 4329},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,22.0,,True,,,,,,,2.089296130854039e-06,,,,,bert-large-cased,blooming-moon-689,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 7500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 7, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5709999799728394,False,,,0.8467081189155579,776.0,,0.7459999918937683,,,
175,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1763.7730643749237,105,1694863665.0467105,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[-500:]', 'batch_size': 1}, 'train': {'shuffle': True, 'slicing': '[-1000:]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[-500:]', 'batch_size': 1}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 1}, 'enable': False, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,5.0,,True,,,,,,,0.04365158322401657,,,,,microsoft/deberta-v2-xlarge,dazzling-wind-684,,,,,,"{'adamw': {'lr': 0.0002, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 15, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.0513103008270264,349.0,,0.4664879441261291,,,
176,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4626.079259395599,181,1694861872.3068354,{'runtime': 4627},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[-500:]', 'batch_size': 1}, 'train': {'shuffle': True, 'slicing': '[-1000:]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[-500:]', 'batch_size': 1}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,10.0,,True,,,,,,,1.0964781961431852e-05,,,,,microsoft/deberta-v2-xlarge,breezy-eon-683,,,,,,"{'adamw': {'lr': 0.0002, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 15, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.484375,False,,,0.8154250383377075,600.0,,0.6997318863868713,,,
177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2196.409217119217,176,1694851310.510277,{'runtime': 2030},artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': False, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,4.0,,,,,,,,,3.8878391807422704e-05,,,,,bert-large-cased,hopeful-lake-672,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 4, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.9113208651542664,False,,,0.7350736260414124,1472.0,,,,,
178,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6599.947950363159,408,1694797106.1375422,{'runtime': 6600},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': False, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,83.0,,True,,,,,,,0.030199517204020192,,,,,bert-large-cased,deft-terrain-668,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 7500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 7, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5199999809265137,False,,,0.24146950244903564,2435.0,,0.4659999907016754,,,
179,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21388.017303705215,799,1694796649.1123157,{'runtime': 21886},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[-500:]', 'batch_size': 1}, 'train': {'shuffle': True, 'slicing': '[-1000:]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[-500:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,16.0,,True,,,,,,,0.006918309709189364,,,,,microsoft/deberta-v2-xlarge,leafy-pine-667,,,,,,"{'adamw': {'lr': 0.0002, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 15, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.504544734954834,2499.0,,0.5335120558738708,,,
180,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13768.85400533676,494,1694774810.4557483,{'runtime': 13769},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[-500:]', 'batch_size': 1}, 'train': {'shuffle': True, 'slicing': '[-1000:]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[-500:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,10.0,,True,,,,,,,0.001584893192461114,,,,,microsoft/deberta-v2-xlarge,radiant-leaf-665,,,,,,"{'adamw': {'lr': 0.0002, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 15, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.453125,False,,,0.8152824640274048,1550.0,,0.5335120558738708,,,
181,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33561.041081905365,1095,1694790483.013324,{'runtime': 33560},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,100.0,,True,,,,,,,7.585775750291836e-06,,,,,bert-large-cased,pretty-dragon-661,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 7500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 7, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.550000011920929,False,,,0.8401982188224792,6700.0,,0.7519999742507935,,,
182,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,109.50155782699584,600,1694738064.2245388,{'runtime': 108},/workspace/brainbias/artifacts,5.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,120.0,,,,,,['mse'],"[1, 1]",0.0006538379548447884,,,,,,,zesty-hill-659,120.0,10000.0,10000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,1.0756090879440308,,,['DS000212_LFB_Dataset'],0.8862398266792297,600.0,,,,,
183,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,333.45714712142944,1199,1694737799.325046,,/workspace/brainbias/artifacts,5.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,119.0,,,,,,['mse'],"[1, 1]",0.0006538379548447884,,,,,,,curious-durian-658,120.0,10000.0,10000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,,,,['DS000212_LFB_Dataset'],0.859306812286377,599.0,,,,,
184,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,281.85196018218994,600,1694737430.2274091,{'runtime': 126},/workspace/brainbias/artifacts,5.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,120.0,,,,,,['mse'],"[1, 1]",0.0006538379548447884,,,,,,,driven-forest-657,120.0,10000.0,10000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,1.0849390029907229,,,['DS000212_LFB_Dataset'],0.952916145324707,600.0,,,,,
185,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3476.682554244995,353,1694734937.6528363,{'runtime': 3476},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:400]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:400]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'enable': False, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,100.0,,True,,,,,,,0.0010349590933006062,,,,,bert-large-cased,comfy-pine-656,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5199999809265137,False,,,0.5493258237838745,8000.0,,0.5325000286102295,,,
186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.8928141593933105,4,1694706412.474908,,/workspace/brainbias/artifacts,5.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,['mse'],"[1, 1]",0.0006538379548447884,,,,,,,wild-dew-655,1.0,10000.0,10000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,,,,['DS000212_LFB_Dataset'],1.0599699020385742,4.0,,,,,
187,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13.16309094429016,4,1694705519.30897,,/workspace/brainbias/artifacts,5.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,['mse'],"[1, 1]",0.0006538379548447884,,,,,,,blooming-dawn-651,1.0,10000.0,10000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,,,,['DS000212_LFB_Dataset'],1.056660532951355,4.0,,,,,
188,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,205.98233342170715,10,1694702768.666711,,/workspace/brainbias/artifacts,5.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,['mse'],"[1, 1]",0.0006538379548447884,,,,,,,fragrant-wave-648,1.0,-1.0,-1.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,,,,['DS000212_LFB_Dataset'],1.126307487487793,4.0,,,,,
189,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29650.626119852062,1889,1694731437.9504218,{'runtime': 29650},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:400]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:400]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,100.0,,True,,,,,,,4.1121641186780494e-25,,,,,bert-large-cased,serene-armadillo-647,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5099999904632568,False,,,0.8650360107421875,46400.0,,0.6800000071525574,,,
190,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,564.827234506607,29,1694701678.1164696,{'runtime': 593},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:200]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:200]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,1.0,,True,,,,,,,2.163937286996608e-05,,,,,bert-large-cased,pious-universe-646,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.2942802906036377,749.0,,,,,
191,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,393.3537392616272,26,1694700038.389501,{'runtime': 393},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,True,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,2.0,,,,,,,,,0.0005,,,,,bert-base-cased,stoic-haze-641,,,,,,"{'adamw': {'lr': 0.0005, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 0, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,False,,,0.9388046264648438,625.0,,0.5299999713897705,,,
192,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,528.6731519699097,37,1694699548.392306,{'runtime': 528},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,True,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,3.0,,,,,,,,,0.0005,,,,,bert-base-cased,glamorous-disco-640,,,,,,"{'adamw': {'lr': 0.0005, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 0, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,False,,,0.44301271438598633,890.0,,0.5299999713897705,,,
193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,610.4560539722443,96,1694696301.306927,{'runtime': 610},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,True,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,90.0,,,,,,,,0.0005000000237487257,0.0005,,,,,bert-base-cased,colorful-sponge-638,,,,,,"{'adamw': {'lr': 0.0005, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 0, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,False,,,1.4157085418701172,181.0,,0.5099999904632568,,,
194,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,630.4593250751495,62,1694695440.963737,{'runtime': 630},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,True,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,58.0,,,,,,,,,0.0005,,,,,bert-base-cased,flowing-shape-635,,,,,,"{'adamw': {'lr': 0.0005, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 0, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,False,,,1.4192135334014893,116.0,,0.5099999904632568,,,
195,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7602.054200410843,22,1694702378.1364744,,/workspace/brainbias/artifacts,5.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,['mse'],"[1, 1]",0.0006538379548447884,,,,,,,colorful-voice-634,1.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,,,,['DS000212_LFB_Dataset'],1.096705675125122,0.0,,,,,
196,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,450.29399609565735,40,1694694713.2914531,{'runtime': 449},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,38.0,,,,,,,,,0.0005,,,,,bert-base-cased,rosy-silence-632,,,,,,"{'adamw': {'lr': 0.0005, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 0, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,False,,,1.4307936429977417,76.0,,0.5099999904632568,,,
197,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,475.38592004776,71,1694694092.432484,{'runtime': 475},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,67.0,,,,,,,,,0.05,,,,,bert-base-cased,lucky-forest-631,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 0, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.515999972820282,False,,,1.6207704544067385,134.0,,0.5099999904632568,,,
198,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28.06265234947205,19,1694690154.7864945,,/workspace/brainbias/artifacts,5.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,['mse'],"[1, 1]",0.0006538379548447884,,,,,,,treasured-jazz-630,4.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,,,,['DS000212_LFB_Dataset'],0.941938579082489,19.0,,,,,
199,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7609.680545568466,445,1694689838.737609,{'runtime': 7611},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:500]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,23.0,,,,,,,,,0.00926510094425921,,,,,bert-large-cased,quiet-salad-626,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.9, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 75, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5199999809265137,False,,,0.8246591687202454,10959.0,,0.5339999794960022,,,
200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5999.709806442261,348,1694682074.1177003,{'runtime': 6001},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:500]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,18.0,,,,,,,,,0.00093206534790699,,,,,bert-large-cased,solar-paper-624,,,,,,"{'adamw': {'lr': 0.001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 75, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5199999809265137,False,,,1.0207709074020386,8597.0,,0.5339999794960022,,,
201,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1798.0449018478394,108,1694675452.201084,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:500]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': False, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,46.0,,,,,,,,,0.001,,,,,bert-large-cased,desert-thunder-623,,,,,,"{'adamw': {'lr': 0.001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 75, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,0.7244374752044678,2349.0,,0.5339999794960022,,,
202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,47400.09772133827,2811,1694673132.1682844,{'runtime': 47401},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:500]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,148.0,,,,,,,,,1.1163135664444592e-32,,,,,bert-large-cased,resilient-snowflake-620,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5199999809265137,False,,,0.8919395804405212,69083.0,,0.5339999794960022,,,
203,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,640.5196290016174,35,1694625647.942229,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:500]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,1.0,,,,,,,,,6.757290490602833e-05,,,,,bert-large-cased,earthy-donkey-619,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,0.863381564617157,899.0,,,,,
204,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1241.207018136978,52,1694623278.4101412,{'runtime': 1254},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:100]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,99.0,,,,,,,,,8.26168623835587e-05,,,,,bert-large-cased,toasty-meadow-618,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 300}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 5, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.0496779680252075,499.0,,0.5400000214576721,,,
205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1083.7759268283844,39,1694621830.668202,{'runtime': 1091},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:50]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:50]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,99.0,,,,,,,,,0.002355064348623125,,,,,bert-base-cased,ethereal-snow-617,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 200}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 5, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 5}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,0.10054581612348557,499.0,,0.6200000047683716,,,
206,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,555.8120539188385,31,1694620650.957609,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:50]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:50]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,39.0,,,,,,,,,0.05000000000000001,,,,,bert-base-cased,fresh-breeze-616,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 200}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 5}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,3.2119061946868896,599.0,,0.6200000047683716,,,
207,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,103.39229798316956,11,1694619856.150327,{'runtime': 129},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:50]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:50]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,0.0,,,,,,,,,0.05000000000000001,,,,,bert-base-cased,dandy-jazz-614,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 20}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 5}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.857195258140564,299.0,,,,,
208,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8252.080195903778,599,1694608756.888286,{'runtime': 8312},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:200]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:200]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,31.0,,,,,,,,,9.999999999999996e-05,,,,,bert-large-cased,breezy-donkey-611,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,0.8129327893257141,14749.0,,0.5249999761581421,,,
209,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,556.1202301979065,18,1694599496.9463751,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:200]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:200]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,bert-large-cased,honest-sound-604,,,,,,"{'adamw': {'lr': 0.009, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.595057249069214,949.0,,,,,
210,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1837.133608341217,58,1694598279.4997094,{'runtime': 1876},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:2000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,bert-large-cased,hopeful-sound-599,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,0.9697286486625672,2949.0,,,,,
211,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,412.3695046901703,60,1694596122.8819456,{'runtime': 416},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,58.0,,,,,,,,,,,,,,bert-base-cased,usual-meadow-598,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 0, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,0.29070496559143066,117.0,,0.550000011920929,,,
212,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,354.5366461277008,2,1694593855.3655252,{'runtime': 372},/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,,True,input,label,cross_entropy,commonsense,hendrycks/ethics,refs/pr/3,50.0,False,[:1000],50.0,True,[:1000],50.0,False,[:1000],,True,input,label,mse_loss,learning_from_brains,data/ds000212,,LAST,,2.0,False,,,,2.0,,,,,,,,,,,,,,bert-base-cased,glowing-donkey-594,,,,,,,"[0.9, 0.999]",1e-08,0.001,0.01,10000.0,,True,,,,,,,,,0.1,False,,,0.0,False,,,1.0,False,1.0,1.0,0.0,,100.0,-1.0,,,,,,32-true,1.0,,,,,,,,,,,,,,,,False,,,,5.0,,0.4900000095367432,,,
213,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,588.2305908203125,1,1694590875.2967918,{'runtime': 586},/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,,True,input,label,cross_entropy,commonsense,hendrycks/ethics,refs/pr/3,50.0,False,[:1000],50.0,True,[:1000],50.0,False,[:1000],,True,input,label,mse_loss,learning_from_brains,data/ds000212,,LAST,,2.0,False,,,,1.0,,,,,,,,,,,,,,bert-base-cased,different-dust-591,,,,,,,"[0.9, 0.999]",1e-08,0.001,0.01,10000.0,,True,,,,,,,,,0.1,False,,,0.0,False,,,1.0,False,1.0,1.0,1.0,,1.0,-1.0,,,,,,32-true,1.0,,,,,,,,,,,,,,,0.5180000066757202,False,,,,2.0,,0.5,,,
214,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,351.6646592617035,2,1694589344.0712423,{'runtime': 348},/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,,True,input,label,cross_entropy,commonsense,hendrycks/ethics,refs/pr/3,50.0,False,[:100],50.0,True,[:100],50.0,False,[:100],,True,input,label,mse_loss,learning_from_brains,data/ds000212,,LAST,,2.0,False,,,,0.0,,,,,,,,,,,,,,bert-base-cased,brisk-frost-588,,,,,,,"[0.9, 0.999]",1e-08,0.001,0.01,10000.0,,True,,,,,,,,,0.1,False,,,0.0,False,,,1.0,False,1.0,1.0,1.0,,1.0,-1.0,,,,,0.0,32-true,1.0,,,,,,,,,,,,,,,0.4900000095367432,False,,,1.499633550643921,102.0,,,,,
215,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,78.70291304588318,24,1694588658.584622,{'runtime': 77},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:100]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,bert-base-cased,efficient-frost-586,,,,,,"{'adamw': {'lr': 0.001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5199999809265137,False,,,0.8132522106170654,1160.0,,0.5099999904632568,,,
216,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,583.3752310276031,9,1694588266.614244,{'runtime': 580},/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,,True,input,label,cross_entropy,commonsense,hendrycks/ethics,refs/pr/3,50.0,False,[:100],50.0,True,[:100],50.0,False,[:100],,True,input,label,mse_loss,learning_from_brains,data/ds000212,,LAST,,2.0,False,,,,0.0,,,,,,,,,,,,,,bert-base-cased,eternal-pond-584,,,,,,,"[0.9, 0.999]",1e-08,0.001,0.01,10000.0,,True,,,,,,,,,0.1,False,,,0.0,False,,,1.0,False,1.0,1.0,1.0,,1.0,-1.0,,,,,0.0,32-true,1.0,,,,,,,,,,,,,,,0.5199999809265137,False,,,1.3249666690826416,461.0,,,,,
217,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,255.35976839065552,35,1694511669.9691424,,artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:100]', 'batch_size': 50}, 'enable': False, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,bert-large-cased,feasible-feather-557,,,,,,"{'adamw': {'lr': 0.002, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,True,,,0.8891825675964355,1799.0,,,,,
218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,35719.140535354614,929,1694490319.3618894,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,64.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,colorful-energy-549,,,,,,"{'adamw': {'lr': 0.0002, 'eps': 1e-05, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 400, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 500, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 700, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.2627606391906738,45449.0,,0.5299999713897705,,,
219,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1356.2867658138275,34,1694454398.479447,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,skilled-sound-548,,,,,,"{'adamw': {'lr': 0.0002, 'eps': 1e-05, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 400, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 500, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 700, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.8882368803024292,1749.0,,,,,
220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1877.7357516288755,21,1694446619.1686597,,artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,35.0,,,,,,,,,,,,,,bert-large-cased,copper-flower-542,,,,,,"{'adamw': {'lr': 0.002, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_ReduceLROnPlateau': False, 'lr_scheduler_interval': 'epoch', 'lr_scheduler_frequency': 10, 'reduceLROnPlateau_config': {'eps': 1e-08, 'factor': 0.1, 'min_lr': 0, 'verbose': True, 'cooldown': 0, 'patience': 10}}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,True,,,1.721357345581055,539.0,,0.5183553695678711,,,
221,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,437.897269487381,18,1694415097.2392254,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'enable': False, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,chocolate-universe-541,,,,,,"{'adamw': {'lr': 0.0002, 'eps': 1e-05, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_ReduceLROnPlateau': True, 'lr_scheduler_interval': 'epoch', 'lr_scheduler_frequency': 10, 'reduceLROnPlateau_config': {'eps': 1e-08, 'factor': 0.1, 'min_lr': 0, 'verbose': True, 'cooldown': 0, 'patience': 10}}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 400, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 500, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 700, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,0.5683512091636658,949.0,,,,,
222,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,51536.833899497986,632,1694383256.9248154,{'runtime': 51554},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,999.0,,,,,,,,,,,,,,bert-large-cased,lunar-brook-540,,,,,,"{'adamw': {'lr': 0.002, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_ReduceLROnPlateau': False, 'lr_scheduler_interval': 'epoch', 'lr_scheduler_frequency': 10, 'reduceLROnPlateau_config': {'eps': 1e-08, 'factor': 0.1, 'min_lr': 0, 'verbose': True, 'cooldown': 0, 'patience': 10}}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,True,,,1.5871541500091553,14999.0,,0.5183553695678711,,,
223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,145018.21653842926,3582,1694414534.3075185,{'runtime': 145104},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,249.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,earthy-sun-539,,,,,,"{'adamw': {'lr': 0.0002, 'eps': 1e-05, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_ReduceLROnPlateau': True, 'lr_scheduler_interval': 'epoch', 'lr_scheduler_frequency': 10, 'reduceLROnPlateau_config': {'eps': 1e-08, 'factor': 0.1, 'min_lr': 0, 'verbose': True, 'cooldown': 0, 'patience': 10}}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 400, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 500, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 700, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.513693928718567,174999.0,,0.4699999988079071,,,
224,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2279.6283733844757,47,1694269432.5498593,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,,True,input,label,cross_entropy,commonsense,hendrycks/ethics,refs/pr/3,50.0,False,[:100],50.0,True,[:100],50.0,False,[:100],,True,input,label,mse_loss,learning_from_brains,data/ds000212,,LAST,,2.0,False,,,,47.0,,,,,,,,,,,,,,bert-base-cased,worldly-planet-538,,,,,,,"[0.9, 0.999]",1e-08,0.001,0.01,,True,,10.0,epoch,0.0,1e-08,0.1,0.0,10.0,True,0.1,False,,,0.0,False,,,1.0,False,1.0,15.0,1.0,,-1.0,-1.0,,,,,1.0,32-true,1.0,,,,,,,,,,,,,,,,False,,,,47.0,,0.5799999833106995,,,
225,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1483.1271092891693,30,1694189916.885991,{'runtime': 1481},/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,,True,input,label,cross_entropy,commonsense,hendrycks/ethics,refs/pr/3,50.0,False,[:100],50.0,True,[:100],50.0,False,[:100],,True,input,label,mse_loss,learning_from_brains,data/ds000212,,LAST,,2.0,False,,,,30.0,,,,,,,,,,,,,,bert-base-cased,sparkling-galaxy-536,,,,,,,"[0.9, 0.999]",1e-08,0.001,0.01,,True,,10.0,epoch,0.0,1e-08,0.1,0.0,10.0,True,0.1,False,,,0.0,False,,,1.0,False,1.0,15.0,1.0,,-1.0,-1.0,,,,,1.0,32-true,1.0,,,,,,,,,,,,,,,0.5099999904632568,False,,,,30.0,,0.5600000023841858,,,
226,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15827.953704357147,213,1694191371.2948494,{'runtime': 15894},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:2000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,338.0,,,,,,,,,,,,,,bert-large-cased,desert-plasma-534,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_ReduceLROnPlateau': True, 'lr_scheduler_interval': 'epoch', 'lr_scheduler_frequency': 10, 'reduceLROnPlateau_config': {'eps': 1e-08, 'factor': 0.1, 'min_lr': 0, 'verbose': True, 'cooldown': 0, 'patience': 10}}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.6091513633728027,5084.0,,0.5299999713897705,,,
227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19575.87105178833,123,1694191342.1684468,{'runtime': 19702},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:60%]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:60%]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,196.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,good-plant-532,,,,,,"{'adamw': {'lr': 0.002, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_ReduceLROnPlateau': True, 'lr_scheduler_interval': 'epoch', 'lr_scheduler_frequency': 10, 'reduceLROnPlateau_config': {'eps': 1e-08, 'factor': 0.1, 'min_lr': 0, 'verbose': True, 'cooldown': 0, 'patience': 10}}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 250, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,True,,,1.714035987854004,2949.0,,0.5302897095680237,,,
228,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,61727.295224905014,510,1694151976.0942929,{'runtime': 61887},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,806.0,,,,,,,,,,,,,,bert-large-cased,silvery-blaze-530,,,,,,"{'adamw': {'lr': 0.002, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_ReduceLROnPlateau': False, 'lr_scheduler_interval': 'epoch', 'lr_scheduler_frequency': 10, 'reduceLROnPlateau_config': {'eps': 1e-08, 'factor': 0.1, 'min_lr': 0, 'verbose': True, 'cooldown': 0, 'patience': 10}}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,True,,,1.6157753467559814,12104.0,,0.5183553695678711,,,
229,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9305.182913780212,81,1694089861.1849058,{'runtime': 9353},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:0]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,129.0,,,,,,,,,,,,,,bert-large-cased,dainty-dragon-529,,,,,,"{'adamw': {'lr': 0.0006538379548447884, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_ReduceLROnPlateau': False, 'reduceLROnPlateau_config': {'eps': 1e-08, 'factor': 0.1, 'min_lr': 1e-10, 'verbose': True, 'cooldown': 100, 'patience': 10}, 'lr_scheduler_steps_frequency': 1500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 500, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.6333695650100708,1949.0,,0.5183553695678711,,,
230,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,825.7204818725586,9,1694020707.3379838,{'runtime': 829},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:40%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[50%:]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[60%:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,0.8,15.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,rosy-deluge-521,,,,,,"{'adamw': {'lr': 0.0006538379548447884, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_ReduceLROnPlateau': True, 'reduceLROnPlateau_config': {'eps': 1e-08, 'factor': 0.1, 'min_lr': 1e-10, 'verbose': True, 'cooldown': 100, 'patience': 10}, 'lr_scheduler_steps_frequency': 3000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '16-mixed', 'max_epochs': 3000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.48423707485198975,False,,,3.016477108001709,225.0,,0.5004270076751709,,,
231,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17693.451994419098,162,1694019094.1085374,{'runtime': 17693},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:40%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[50%:]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[60%:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,257.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,feasible-fog-514,,,,,,"{'adamw': {'lr': 1e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 3000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7080706357955933,False,,,0.7475665211677551,3869.0,,0.5286080241203308,,,
232,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,528.9845213890076,4,1693999750.2290585,{'runtime': 531},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:40%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[50%:]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[60%:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,0.8,12.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,worthy-sea-513,,,,,,"{'adamw': {'lr': 1e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 3000, 'min_epochs': None, 'overfit_batches': 1, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4810844957828522,False,,,,12.0,,0.5,,,
233,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1285.1543684005735,9,1693998298.2271843,{'runtime': 1288},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:40%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[50%:]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[60%:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,0.8,15.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,swift-bee-511,,,,,,"{'adamw': {'lr': 1e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 3000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5353089570999146,False,,,2.2010416984558105,225.0,,0.5004270076751709,,,
234,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1077.2237186431885,7,1693993960.9093397,{'runtime': 1080},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:40%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[50%:]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[60%:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,0.8,12.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,crisp-shape-510,,,,,,"{'adamw': {'lr': 3e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 1000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5157629251480103,False,,,1.6423921585083008,180.0,,0.4995730221271515,,,
235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40172.21165847778,633,1693976579.7276266,{'runtime': 40174},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:30%]', 'batch_size': 40}, 'train': {'shuffle': True, 'slicing': '[:30%]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:20%]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,1000.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,fine-music-506,,,,,,"{'adamw': {'lr': 2e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 1000, 'min_epochs': None, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.517241358757019,False,,,1.3961889743804932,15000.0,,0.5250965356826782,,,
236,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,835.060352563858,11,1693936281.4879186,{'runtime': 851},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:30%]', 'batch_size': 40}, 'train': {'shuffle': True, 'slicing': '[:30%]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:20%]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,19.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,winter-pond-505,,,,,,"{'adamw': {'lr': 2e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 2000, 'min_epochs': None, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.6854017972946167,299.0,,0.5392535328865051,,,
237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15367.283487319946,390,1693931395.9048743,{'runtime': 15390},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'justice', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:10%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:40%]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'scenario', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:20%]', 'batch_size': 10}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,617.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,glamorous-plant-498,,,,,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 1000, 'min_epochs': None, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.502439022064209,False,,,1.181565523147583,9269.0,,0.4990758001804352,,,
238,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1651.9501502513883,41,1693915546.9004052,{'runtime': 1875},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'justice', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:10%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:30%]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'scenario', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:20%]', 'batch_size': 10}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,66.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,cool-thunder-497,,,,,,"{'adamw': {}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 1000, 'min_epochs': None, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,False,,,1.4534071683883667,999.0,,0.5009242296218872,,,
239,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,489.81043100357056,12,1693912386.875182,{'runtime': 489},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'loss_fn': 'mse_loss', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,26.0,,,,,,,,,,,,,,bert-base-cased,frosty-wildflower-491,,,,,,"{'adamw': {}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 100, 'min_epochs': None, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 5}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4884937107563019,False,,,1.5648138523101809,390.0,,0.5339855551719666,,,
240,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,141.8937790393829,75,1692364012.445406,{'runtime': 144},artifacts,2.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,deepset/deberta-v3-large-squad2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0005199336193858578,,,,,,,snowy-sweep-1,5.0,0.0,0.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5339023470878601,False,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.5614852905273438,75.0,,,,,
241,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14.009280681610107,14,1692363609.5749238,{'runtime': 17},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,devoted-blaze-312,1.0,0.0,0.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,,False,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.7569584846496582,14.0,,,,,
242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23.814956665039062,51,1692358644.4584856,{'runtime': 23},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,peach-field-304,5.0,0.0,0.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,,False,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.592284917831421,51.0,,,,,
243,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,120.1744191646576,256,1691703439.6054533,,/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,true-wood-294,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.7556881308555603,256.0,,,,,
244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.37414288520813,75,1691703163.0681038,{'runtime': 53},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,upbeat-snow-292,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.8956043720245361,75.0,,,,,
245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,216.0513601303101,450,1691703089.883187,{'runtime': 215},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,ethereal-glitter-291,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.9439788460731506,450.0,,,,,
246,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,118.99122071266174,225,1691702854.835157,{'runtime': 118},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,silvery-bird-290,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.47066667675971985,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.7944100499153137,225.0,,,,,
247,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.24850416183472,75,1691702718.7630472,{'runtime': 53},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,visionary-leaf-289,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.47066667675971985,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.9653942584991456,75.0,,,,,
248,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,217.61115074157715,450,1691702646.4297068,{'runtime': 215},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,celestial-sea-288,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.8103920221328735,450.0,,,,,
249,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,118.9533236026764,225,1691702409.2566895,{'runtime': 118},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,morning-totem-287,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.6453253626823425,225.0,,,,,
250,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.93316125869751,75,1691702270.9137464,{'runtime': 54},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,zesty-serenity-286,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.5479435324668884,75.0,,,,,
251,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.4119517803192,450,1691702196.3165867,{'runtime': 124},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,earthy-microwave-285,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.0819942951202393,450.0,,,,,
252,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,73.93520617485046,225,1691702050.3673062,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,grateful-shape-284,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.1209884881973269,225.0,,,,,
253,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40.019001722335815,75,1691701958.0740776,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,unique-snow-283,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.303211331367493,75.0,,,,,
254,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,126.1474540233612,450,1691701899.073534,{'runtime': 125},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,decent-gorge-282,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.4549416303634643,450.0,,,,,
255,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,75.20999574661255,225,1691701754.241773,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,earthy-fog-281,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.0681411027908323,225.0,,,,,
256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40.022719621658325,75,1691701660.7415686,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,distinctive-oath-280,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.4718514680862427,75.0,,,,,
257,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.5032205581665,450,1691701600.8667505,{'runtime': 124},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,elated-field-279,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.5101988315582275,450.0,,,,,
258,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,74.1295440196991,225,1691701456.270313,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,clean-voice-278,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.6455064415931702,225.0,,,,,
259,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40.054593324661255,75,1691701363.6421444,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,legendary-plant-277,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.6366989612579346,75.0,,,,,
260,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,124.99004793167114,450,1691701303.759943,{'runtime': 124},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,likely-sunset-276,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.100278377532959,450.0,,,,,
261,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,74.0088403224945,225,1691701159.5706682,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,likely-sound-275,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.7950406074523926,225.0,,,,,
262,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40.06700801849365,75,1691701063.224821,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,clean-pond-274,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.315290451049805,75.0,,,,,
263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.73938632011414,450,1691701004.1353784,{'runtime': 125},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,wandering-tree-273,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.5142205953598022,450.0,,,,,
264,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,74.35074257850647,225,1691700859.7863815,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,fiery-snowflake-272,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.4337380528450012,225.0,,,,,
265,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40.15264081954956,75,1691700766.270187,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,royal-flower-271,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.8875417113304138,75.0,,,,,
266,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,305.0161054134369,450,1691700704.7103665,{'runtime': 304},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,wild-shadow-270,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.8269767761230469,450.0,,,,,
267,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,163.58999848365784,225,1691700379.7851374,{'runtime': 163},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,happy-wood-269,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.9713563919067385,225.0,,,,,
268,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,69.38036012649536,75,1691700195.964966,{'runtime': 68},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,cool-jazz-268,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.47066667675971985,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",4.190782070159912,75.0,,,,,
269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,306.29916310310364,450,1691700106.707741,{'runtime': 305},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,light-violet-267,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.7461037635803225,450.0,,,,,
270,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,163.77628374099731,225,1691699781.2915287,{'runtime': 163},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,summer-wave-266,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6552588939666748,225.0,,,,,
271,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,69.41340684890747,75,1691699598.0933988,{'runtime': 68},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,elated-disco-265,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.598095178604126,75.0,,,,,
272,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,304.6734654903412,450,1691699509.4889956,{'runtime': 304},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,electric-wave-264,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.5745768547058103,450.0,,,,,
273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,163.24410343170166,225,1691699184.5509284,{'runtime': 162},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,dark-yogurt-263,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6248266696929932,225.0,,,,,
274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,69.42444181442261,75,1691699001.7450788,{'runtime': 68},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,gallant-star-262,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6695163249969482,75.0,,,,,
275,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,304.5851058959961,450,1691698912.888636,{'runtime': 303},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,glorious-flower-261,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.8714656829833984,450.0,,,,,
276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,163.35999631881714,225,1691698588.7267463,{'runtime': 162},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,legendary-firebrand-260,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.6747164726257324,225.0,,,,,
277,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,69.11856079101562,75,1691698392.8736107,{'runtime': 68},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,ethereal-dawn-259,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.47066667675971985,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",3.119204521179199,75.0,,,,,
278,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,305.54650139808655,450,1691698303.4538465,{'runtime': 304},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,swept-shadow-258,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6465787887573242,450.0,,,,,
279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,162.65915870666504,225,1691697978.8574555,{'runtime': 161},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,morning-breeze-257,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.697502374649048,225.0,,,,,
280,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,69.65706157684326,75,1691697796.8474677,{'runtime': 68},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,sweet-voice-256,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.550817847251892,75.0,,,,,
281,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,215.7277839183807,450,1691697707.076465,{'runtime': 215},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,zesty-darkness-255,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.714012622833252,450.0,,,,,
282,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,118.68253803253174,225,1691697473.0513911,{'runtime': 118},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,stilted-shape-254,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.644906759262085,225.0,,,,,
283,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.25298619270325,75,1691697334.3035283,{'runtime': 53},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,devout-pine-253,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.0186960697174072,75.0,,,,,
284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,215.59033751487732,450,1691697260.1897354,{'runtime': 214},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,lemon-durian-252,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.8712323904037476,450.0,,,,,
285,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,119.05017590522766,225,1691697025.031373,{'runtime': 118},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,swept-silence-251,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6673717498779297,225.0,,,,,
286,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.45801877975464,75,1691696887.9418898,{'runtime': 53},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,genial-glitter-250,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.8842122554779053,75.0,,,,,
287,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,215.31425285339355,450,1691696813.9749207,{'runtime': 214},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,eager-donkey-249,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.594438552856445,450.0,,,,,
288,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,118.8782045841217,225,1691696578.7503536,{'runtime': 118},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,sandy-cherry-248,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.639101266860962,225.0,,,,,
289,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.34067463874817,75,1691696441.8169615,{'runtime': 53},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,dazzling-salad-247,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.03407621383667,75.0,,,,,
290,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,215.6220171451569,450,1691696367.8548672,{'runtime': 214},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,deep-wind-246,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.47066667675971985,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6660449504852295,450.0,,,,,
291,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,119.0193808078766,225,1691696132.5246007,{'runtime': 118},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,upbeat-wave-245,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6494665145874023,225.0,,,,,
292,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.18520474433899,75,1691695994.9433727,{'runtime': 53},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,decent-yogurt-244,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.250279426574707,75.0,,,,,
293,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,215.83715343475345,450,1691695920.2698283,{'runtime': 215},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,absurd-wave-243,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.681272029876709,450.0,,,,,
294,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,118.86686706542967,225,1691695684.569427,{'runtime': 118},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,silvery-jazz-242,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.8072099685668943,225.0,,,,,
295,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.734833002090454,75,1691695545.817568,{'runtime': 54},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,unique-silence-241,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.8151156902313232,75.0,,,,,
296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.29230380058289,450,1691695471.2937107,{'runtime': 124},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,cerulean-monkey-240,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.374691963195801,450.0,,,,,
297,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,73.96562099456787,225,1691695326.906358,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,eager-dragon-239,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.500704288482666,225.0,,,,,
298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,39.852197885513306,75,1691695234.3521929,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,dashing-dragon-238,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.9166682958602903,75.0,,,,,
299,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.57779479026794,450,1691695175.2253768,{'runtime': 125},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,magic-grass-237,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.1444268226623535,450.0,,,,,
300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,74.12735939025879,225,1691695030.4007485,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,breezy-planet-236,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.5465470552444458,225.0,,,,,
301,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40.20835018157959,75,1691694937.774743,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,absurd-wave-235,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.5771337747573853,75.0,,,,,
302,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.14251923561096,450,1691694877.441506,{'runtime': 124},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,sunny-sound-234,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.5508317947387695,450.0,,,,,
303,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,73.97358989715576,225,1691694732.9303708,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,glowing-silence-233,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.5223078727722168,225.0,,,,,
304,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,39.94889211654663,75,1691694640.25634,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,decent-cloud-232,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.7800180912017822,75.0,,,,,
305,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.12947869300842,450,1691694580.8316226,{'runtime': 124},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,celestial-snowflake-231,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.889233112335205,450.0,,,,,
306,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,74.10032391548157,225,1691694436.3141448,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,cool-spaceship-230,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.47066667675971985,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.179415225982666,225.0,,,,,
307,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,39.71375513076782,75,1691694343.502318,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,iconic-sponge-229,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",6.305532932281494,75.0,,,,,
308,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.6423840522766,450,1691694284.374371,{'runtime': 124},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,legendary-cloud-228,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.3318276405334473,450.0,,,,,
309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,74.09116220474243,225,1691694139.356285,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,golden-leaf-227,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.5277317762374878,225.0,,,,,
310,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40.16663074493408,75,1691694046.7353578,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,glorious-silence-226,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.6236658096313477,75.0,,,,,
311,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,303.1976971626282,450,1691693985.2946482,{'runtime': 302},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,lemon-brook-225,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.786839246749878,450.0,,,,,
312,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,162.21836161613464,225,1691693663.1620965,{'runtime': 161},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,pretty-smoke-224,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.4787297248840332,225.0,,,,,
313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,69.2082097530365,75,1691693481.159444,{'runtime': 68},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,dark-glitter-223,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6698369979858398,75.0,,,,,
314,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,303.3633785247803,450,1691693391.1885016,{'runtime': 302},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,silvery-yogurt-222,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.5630512237548828,450.0,,,,,
315,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,164.77531814575195,225,1691693067.9702911,{'runtime': 162},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,lemon-totem-221,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.579892873764038,225.0,,,,,
316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,69.26238560676575,75,1691692882.6757035,{'runtime': 68},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,amber-surf-220,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6466553211212158,75.0,,,,,
317,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,303.1390633583069,450,1691692793.8172314,{'runtime': 302},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,logical-plasma-219,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.3314032554626465,450.0,,,,,
318,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,162.39197325706482,225,1691692469.3773603,{'runtime': 162},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,breezy-brook-218,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.3460919857025146,225.0,,,,,
319,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,68.93310022354126,75,1691692287.1384673,{'runtime': 68},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,fresh-blaze-217,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.5294201374053955,75.0,,,,,
320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,302.41730284690857,450,1691692198.652135,{'runtime': 301},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,icy-music-216,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.5736466646194458,450.0,,,,,
321,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,162.2992217540741,225,1691691877.3278728,{'runtime': 161},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,clear-bee-215,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.5646907091140747,225.0,,,,,
322,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,68.7342324256897,75,1691691694.5481274,{'runtime': 68},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,sweet-frost-214,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.4079043865203857,75.0,,,,,
323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,303.2235815525055,450,1691691605.9242215,{'runtime': 302},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,fluent-dream-213,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.4169727563858032,450.0,,,,,
324,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,162.7352385520935,225,1691691282.3987064,{'runtime': 162},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,woven-paper-212,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.558955430984497,225.0,,,,,
325,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,69.41073870658875,75,1691691099.4596288,{'runtime': 69},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,worthy-dream-211,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.3112133741378784,75.0,,,,,
326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,214.1546061038971,450,1691691011.190936,{'runtime': 213},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,misunderstood-water-210,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.2961459159851074,450.0,,,,,
327,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,117.96131253242493,225,1691690778.4787295,{'runtime': 117},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,stilted-moon-209,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.728179693222046,225.0,,,,,
328,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.53465700149536,75,1691690640.07271,{'runtime': 54},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,glowing-bush-208,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6920983791351318,75.0,,,,,
329,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,214.20674324035645,450,1691690565.6689005,{'runtime': 213},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,treasured-field-207,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.700977087020874,450.0,,,,,
330,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,118.41929650306702,225,1691690330.8559046,{'runtime': 118},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,visionary-forest-206,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.8393945693969729,225.0,,,,,
331,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.55442523956299,75,1691690192.2929292,{'runtime': 54},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,avid-wildflower-205,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.965328335762024,75.0,,,,,
332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,214.50055360794067,450,1691690116.9989977,{'runtime': 214},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,feasible-morning-204,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.315989375114441,450.0,,,,,
333,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,118.19596099853516,225,1691689883.942497,{'runtime': 117},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,graceful-microwave-203,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.757194757461548,225.0,,,,,
334,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.40466737747192,75,1691689745.0720105,{'runtime': 53},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,dutiful-microwave-202,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.72176194190979,75.0,,,,,
335,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,213.98809218406677,450,1691689671.0136633,{'runtime': 213},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,flowing-meadow-201,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.136988639831543,450.0,,,,,
336,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,118.09610772132874,225,1691689438.3098726,{'runtime': 117},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,stellar-terrain-200,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",4.149891376495361,225.0,,,,,
337,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.42960524559021,75,1691689301.4823782,{'runtime': 53},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,autumn-gorge-199,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.8605810403823853,75.0,,,,,
338,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,214.7724411487579,450,1691689226.328546,{'runtime': 214},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,ruby-bee-198,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.813054084777832,450.0,,,,,
339,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,118.5673680305481,225,1691688991.1273391,{'runtime': 118},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,fallen-armadillo-197,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.576290488243103,225.0,,,,,
340,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,54.41227412223816,75,1691688852.8150892,{'runtime': 54},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,driven-rain-196,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6173533201217651,75.0,,,,,
341,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.5076596736908,450,1691688779.1015337,{'runtime': 125},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,glowing-thunder-195,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.9803603887557983,450.0,,,,,
342,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,74.03320813179016,225,1691688634.695367,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,floral-glade-194,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.4053187370300293,225.0,,,,,
343,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40.07333254814148,75,1691688540.3827126,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.08547,,,,,,,charmed-bush-193,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.8140897750854492,75.0,,,,,
344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.51829028129578,450,1691688480.603115,{'runtime': 124},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,glowing-monkey-192,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6955914497375488,450.0,,,,,
345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,75.02715373039246,225,1691688335.1240487,{'runtime': 74},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,scarlet-snow-191,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.8418357372283936,225.0,,,,,
346,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40.14715576171875,75,1691688241.5627167,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.001,,,,,,,wandering-field-190,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.7972800731658936,75.0,,,,,
347,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.60201287269592,450,1691688181.04721,{'runtime': 124},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,hearty-universe-189,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.7517043352127075,450.0,,,,,
348,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,74.06705665588379,225,1691688036.2196846,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,desert-surf-188,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.641331672668457,225.0,,,,,
349,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,39.911725759506226,75,1691687943.7429256,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.01,,,,,,,peachy-jazz-187,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.73944091796875,75.0,,,,,
350,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.26712584495544,450,1691687884.628892,{'runtime': 124},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,major-smoke-186,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.47066667675971985,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.1732425689697266,450.0,,,,,
351,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,74.0370581150055,225,1691687738.588648,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,valiant-sun-185,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",2.7579233646392822,225.0,,,,,
352,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40.12244367599487,75,1691687644.0611827,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.1,,,,,,,grateful-monkey-184,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",4.137846946716309,75.0,,,,,
353,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,125.60356402397156,450,1691687584.306039,{'runtime': 124},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,floral-sunset-183,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.4711110591888428,450.0,,,,,
354,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,73.94520545005798,225,1691687439.6923974,{'runtime': 73},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,glorious-snowball-182,15.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.482679843902588,225.0,,,,,
355,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,39.64598536491394,75,1691687346.8430457,{'runtime': 39},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,driven-paper-181,5.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.4950275421142578,75.0,,,,,
356,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,211.77576994895935,300,1691676372.87019,{'runtime': 211},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,glad-serenity-178,20.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.MIDDLE,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.4058897495269775,300.0,,,,,
357,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,151.5136694908142,300,1691676141.9870574,{'runtime': 151},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,fluent-moon-177,20.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.MIDDLE,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.658252477645874,300.0,,,,,
358,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,91.5265929698944,300,1691675970.298792,{'runtime': 90},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,lyric-donkey-176,20.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.MIDDLE,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.637275218963623,300.0,,,,,
359,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,212.63911843299863,300,1691675830.0974665,{'runtime': 212},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,glowing-grass-174,20.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.8383674621582031,300.0,,,,,
360,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,152.66242742538452,300,1691675599.0927134,{'runtime': 152},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,distinctive-gorge-173,20.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.7465378046035767,300.0,,,,,
361,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,91.90361642837524,300,1691675426.8188894,{'runtime': 91},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,stellar-river-172,20.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.AVG,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",0.8093017339706421,300.0,,,,,
362,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,212.62448358535767,300,1691675289.3764105,{'runtime': 211},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,playful-meadow-170,20.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.8553135395050049,300.0,,,,,
363,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,152.32210111618042,300,1691675058.0556262,{'runtime': 151},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,bright-thunder-169,20.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.876383066177368,300.0,,,,,
364,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,92.01985454559326,300,1691674885.7915056,{'runtime': 91},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,comfy-sky-168,20.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.2708451747894287,300.0,,,,,
365,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,211.06375861167908,300,1691674740.0641887,{'runtime': 210},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,vocal-wave-166,20.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.480273962020874,300.0,,,,,
366,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,151.35541081428528,300,1691674508.798844,{'runtime': 150},/workspace/brainbias/artifacts,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,splendid-deluge-165,20.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6887365579605105,300.0,,,,,
367,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,92.05284285545348,300,1691674337.8549829,{'runtime': 91},/workspace/brainbias/artifacts,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,giddy-brook-164,20.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.479480266571045,300.0,,,,,
368,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,115.55908560752869,150,1691673946.9726467,{'runtime': 115},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,winter-deluge-163,10.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.4433562755584717,150.0,,,,,
369,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,172.53332996368408,240,1691673767.828433,{'runtime': 172},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,faithful-cloud-162,30.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.7376190423965454,240.0,,,,,
370,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31.63632082939148,15,1691673406.913648,{'runtime': 31},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,dandy-aardvark-161,1.0,1500.0,1000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5293333530426025,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.6821930408477783,15.0,,,,,
371,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14.467511177062988,15,1691673296.5622172,{'runtime': 13},/workspace/brainbias/artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,sandy-water-160,1.0,100.0,100.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5099999904632568,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.7725083827972412,15.0,,,,,
372,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14.826961994171144,15,1691672236.02155,{'runtime': 13},,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,devoted-sun-159,1.0,100.0,100.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.5099999904632568,,,"['ethics', 'ds000212']",1.7079012393951416,15.0,,,,,
373,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,227.20888257026672,15,1691666847.4217217,{'runtime': 225},,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,lilac-pine-152,1.0,100.0,100.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.5099999904632568,,,"['ethics', 'ds000212']",1.9245470762252808,15.0,,,,,
374,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,256.6408591270447,15,1691596381.255866,{'runtime': 255},artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,expert-sea-151,1.0,100.0,100.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.SENTENCES,False,True,,,,,,,,,0.5099999904632568,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.9026687145233152,15.0,,,,,
375,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,232.98432850837708,15,1691584490.5218594,{'runtime': 231},artifacts,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,ethereal-grass-145,1.0,100.0,100.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,Sampling.LAST,False,True,,,,,,,,,0.5099999904632568,,,"['EthicsDataset', 'DS000212_LFB_Dataset']",1.7740551233291626,15.0,,,,,
376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13.53174614906311,15,1691498452.534579,{'runtime': 12},,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,iconic-music-127,1.0,100.0,100.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.5099999904632568,,,"['ethics', 'ds000212']",1.8860056400299072,15.0,,,,,
377,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,246.474378824234,15,1691485343.236127,{'runtime': 245},,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,crisp-cloud-124,1.0,100.0,100.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.5099999904632568,,,"['ethics', 'ds000212']",1.6636906862258911,15.0,,,,,
378,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,78.62054920196533,4,1691483476.3431363,{'runtime': 83},,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,wild-frost-121,1.0,100.0,100.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,,,,"['ethics', 'ds000212']",1.8017150163650513,4.0,,,,,
379,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,104.36317324638368,150,1690887250.9727862,{'runtime': 103},,10.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,glamorous-oath-120,10.0,100.0,100.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.5099999904632568,,,"['ethics', 'ds000212']",1.9812517166137695,150.0,,,,,
380,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.204678535461426,15,1690886975.7120516,{'runtime': 14},,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,vocal-darkness-117,1.0,100.0,100.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.5099999904632568,,,"['ethics', 'ds000212']",1.687288522720337,15.0,,,,,
381,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,262.3329954147339,15,1689859641.6032174,{'runtime': 259},,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,kind-sun-107,1.0,100.0,100.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,last,False,True,,,,,,,,,0.5099999904632568,,,"['ethics', 'ds000212']",1.9226269721984863,15.0,,,,,
382,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,230.8513686656952,15,1689409017.5600955,{'runtime': 228},,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,absurd-violet-105,1.0,100.0,100.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,last,False,True,,,,,,,,,0.5099999904632568,,,"['ethics', 'ds000212']",1.8378270864486697,15.0,,,,,
383,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1514.2071933746338,900,1689369928.8818924,{'runtime': 1514},,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,[True],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,60.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,absurd-lion-104,60.0,10000.0,1500.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.5339023470878601,,,"['ethics', 'ds000212']",1.6103469133377075,900.0,,,,,
384,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,83.39023184776306,5,1689255903.8407598,{'runtime': 105},,15.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,lunar-bird-90,1.0,100.0,100.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,,,,"['ethics', 'ds000212']",1.7601584196090698,5.0,,,,,
385,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18.169011116027832,14,1687002507.7511191,{'runtime': 17},,2.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,dutiful-dew-87,1.0,100.0,500.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,,,,['ds000212'],249.1581573486328,14.0,,,,,
386,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18.106547117233276,14,1687002243.018135,{'runtime': 16},,2.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,avid-violet-85,1.0,100.0,500.0,,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,,,,['ds000212'],245.8538360595703,14.0,,,,,
387,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,433.36859035491943,241,1687001728.6824763,{'runtime': 434},,9.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",6.538379548447885e-05,,,,,,,peachy-aardvark-83,30.0,10000.0,150.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.6007586717605591,,,"['ethics', 'ds000212']",0.6991100907325745,241.0,,,,,
388,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,428.8337996006012,239,1687001112.9203155,{'runtime': 429},,9.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",6.538379548447885e-05,,,,,,,easy-dream-82,30.0,10000.0,150.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.566145122051239,,,"['ethics', 'ds000212']",0.9609326124191284,239.0,,,,,
389,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,454.77307176589966,255,1687000307.4256449,{'runtime': 456},,9.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",6.538379548447885e-05,,,,,,,polished-bee-81,30.0,10000.0,150.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.5737316012382507,,,"['ethics', 'ds000212']",0.8659906983375549,255.0,,,,,
390,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,695.613650560379,450,1686944661.5924284,{'runtime': 696},,7.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",0.0006538379548447884,,,,,,,smart-hill-80,30.0,10000.0,350.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.5339023470878601,,,"['ethics', 'ds000212']",1.970319151878357,450.0,,,,,
391,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,951.4527871608734,900,1686943075.3239582,{'runtime': 952},,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,60.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",6.538379548447885e-05,,,,,,,good-firebrand-79,60.0,10000.0,1500.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.682788074016571,,,"['ethics', 'ds000212']",0.5191258192062378,900.0,,,,,
392,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,950.4762217998504,900,1686941989.0848577,{'runtime': 950},,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,60.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",6.538379548447885e-05,,,,,,,wobbly-eon-78,60.0,10000.0,350.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.6495969891548157,,,"['ethics', 'ds000212']",0.3550088107585907,900.0,,,,,
393,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,511.16926097869873,450,1686940847.700629,{'runtime': 511},,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",6.538379548447885e-05,,,,,,,iconic-violet-77,30.0,10000.0,350.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.6287339925765991,,,"['ethics', 'ds000212']",0.7005142569541931,450.0,,,,,
394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,331.2282521724701,282,1686938540.726695,{'runtime': 331},,9.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",6.538379548447885e-05,,,,,,,fallen-surf-76,30.0,10000.0,1.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.4660976827144623,,,"['ethics', 'ds000212']",0.8603218793869019,282.0,,,,,
395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,775.1512789726257,450,1686936123.146772,{'runtime': 774},,9.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",9e-05,,,,,,,wild-voice-75,30.0,10000.0,3000.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.7117117047309875,,,"['ethics', 'ds000212']",1.4417073726654053,450.0,,,,,
396,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,475.6030638217926,450,1686935208.5080135,{'runtime': 475},,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",7.5e-06,,,,,,,pleasant-firefly-73,30.0,10000.0,350.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.6818397641181946,,,"['ethics', 'ds000212']",1.039214849472046,450.0,,,,,
397,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,480.886533498764,450,1686934202.1834764,{'runtime': 480},,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",9e-06,,,,,,,glorious-water-72,30.0,10000.0,350.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.6766240000724792,,,"['ethics', 'ds000212']",1.01784086227417,450.0,,,,,
398,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,236.53536009788513,192,1686933682.794914,{'runtime': 236},,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",9e-05,,,,,,,easy-blaze-71,30.0,10000.0,350.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.6287339925765991,,,"['ethics', 'ds000212']",1.5599491596221924,192.0,,,,,
399,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,477.48506784439087,450,1686932020.1183808,{'runtime': 477},,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",6.538379548447885e-05,,,,,,,northern-eon-67,30.0,10000.0,350.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,False,,False,True,,,,,,,,,0.6201991438865662,,,"['ethics', 'ds000212']",0.645871639251709,450.0,,,,,
400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,343.1767644882202,300,1686931103.5870025,{'runtime': 343},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,,,,,,,,,,,,,,,lilac-universe-65,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6448553800582886,,,,0.9476232528686525,300.0,,,,,
401,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,195.48493266105652,146,1686930690.4255147,{'runtime': 196},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,leafy-pine-64,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5339023470878601,,,,1.1010026931762695,146.0,,,,,
402,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,387.09101724624634,225,1686678498.5191133,{'runtime': 387},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,,,,,,,,,,lively-pyramid-63,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5339023470878601,,,,1.6711009740829468,225.0,,,,,
403,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,178.21907448768616,75,1686677865.4146435,{'runtime': 178},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,noble-fog-62,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5339023470878601,,,,1.586138129234314,75.0,,,,,
404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,70.227463722229,400,1686676110.8567977,{'runtime': 70},,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,0.005052644961669269,,,,,,,fast-sweep-10,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6000000238418579,,,,1.575286865234375,400.0,,,,,
405,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,71.47599053382874,400,1686675898.5170617,{'runtime': 71},,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,0.0002861041782470337,,,,,,,valiant-sweep-7,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5099999904632568,,,,1.3365554809570312,400.0,,,,,
406,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,79.47051787376404,200,1686675544.8950078,{'runtime': 79},,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,0.08983561972127677,,,,,,,giddy-sweep-2,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5199999809265137,,,,4.537929058074951,200.0,,,,,
407,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,47.96842002868652,100,1686675457.299117,{'runtime': 47},,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,0.006082788486519143,,,,,,,smart-sweep-1,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5400000214576721,,,,2.0004944801330566,100.0,,,,,
408,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.684736967086792,2,1686674499.735511,{'runtime': 9},,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,0.0002310341301554967,,,,,,,vital-sweep-1,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.054039478302002,2.0,,,,,
409,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,49.93614077568054,9,1686501893.4854949,{'runtime': 424},,2.0,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bert-base-cased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,"['cross-entropy', 'mse']","[1, 1]",,,,,,,,trim-fire-1,1.0,1000.0,10.0,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.1,True,,False,True,,,,,,,,,,,,['ds000212'],-58.18623352050781,9.0,,,,,
