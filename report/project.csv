,Unnamed: 0,LFB-AVG-test-CosineSimilarity,LFB-AVG-test-CosineSimilarity/dataloader_idx_1,LFB-AVG-test-MeanAbsoluteError,LFB-AVG-test-MeanAbsoluteError/dataloader_idx_1,LFB-AVG-test-MeanSquaredError,LFB-AVG-test-MeanSquaredError/dataloader_idx_1,LFB-AVG-test-behavior-MulticlassAUROC,LFB-AVG-test-behavior-MulticlassAccuracy,LFB-AVG-test-behavior-MulticlassF1Score,LFB-AVG-test-label-CosineSimilarity,LFB-AVG-test-label-MeanAbsoluteError,LFB-AVG-test-label-MeanSquaredError,LFB-LAST-test-CosineSimilarity,LFB-LAST-test-CosineSimilarity/dataloader_idx_1,LFB-LAST-test-MeanAbsoluteError,LFB-LAST-test-MeanAbsoluteError/dataloader_idx_1,LFB-LAST-test-MeanSquaredError,LFB-LAST-test-MeanSquaredError/dataloader_idx_1,LFB-LAST-test-behavior-MulticlassAUROC,LFB-LAST-test-behavior-MulticlassAccuracy,LFB-LAST-test-behavior-MulticlassF1Score,LFB-LAST-test-label-CosineSimilarity,LFB-LAST-test-label-MeanAbsoluteError,LFB-LAST-test-label-MeanSquaredError,LFB-LAST-validation-MeanSquaredError,LFB-SENTENCES-test-behavior-MulticlassAUROC,LFB-SENTENCES-test-behavior-MulticlassAccuracy,LFB-SENTENCES-test-behavior-MulticlassF1Score,LFB-SENTENCES-test-label-CosineSimilarity,LFB-SENTENCES-test-label-MeanAbsoluteError,LFB-SENTENCES-test-label-MeanSquaredError,_runtime,_step,_timestamp,_wandb,artifactspath,batch_size,batches_per_epoch,bs_encoder.layer.10.output.dense,bs_encoder.layer.11.output.dense,bs_encoder.layer.12.output.dense,bs_encoder.layer.13.output.dense,bs_encoder.layer.14.output.dense,bs_encoder.layer.15.output.dense,bs_encoder.layer.16.output.dense,bs_encoder.layer.17.output.dense,bs_encoder.layer.18.output.dense,bs_encoder.layer.19.output.dense,bs_encoder.layer.2.output.dense,bs_encoder.layer.20.output.dense,bs_encoder.layer.21.output.dense,bs_encoder.layer.22.output.dense,bs_encoder.layer.23.output.dense,bs_encoder.layer.3.output.dense,bs_encoder.layer.4.output.dense,bs_encoder.layer.5.output.dense,bs_encoder.layer.6.output.dense,bs_encoder.layer.7.output.dense,bs_encoder.layer.8.output.dense,bs_encoder.layer.9.output.dense,bs_hidden_state_0,bs_hidden_state_1,bs_hidden_state_10,bs_hidden_state_11,bs_hidden_state_12,bs_hidden_state_2,bs_hidden_state_3,bs_hidden_state_4,bs_hidden_state_5,bs_hidden_state_6,bs_hidden_state_7,bs_hidden_state_8,bs_hidden_state_9,bs_hs_0,bs_hs_1,bs_hs_10,bs_hs_11,bs_hs_12,bs_hs_13,bs_hs_14,bs_hs_15,bs_hs_16,bs_hs_17,bs_hs_18,bs_hs_19,bs_hs_2,bs_hs_20,bs_hs_21,bs_hs_22,bs_hs_23,bs_hs_24,bs_hs_3,bs_hs_4,bs_hs_5,bs_hs_6,bs_hs_7,bs_hs_8,bs_hs_9,checkpoint,checkpoint_path,checkpointing,cod_hidden_state_0,cod_hidden_state_1,cod_hidden_state_10,cod_hidden_state_11,cod_hidden_state_12,cod_hidden_state_2,cod_hidden_state_3,cod_hidden_state_4,cod_hidden_state_5,cod_hidden_state_6,cod_hidden_state_7,cod_hidden_state_8,cod_hidden_state_9,cod_hs_0,cod_hs_1,cod_hs_10,cod_hs_11,cod_hs_12,cod_hs_13,cod_hs_14,cod_hs_15,cod_hs_16,cod_hs_17,cod_hs_18,cod_hs_19,cod_hs_2,cod_hs_20,cod_hs_21,cod_hs_22,cod_hs_23,cod_hs_24,cod_hs_3,cod_hs_4,cod_hs_5,cod_hs_6,cod_hs_7,cod_hs_8,cod_hs_9,commonsense-test-MulticlassAUROC,commonsense-test-MulticlassAUROC/dataloader_idx_0,commonsense-test-MulticlassAccuracy,commonsense-test-MulticlassAccuracy/dataloader_idx_0,commonsense-test-MulticlassF1Score,commonsense-test-label-MulticlassAUROC,commonsense-test-label-MulticlassAccuracy,commonsense-test-label-MulticlassF1Score,commonsense-validation-MulticlassAUROC,commonsense-validation-MulticlassAccuracy,commonsense-validation-MulticlassF1Score,commonsense-validation-label-MulticlassAUROC,commonsense-validation-label-MulticlassAccuracy,commonsense-validation-label-MulticlassF1Score,datapath,debug,ds1,ds1/enable,ds1/input_col,ds1/label_col,ds1/loss_fn,ds1/name,ds1/path,ds1/revision,ds1/test/batch_size,ds1/test/shuffle,ds1/test/slicing,ds1/train/batch_size,ds1/train/shuffle,ds1/train/slicing,ds1/validation/batch_size,ds1/validation/shuffle,ds1/validation/slicing,ds2,ds2/enable,ds2/input_col,ds2/label_col,ds2/loss_fn,ds2/name,ds2/path,ds2/revision,ds2/sampling_method,ds2/test,ds2/train/batch_size,ds2/train/shuffle,ds2/train/slicing,ds2/validation,early_stop_threshold,epoch,find_bs,find_learning_rate,find_lr,finetuned_path,last_checkpoint_path,loss_names,loss_weights,lr,lr-AdamW,lr-AdamW/pg1,lr-AdamW/pg2,lr-AdamW/pg3,lr-AdamW/pg4,model_path,name,num_epochs,num_samples_test,num_samples_train,num_workers,only_train_head,plc,plc/adamw/betas,plc/adamw/eps,plc/adamw/lr,plc/adamw/weight_decay,plc/before_lr_decay_warm_up_steps,plc/has_ReduceLROnPlateau,plc/has_learning_rate_decay,plc/lr_scheduler_frequency,plc/lr_scheduler_interval,plc/reduceLROnPlateau_config/cooldown,plc/reduceLROnPlateau_config/eps,plc/reduceLROnPlateau_config/factor,plc/reduceLROnPlateau_config/min_lr,plc/reduceLROnPlateau_config/patience,plc/reduceLROnPlateau_config/verbose,plc/regularization_coef,plc/regularize_from_init,plc/stepLR_gamma,plc/stepLR_step_size,plc/token_location,plc/train_all,pltc,pltc/accumulate_grad_batches,pltc/check_val_every_n_epoch,pltc/enable_checkpointing,pltc/limit_test_batches,pltc/limit_train_batches,pltc/limit_val_batches,pltc/log_every_n_steps,pltc/max_epochs,pltc/max_steps,pltc/max_time,pltc/min_epochs,pltc/min_steps,pltc/num_sanity_val_steps,pltc/overfit_batches,pltc/precision,pltc/val_check_interval,profiler,regularization_coef,regularize_from_init,sampling_method,shuffle_test,shuffle_train,tags,test-LFB-AVG-mse/dataloader_idx_1,test-LFB-LAST-mse/dataloader_idx_1,test-MeanSquaredError,test-MeanSquaredError/dataloader_idx_1,test-MulticlassAccuracy,test-MulticlassAccuracy/dataloader_idx_0,test-commonsense-acc,test-commonsense-acc/dataloader_idx_0,test_acc,to_save_model,torch_float32_matmul_precision,train_datasets,train_loss,trainer/global_step,val-commonsense-acc,val_acc,validation-MeanSquaredError,validation-MulticlassAccuracy,validation-commonsense-acc,cs_hard_set_acc,cs_test_set_acc
2,2,,,,,,,,,,,,,,,,,,,0.3940171301364898,0.4202279448509216,0.3790190815925598,0.1079355776309967,0.7586880922317505,0.9210708737373352,,,,,,,,10006.022001743317,335,1702411660.4963658,{'runtime': 10005},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.2421130097670904,0.256390669466755,0.2529851014972261,0.3105193006297641,0.269989537864194,0.2899217557071819,0.2890734962386055,0.3305163181379219,0.3484243772667684,0.3485064814838209,0.3558675409833041,0.361085082608699,0.17024452499841,0.3501208117688857,0.3359750888920613,0.3426485803333757,0.3338750840861443,0.2658858108901928,0.0924384369934273,0.0755113596313669,0.0339675498472259,0.0375211112733435,0.0788466692838245,0.1451244319859239,0.1587318588482134,,,,,,,,,,,,,,,,,-0.3950316574988877,-0.3256548751331654,-2.543717865987167,-0.7342483679009875,-0.1401350095788212,-0.1865166897068242,-0.4298580956668336,0.3403161767135491,-0.7298951653397703,-0.3366483717499082,-0.0396967613260788,-1.0949866732650797,-0.5664583648050228,-0.2931657939898813,-0.3285664223268463,-0.4014332005559198,-0.6684495440062896,-0.493275911056557,0.0598569306225045,-0.0247486844604867,0.2009807414447496,-0.2546090747838603,-0.498817333246482,-0.2252929979045017,0.0211695234494703,,,,,,0.700333297252655,0.6184999942779541,0.5576567053794861,,,,0.9250667095184326,0.8542667627334595,0.8496127724647522,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/bert-large-cased_fmri_and_ethics_hm_23-12-12_1720.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,bert-large-cased,bert-large-cased_fmri_and_ethics_hm_23-12-12_1720,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 7, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0190038159489631,0.0,,,,,,0.6184999942779541,0.8542667627334595
4,4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4489.985455274582,170,1701451516.4453044,{'runtime': 4488},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0249554770468826,-0.0912409099590175,0.1344264781530266,0.2691777172412254,0.2762925410517217,0.289027724587929,0.2559081726043197,0.1599662246549829,0.2402230804747393,0.2431764233954629,0.2204798860321432,0.3181671135333892,-0.0558782405696531,0.147456277539744,0.2048307913769439,0.181662631042491,0.2197784401709918,0.1093208083669876,0.0320785354135661,0.0380215696986056,0.0656027122832411,0.1208265930580752,0.018222424876267,-0.0734666589287589,0.0430174885560695,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-12-01_1542.ckpt,,,,,,,,,,,,,,,-1.615658966710653,-2.208139529067968,-1.1619817550438745,-1.308580228900031,-0.7514451691985655,-0.0839684259230248,0.2012729475704603,-0.2283543890733028,-1.1463726489614945,-0.7294714885735973,-0.587060805720143,-1.4021788121795682,-1.112564281680947,-0.7764115988285483,-0.0447301959403965,-0.3121373690571682,-0.7628998776467213,-0.5816572503554724,-0.5203527599604985,-0.3502813412553132,-0.4222301752420445,-0.3712439821199975,-1.7294433950155137,-4.115644149108448,-0.7129851137768122,,,,,,0.8342000246047974,0.6956475973129272,0.6695106029510498,,,,0.9756000638008118,0.898933470249176,0.8955197334289551,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-SENTENCES', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-12-01_1610.ckpt,,,,,1e-05,1e-05,,,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-12-01_1610,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0422022342681884,0.0,,,,,,0.6956475973129272,0.898933470249176
5,5,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,0.4798386991024017,0.4924730956554413,0.0604775212705135,0.7902217507362366,1.003330111503601,1636.7356843948364,86,1701447007.5333154,{'runtime': 1636},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0237322998770078,-0.0927993367421225,0.1048951891785701,0.205946502271357,0.1941915730178515,0.2165766312647223,0.2027325206574954,0.0796964214507417,0.1382358837988911,0.2065775618077397,0.2037087609862774,0.2085769318283549,-0.0793260920962056,0.2044613120744782,0.2383382118469835,0.2974762125773903,0.1887007800104284,0.1980934636555273,0.0203243699514139,0.0220342596787722,0.0408168479430168,0.0656643134609465,0.0737089852350687,0.0425883856131094,0.0833529119492576,,,,,,,,,,,,,,,,,-1.6142067254268555,-2.2087372654566244,-1.160225966882595,-1.309122917517599,-0.7513719502664689,-0.0809815592103231,0.2068484767592809,-0.2252516375373239,-1.146922344698628,-0.7303114141515865,-0.5824315895803349,-1.4047781673833248,-1.11514838300806,-0.7757507339873393,-0.0453326150898105,-0.3125809718799375,-0.762521720630823,-0.578723560651033,-0.519530819031093,-0.3507109897175902,-0.4212969705079135,-0.3717597481388175,-1.7297857024231271,-4.117548847622819,-0.7104473407881409,,,,,,0.5156666040420532,0.5011667013168335,0.4267807006835937,,,,0.5196333527565002,0.510200023651123,0.4844792485237121,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-SENTENCES', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-12-01_1542.ckpt,,,,,9.70299e-06,9.70299e-06,9.70299e-06,9.70299e-06,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-12-01_1542,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,2.312690734863281,0.0,,,,,,0.5011667013168335,0.510200023651123
6,6,,,,,,,,,,,,,,,,,,,,,,,,,,0.2080127447843551,0.354166716337204,0.3642039895057678,0.0544985309243202,0.7944301962852478,1.013938069343567,1508.1090154647827,42,1701439331.0709984,{'runtime': 1507},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0237547551408289,-0.0967884338261282,0.1162140855674351,0.2262160131725716,0.1866417382731077,0.2285471950491215,0.1772545530682354,0.1040841759028597,0.0933294972452737,0.1008293531431162,0.1464177950826804,0.213947425359324,-0.0741767789323038,0.2021234867700457,0.2472358795687226,0.3317453928845777,0.2962854456725574,0.1979386314215719,0.0299255167045786,0.0787480454748907,0.033772767847635,0.0604079689810326,0.0176960477533931,0.0457635897159214,0.0623490975414415,,,,,,,,,,,,,,,,,-1.6141922926612584,-2.208675137943916,-1.1602203304647882,-1.3091162509412797,-0.7513269964043743,-0.0809451397435379,0.2068336231442788,-0.2252515702895861,-1.1468785389094929,-0.7302673601023266,-0.5823837489767203,-1.4046290577690923,-1.115081262006846,-0.7757650550873245,-0.04538808371218,-0.3125932310666146,-0.7625378875008781,-0.5787328883980505,-0.5195336447331318,-0.3507110799839379,-0.4212973451829194,-0.3716906651253557,-1.7297272346983052,-4.117488192709129,-0.7103917510161382,,,,,,0.5262476801872253,0.5,0.3387535810470581,,,,0.4862410128116608,0.5,0.346112459897995,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-SENTENCES', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-12-01_1336.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-12-01_1336,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,1.9734681844711304,0.0,,,,,,0.5,0.5
7,7,,,,,,,,,,,,,,,,,,,,,,,,,,0.2286858409643173,0.5798609852790833,0.5624273419380188,0.084256462752819,0.7808749675750732,0.9804503321647644,4050.0633239746094,149,1701437800.06374,{'runtime': 4049},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0237182894853037,-0.0943992096675951,0.0659609899286719,0.1681692854782161,0.125452682986317,0.1932837580083407,0.2091099729753509,0.1091382343256723,0.1374651310749356,0.160109301189872,0.1545684948924479,0.1811734316452531,-0.0886519192067568,0.1443456766217246,0.1248088978108965,0.2811728287989788,0.2890125837548907,0.214768070806148,0.023388748782851,0.044131517715857,0.078371214453789,0.0802747864277763,-0.0376493427321856,-0.0113695229462066,0.0111035524708595,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-12-01_1228.ckpt,,,,,,,,,,,,,,,-1.6141675059403284,-2.208721796619736,-1.1601979891953116,-1.3090968091950406,-0.7513468938081718,-0.0809787279539828,0.2068882292566515,-0.2252612920640846,-1.1468785347086916,-0.7302237784430969,-0.5823781601310205,-1.4046945447225836,-1.1150627406319864,-0.7757227884486539,-0.0453444547364174,-0.3125877826895435,-0.7625092028897482,-0.5787081418222024,-0.5195310805232756,-0.3506869719422112,-0.421316476252213,-0.3717639743681431,-1.7297219567486115,-4.117443635055862,-0.7103849560345559,,,,,,0.4669571220874786,0.5,0.3387535810470581,,,,0.4242134988307953,0.5,0.346112459897995,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-SENTENCES', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-12-01_1229.ckpt,,,,,9.6059601e-06,9.6059601e-06,9.6059601e-06,9.6059601e-06,roberta-large,roberta-large_Ethics-HM_then_fmri-HM_23-12-01_1229,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,1.217678785324097,0.0,,,,,,0.5,0.5
9,9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9445.414831638336,128,1701347746.934543,{'runtime': 9444},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0250382231934036,-0.0867198072157645,0.1446783596310283,0.2154342792328944,0.2082350769040751,0.2735256294185125,0.211258930052863,0.1217302891170816,0.2374113503693337,0.1508494586052872,0.179289584305289,0.1853708111279587,-0.0855537021116274,0.2979069260918492,0.3049253581310251,0.2977352576513437,0.2316619578630318,0.2431334723689034,0.0144545555615131,0.0780248679365536,0.0535749565108085,0.0849975714401684,0.0404862964627002,-0.0041192979484144,0.1106823066997365,,roberta-large_fmri-hm_then_ethics-hm_23-11-30_0803.ckpt,,,,,,,,,,,,,,,-1.6137991588252998,-2.210564915252,-1.1633221132527267,-1.3072053124469178,-0.7506934059274173,-0.0815336978452752,0.2105698956702281,-0.2256282603121311,-1.1461605762462814,-0.7287246698846321,-0.5795339992775199,-1.4004159655351804,-1.112912378374705,-0.7763086551006577,-0.0460053623878367,-0.3144218697877221,-0.7641950031672471,-0.5793314433395569,-0.5196612267409917,-0.3499533811308932,-0.4247381856816736,-0.3711611843333715,-1.7296387779181883,-4.119884264687217,-0.7106302108553775,,,,,,0.8093427419662476,0.7096120119094849,0.6986552476882935,,,,0.960145890712738,0.8971213698387146,0.8980017900466919,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-11-30_0958.ckpt,,,,,9.135172474836406e-06,9.135172474836406e-06,,,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-11-30_0958,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.1388656795024871,0.0,,,,,,0.7096120119094849,0.8971213698387146
10,10,,,,,,,0.6299339532852173,0.5115512013435364,0.4786321222782135,0.0741927176713943,0.3034901022911072,0.1485875993967056,,,,,,,,,,,,,,,,,,,,4172.815001726151,31,1701335630.8362486,{'runtime': 4160},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0235262094765072,-0.09504693167066,0.1655072889598296,0.2604541263379807,0.2269781857070123,0.2733871259116047,0.1577627197395428,0.0841126789374966,0.1070385499194652,0.1253318977776656,0.0707953394847629,0.0171230235396255,-0.0648425771174642,0.1345871997282995,0.1597164593885691,0.2952054711169297,0.2261271562136516,0.2502059370923975,0.0361640318610117,0.1034501894400452,0.0552949619014689,0.0956325075203635,0.0222919495106237,0.0863857067401084,0.1345122396653939,,,,,,,,,,,,,,,,,-1.6137802404984374,-2.20833515683897,-1.1603179118278075,-1.3095085627854872,-0.7515084976530655,-0.081006752964613,0.2069731563302932,-0.2256744429387385,-1.146582269425226,-0.7296770510683059,-0.5819965268357385,-1.4068147315883115,-1.115535919591374,-0.7756267054271369,-0.0453329523015706,-0.3123488567011596,-0.7625363712608697,-0.5773782812030805,-0.5194183838390651,-0.3504062631339222,-0.4206705126595693,-0.3717247901156804,-1.7303657949811293,-4.118246524094325,-0.7100748940296528,,,,,,0.5097213983535767,0.5039839744567871,0.359251469373703,,,,0.5904950499534607,0.5053257942199707,0.3617262244224548,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 16}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 32}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 16}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-11-30_0803.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-11-30_0803,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 32, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.6856201887130737,0.0,,,,,,0.5039839744567871,0.5053257942199707
11,11,,,,,,,0.4188034236431122,0.5548432469367981,0.5344933271408081,0.1014572530984878,0.2896444797515869,0.1356947720050811,,,,,,,,,,,,,,,,,,,,2362.000763177872,75,1701286427.059199,{'runtime': 2360},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0242814613028502,-0.0882452356519172,0.2002760982836444,0.3068059793228878,0.2876850804818012,0.3198607867890674,0.2172281744637117,0.142493175127006,0.2808706959676554,0.2475832446720299,0.2463143514812616,0.2427104153808241,-0.0687754560865428,0.2716188907724639,0.2060871872195903,0.3012864545825892,0.3323443736021456,0.3285339523667555,0.0333955904388126,0.0680449761809641,0.0147153031570259,0.0552854221609577,0.0133145631947894,0.121590789839689,0.1731587597082907,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-11-29_1732.ckpt,,,,,,,,,,,,,,,-1.6143861748239066,-2.208595018612308,-1.1606810783705956,-1.3085354596013483,-0.7517561022985153,-0.0795961197755503,0.2045867715352489,-0.2249851239718836,-1.1467335609092713,-0.7300724234024094,-0.5792006564870336,-1.405755193173816,-1.1150911586265773,-0.7754793551153032,-0.0439132296891413,-0.3128079714588017,-0.7623232537716356,-0.5799600176364685,-0.5205976351938844,-0.3498218216995645,-0.4223737132426766,-0.3720393355012283,-1.7306245041081143,-4.118143446440789,-0.7121649108138774,,,,,,0.5071666836738586,0.4979999959468841,0.3368763029575348,,,,0.5181666612625122,0.5,0.3451509773731231,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-11-29_1854.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_Ethics-HM_then_fmri-HM_23-11-29_1854,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.957387149333954,0.0,,,,,,0.4979999959468841,0.5
12,12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4875.865094184876,249,1701284027.5457172,{'runtime': 4874},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0243776829025623,-0.0941678109132181,0.1519832134261574,0.2304154778690037,0.2351939708523927,0.2445997773796945,0.2335372939411562,0.1743428129712249,0.2217177981543934,0.2552266838430074,0.2358809591363189,0.1436390912178644,-0.0402202543079027,0.2778576856804031,0.1694104767354765,0.1779104524461635,0.1657816364461481,0.0634096380305574,0.0142681396433723,0.0341438185067964,0.061884038451969,0.1146131890429349,0.0709385121410503,0.0809172631802081,0.1367950572173882,,,,,,,,,,,,,,,,,-1.6173550153022809,-2.211073157740737,-1.1613764141758267,-1.308460050112357,-0.7518046285605742,-0.0788931519785001,0.2069333065446277,-0.223767885593618,-1.1456253616791194,-0.7283621284226525,-0.589274095622321,-1.4033884515831918,-1.1139072196363524,-0.7760608971443157,-0.0460794141306233,-0.3125189024335197,-0.7635214418456013,-0.5813652700193612,-0.5209220966723991,-0.3497434369546843,-0.4240074724305314,-0.3734358372966193,-1.727313775526671,-4.115557859472046,-0.7178036027814216,,,,,,0.8348143100738525,0.7160476446151733,0.6955580115318298,,,,0.9630333185195924,0.9016667604446412,0.8989951014518738,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-11-29_1732.ckpt,,,,,9.135172474836406e-06,9.135172474836406e-06,,,roberta-large,roberta-large_Ethics-HM_then_fmri-HM_23-11-29_1732,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0016911027487367,0.0,,,,,,0.7160476446151733,0.9016667604446412
13,13,,,,,,,0.4051282107830048,0.5292022824287415,0.5033169984817505,0.0890001580119133,0.3042194843292236,0.1492712795734405,,,,,,,,,,,,,,,,,,,,6967.401952505112,732,1701279112.8575604,{'runtime': 6965},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0209501903292929,-0.1115541129128846,0.0433387138217243,0.1675675498149362,0.1990873853560917,0.2076755007792165,0.1983059020096092,0.046466981383939,0.2074542430544133,0.2147382113974406,0.1980126259795521,0.0827286935234366,-0.0242659193308112,-0.0008307148466091,0.0692528857065416,0.0855716800503127,0.0609270188914654,0.0564748065262366,0.0557420388639785,0.0268362581800004,0.0457297113690946,0.0523646490891524,-0.0314944412184064,-0.0532726482099968,-0.004813362228477,,,,,,,,,,,,,,,,,-1.6104584821998205,-2.2089846048937014,-1.1612251585290831,-1.3123751044498273,-0.7531842589485851,-0.0866201413038216,0.2204520402015253,-0.2311546199327689,-1.1481447364272772,-0.7302826573732193,-0.5841165247200053,-1.4087903055358404,-1.1208688381429543,-0.7768601752615243,-0.0463360115584752,-0.3146052209449355,-0.7661061464602039,-0.5828347268093184,-0.5200516678539286,-0.3506675610854373,-0.4158261747634444,-0.3701505269995042,-1.726324595654321,-4.1145600645453,-0.70337596895935,,,,,,0.8622499704360962,0.7246666550636292,0.6832950711250305,,,,0.9723333716392516,0.9092668294906616,0.9053016901016236,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri_and_ethics_hm_23-11-29_1535.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_fmri_and_ethics_hm_23-11-29_1535,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,3.0278701160568744e-05,0.0,,,,,,0.7246666550636292,0.9092668294906616
14,14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5951.545321464539,372,1701266977.6834176,{'runtime': 5950},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0214883786454356,-0.098116832058214,0.0466620883196877,0.19662081275962,0.2161776472187982,0.2547328928076123,0.2481407862625823,0.2550280908312474,0.2817961580664015,0.3015488081751784,0.2978654499269869,0.269105875295053,-0.0649483764605086,0.3297045951553961,0.2258373543003452,0.1766832706410186,0.1639408021521455,0.1055855847612052,0.0263429093603526,0.063450236146138,0.0203379895768894,0.0829368636721907,0.0352104864676879,0.0218417403480295,0.0775447962450637,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-11-29_1153.ckpt,,,,,,,,,,,,,,,-1.6172510076228144,-2.2136238556576293,-1.1596428166286374,-1.3079343793215237,-0.7523706018750638,-0.0809542516464587,0.200345986747127,-0.2248890150364688,-1.1471598786563195,-0.7371514952043976,-0.5769477781631829,-1.4052358009639314,-1.1195920517507965,-0.7744830250698356,-0.0449705091896905,-0.3128136322406425,-0.7637735760922815,-0.5793409162958827,-0.5167464154223023,-0.3519214564098853,-0.4248372380257648,-0.3734972200166493,-1.7309636263980233,-4.125314387073975,-0.7144061816525586,,,,,,0.8368416428565979,0.7325142025947571,0.7138497829437256,,,,0.9667999744415284,0.9054000377655028,0.9029794931411744,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-11-29_1230.ckpt,,,,,7.856781408072185e-06,7.856781408072185e-06,,,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-11-29_1230,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0001280225842492,0.0,,,,,,0.7325142025947571,0.9054000377655028
15,15,,,,,,,,,,,,,,,,,,,0.4136752486228943,0.5160256624221802,0.4998779296875,0.1421500891447067,0.7474824786186218,0.8954382538795471,,,,,,,,2082.5503146648407,98,1701260990.8589876,{'runtime': 2081},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0238823524989942,-0.098668734018178,0.1586519808177854,0.2261936398603926,0.2206914591277141,0.2734687707559511,0.2190283282579733,0.1529702612385729,0.1552393508778521,0.233534626019899,0.2338041263451868,0.246876948880657,-0.1009778545089311,0.2040663074350334,0.2205469999773726,0.3015020810651426,0.2744771683001156,0.117690233741433,0.0198247922384791,0.0980062616181542,0.0395118337617355,0.0398837245664158,0.0167609446002724,0.1298310938952289,0.163751122029831,,,,,,,,,,,,,,,,,-1.6130164885800302,-2.205312849014727,-1.1604888977094228,-1.3102145516306964,-0.7512392566381334,-0.0796131710375638,0.2074636217201145,-0.2255777396477554,-1.147131040203889,-0.7303602279292778,-0.5822100584210286,-1.4047000780915735,-1.1124566315116242,-0.775699161830214,-0.047283272513382,-0.3140673319582185,-0.7622602527440436,-0.5775951903311802,-0.518604445769165,-0.3495723982287726,-0.4213000251248515,-0.3694955601507166,-1.73093864921968,-4.11364668996964,-0.705561542667791,,,,,,0.5223333835601807,0.4943332970142365,0.3541333973407745,,,,0.5380333065986633,0.4980667233467102,0.3741626441478729,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri-hm_then_ethics-hm_23-11-29_1153.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_fmri-hm_then_ethics-hm_23-11-29_1153,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,1.343380331993103,0.0,,,,,,0.4943332970142365,0.4980667233467102
16,16,,,,,,,,,,,,,,,,,,,0.3999999761581421,0.4797008633613586,0.4628612399101257,0.1384874582290649,0.7500361204147339,0.9012256264686584,,,,,,,,1753.4649102687836,62,1701193974.9931812,{'runtime': 1753},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.025508958214229,-0.0997791679064225,0.0170629348918133,0.2042447263650054,0.1828313356083502,0.2244214575826484,0.1787906479481723,0.1246341401044158,0.2320919689801675,0.2762829122498744,0.244108338747996,0.2397099323847767,-0.0732145495359247,0.1378970523566568,0.0742759847530165,0.0659600139522927,0.0123848857482964,-0.0129434558488803,0.044641896827423,0.0741269592641378,0.059888357004961,0.037622951384061,-0.1024574981735686,-0.0240070087125846,0.0234215857779951,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-11-28_1428.ckpt,,,,,,,,,,,,,,,-1.6136909515282647,-2.2109154759703338,-1.160421390472642,-1.3076289598687558,-0.7522969950223013,-0.0846838198843003,0.2114304242181999,-0.2280584762061344,-1.148281367569417,-0.7304341308541,-0.5890671678801258,-1.404511713904374,-1.113336285452,-0.7759070020953989,-0.0444827916685681,-0.3116280901214421,-0.7619934030259188,-0.5753083900616469,-0.5193969870245436,-0.3505093637934173,-0.4228886911180844,-0.3716771317969964,-1.729472972470612,-4.1183002854858985,-0.7056562401582704,,,,,,0.8480000495910645,0.7219999432563782,0.6737710237503052,,,,0.9706334471702576,0.851933479309082,0.8364242315292358,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-11-28_1723.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_Ethics-HM_then_fmri-HM_23-11-28_1723,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,1.9866490364074707,0.0,,,,,,0.7219999432563782,0.851933479309082
17,17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8600.898707151413,282,1701190313.977652,{'runtime': 8600},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0254672453578287,-0.0951825903368938,0.1222408860605704,0.2436812521171982,0.2493295995734161,0.2717253461726208,0.2371880027188491,0.1323561308726809,0.1949348882833324,0.2354591631468431,0.2093721088419898,0.2682551541147621,-0.0659145200390437,0.2851684021423386,0.1997932648724214,0.1261273677974619,0.1008227116505026,0.0984862513303337,0.0484919811549076,0.0615919361633281,0.0441831651842972,0.1075807946886458,0.0418061690011628,0.0205194306539432,0.1357705224518346,,,,,,,,,,,,,,,,,-1.6155015012268057,-2.2133631372750853,-1.1607037618987115,-1.307172720296823,-0.7520348264140908,-0.0826749618874818,0.2098224569712786,-0.2270517457683685,-1.147173461652316,-0.7314136151264623,-0.5885013893553237,-1.403764660757572,-1.1132318700606527,-0.7754123229140395,-0.0448526231880674,-0.311901221366899,-0.7624054733403076,-0.5749790172170537,-0.5192084945835354,-0.3503567013342337,-0.4240106755735846,-0.3728128510891018,-1.7301222285541495,-4.119954793412913,-0.7077971391832869,,,,,,0.8364237546920776,0.7298855781555176,0.712294340133667,,,,0.9649999737739564,0.895466685295105,0.8918060064315796,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_Ethics-HM_then_fmri-HM_23-11-28_1428.ckpt,,,,,8.86384871716129e-06,8.86384871716129e-06,,,roberta-large,roberta-large_Ethics-HM_then_fmri-HM_23-11-28_1428,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.000707154336851,0.0,,,,,,0.7298855781555176,0.895466685295105
18,18,,,,,,,,,,,,,,,,,,,0.3970085680484772,0.4323361814022064,0.4070411026477813,0.0976183041930198,0.7632268071174622,0.9326249957084656,,,,,,,,6375.32038640976,334,1701181680.8938644,{'runtime': 6374},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0250207853548291,-0.0856684213767752,0.0879242021058418,0.1891282583248274,0.1649797607792787,0.1713919792628796,0.1483084931380069,0.1747189520298331,0.2610122692646254,0.1884247020164116,0.21339006558669,0.1688810235265027,-0.0478650517066901,0.1975868701011491,0.1162969453745777,0.0737454011074049,0.0798696688168644,0.1120793711438615,0.0089205407788612,0.0578554950048705,0.0136204819158116,0.0288894106493373,0.0541521025121079,0.0072942151478843,0.0592112797028165,,,,,,,,,,,,,,,,,-1.6130375710893592,-2.208668459547233,-1.1607950409564345,-1.307300322476974,-0.7509513433778949,-0.0808661483493806,0.2024798032026952,-0.221756999095446,-1.1455942233398675,-0.7282291656037747,-0.5830061523398768,-1.4094657839185132,-1.1100675136687044,-0.7748610303696106,-0.0440744224090976,-0.3103236559181124,-0.7639360631121435,-0.5820542995471323,-0.5208692158067181,-0.3508053469645866,-0.4218128560348857,-0.3729211430119399,-1.7341944722011775,-4.121432537536837,-0.7053681253956579,,,,,,0.8383333683013916,0.6988332867622375,0.6524568200111389,,,,0.9744666814804076,0.9144001603126526,0.9114811420440674,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/roberta-large_fmri_and_ethics_hm_23-11-28_1241.ckpt,,,,,1e-05,1e-05,1e-05,1e-05,roberta-large,roberta-large_fmri_and_ethics_hm_23-11-28_1241,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 7, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0190411005169153,0.0,,,,,,0.6988332867622375,0.9144001603126526
20,20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8135.478876590729,282,1700933924.4364617,{'runtime': 8134},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0240583899103545,-0.090280350536092,-0.0328362120975021,0.1148247336453729,0.1677321681948362,0.134078025851195,-0.0519434582443836,0.0468738722162735,0.0969151983478006,0.1410619917499656,0.1522542573180056,0.1179243169367623,-0.0663424115305906,0.1453475300993094,0.1381450964763271,0.1724250007320975,0.1822607014072994,0.1954638013688874,0.0211078642605459,0.0172218954552041,-0.0490099436254565,-0.0664004685953126,-0.1478176876518308,-0.0162551544345799,0.0290233365804339,,artifacts/RoBERTa-transfer.ckpt,,,,,,,,,,,,,,,-1.6132857715972753,-2.2073069003353667,-1.161742461169344,-1.3087781507626457,-0.7516037631552006,-0.0820304455634233,0.2095644665077778,-0.2266427021638597,-1.150409320358671,-0.7298369130760376,-0.5812432920916797,-1.4052369228495762,-1.1125681317638,-0.7754066511785034,-0.0464865895433392,-0.3156006531496826,-0.7609915063767896,-0.5782169517801228,-0.5208718385311983,-0.3508902422563547,-0.4225533905964509,-0.3691895544919645,-1.7293299932384647,-4.118776254593421,-0.7073981886329572,,,,,,0.8214333653450012,0.6837714314460754,0.6523687839508057,,,,0.9674666523933412,0.8958001732826233,0.8934198021888733,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,9.70299e-06,9.70299e-06,,,roberta-large,"roberta-large, fmri-HM then Ethics-HM 23-11-25 15:22",,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0160288214683532,0.0,,,,,,0.6837714314460754,0.8958001732826233
21,21,,,,,,,0.4029915034770965,0.5477208495140076,0.5134513974189758,0.110610507428646,0.2831067442893982,0.130011573433876,,,,,,,,,,,,,,,,,,,,5389.620526790619,326,1700925766.4449048,{'runtime': 5388},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0228732971366341,-0.088310583020068,-0.0617141815624038,0.0205886462088598,0.0344435721786047,-0.006322623349518,-0.0800567443527398,0.1177363467222041,0.1557613886556741,0.1762176056908204,0.1757022008011809,0.1685738481738062,-0.08198783743753,0.0766430333564133,0.1507434278771007,0.2908523197005816,0.2973224635964698,0.3117070767823198,0.0181015680007698,0.0122898209931593,-0.0285318764385367,-0.1225808150048812,-0.1772909840093006,-0.00832675501572,-0.1027686706031549,,,,,,,,,,,,,,,,,-1.615552080166088,-2.209410678253221,-1.1608521021115386,-1.3091487521262395,-0.751651270493807,-0.079466772670534,0.2071447289756971,-0.2258219943935775,-1.1481959958911157,-0.7294141202364635,-0.5830035017288631,-1.4054833204155948,-1.1162571428901198,-0.7757546783800533,-0.0460633809035624,-0.3133187356378979,-0.7616114828070866,-0.5776585780082504,-0.519890874263133,-0.3515981277759299,-0.4218771386918636,-0.3724687542464031,-1.730282483675914,-4.1206274204759445,-0.7113912338361752,,,,,,0.4923332929611206,0.5019999742507935,0.3244191110134125,,,,0.4635332524776459,0.5,0.3182210922241211,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/RoBERTa-transfer.ckpt,,,,,8.86384871716129e-06,8.86384871716129e-06,8.86384871716129e-06,8.86384871716129e-06,roberta-large,"roberta-large, fmri-HM then Ethics-HM 23-11-25 13:52",,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.7352885007858276,0.0,,,,,,0.5019999742507935,0.5
22,22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4958.03945016861,170,1700920355.8528512,{'runtime': 4957},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0245467462002721,-0.102177797140534,0.1507468724604944,0.2141917500613097,0.1914429083150352,0.2308737300845273,0.0866189709160829,0.0262826559453931,0.0678586084416479,0.1096069432990503,0.0330310259856009,0.17927511896573,-0.0765349696798164,0.179035587163331,0.1216238747749043,0.2221822084101524,0.1850772192859585,0.2332410632031476,0.0210221387994949,0.0581969285387943,0.0727402566127141,0.067243074405369,0.0382180794470106,0.00461333197068,0.0957661801783622,,,,,,,,,,,,,,,,,-1.614143898441394,-2.207513932494668,-1.1603093632114767,-1.3090574288074337,-0.7512958992446606,-0.0798863433064351,0.206112457727751,-0.224394572300481,-1.146716432418336,-0.7316438603472295,-0.5775176680137653,-1.404650631538471,-1.1138963852751838,-0.7764377288059487,-0.0436208260241395,-0.312146667606175,-0.7639888378919815,-0.5751825445504615,-0.5188402948456254,-0.3500535865498826,-0.4225979311047199,-0.3708369651766074,-1.7289388273139816,-4.119926664283553,-0.711271444204703,,,,,,0.8188997507095337,0.7050762176513672,0.6772086024284363,,,,0.9723333716392516,0.9055335521697998,0.9006666541099548,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1e-05,1e-05,,,roberta-large,"roberta-large, (fmri and Ethics)-HM 23-11-25 12:29",,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 7, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0105487955734133,0.0,,,,,,0.7050762176513672,0.9055335521697998
27,27,,,,,,,,,,,,,,,,,,,0.5933674573898315,0.5063095092773438,0.4558302760124206,0.1190103441476822,0.7358659505844116,0.8705196380615234,,,,,,,,148.2829713821411,10,1700911962.5454245,{'runtime': 148},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5499786734580994,0.5228332877159119,0.4159770905971527,,,,0.7622441649436951,0.6032092571258545,0.5370250344276428,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 20}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,bert-base-cased,proud-mountain-1044,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 20, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 2, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,2.0839014053344727,0.0,,,,,,0.5228332877159119,0.6032092571258545
28,28,,,,,,,,,,,,,,,,,,,0.3422069251537323,0.2378205358982086,0.2103311419486999,0.0553326271474361,0.7852998375892639,0.9855875372886658,,,,,,,,78.75860667228699,31,1700911759.0529256,{'runtime': 78},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5083190202713013,0.4895428717136383,0.4445077180862427,,,,0.4014137089252472,0.4166666865348816,0.3823953866958618,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,2e-05,2e-05,2e-05,2e-05,bert-base-cased,lively-field-1043,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.9551817178726196,0.0,,,,,,0.4895428717136383,0.4166666865348816
29,29,,,,,,,,,,,,,,,,,,,0.4640086889266968,0.3000000119209289,0.2290849834680557,0.1564229875802993,0.7275493144989014,0.8520244359970093,,,,,,,,304.15432596206665,50,1700911304.038074,{'runtime': 303},/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5146931409835815,0.5,0.3401784896850586,,,,0.5985357165336609,0.5,0.3461896479129791,/workspace/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,bert-base-cased,classic-voice-1039,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,2.3362205028533936,0.0,,,,,,0.5,0.5
30,30,,,,,,,,,,,,,,,,,,,0.3276698291301727,0.2833333313465118,0.2009162008762359,0.0698113664984703,0.756767988204956,0.9166369438171388,,,,,,,,2839.4242367744446,5,1698088059.8852549,{'runtime': 2839},/Users/ajmeek/PycharmProjects/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4660245776176452,0.5,0.3401784896850586,,,,0.4753170907497406,0.5,0.3461896479129791,/Users/ajmeek/PycharmProjects/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,bert-base-cased,ancient-pond-1036,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,2.337463855743408,0.0,,,,,,0.5,0.5
31,31,,,,,,,,,,,,,,,,,,,0.5916516184806824,0.5816559195518494,0.5365456938743591,0.1518367528915405,0.7329474091529846,0.8621099591255188,,,,,,,,350.0743124485016,34,1697290535.6947825,{'runtime': 349},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5585340857505798,0.5348985195159912,0.4974630177021026,,,,0.7947016358375549,0.6993557810783386,0.6893270611763,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 16}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 32}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 16}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,True,,False,,,,,,,2e-05,2e-05,2e-05,2e-05,bert-base-cased,snowy-smoke-1035,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,2.05515718460083,0.0,,,,,,0.5348985195159912,0.6993557810783386
32,32,,,,,,,,,,,,,,,,,,,0.6095253825187683,0.564047634601593,0.5187015533447266,0.1386710405349731,0.7313676476478577,0.8605422973632812,,,,,,,,498.7980980873108,50,1697203255.86921,{'runtime': 498},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5579070448875427,0.5289405584335327,0.4789683222770691,,,,0.8022892475128174,0.6998220682144165,0.6864106059074402,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 20}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,bert-base-cased,fresh-snow-1026,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 20, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,1.84742283821106,0.0,,,,,,0.5289405584335327,0.6998220682144165
33,33,,,,,,,,,,,,,,,,,,,0.4076556861400604,0.2651709616184234,0.2280066311359405,0.0635792762041091,0.7846009135246277,0.9845790266990662,,,,,,,,106.80224704742432,32,1697202629.877124,{'runtime': 106},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5404333472251892,0.5356476902961731,0.4899471700191498,,,,0.504092276096344,0.5595238208770752,0.5595238208770752,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,2e-05,2e-05,2e-05,2e-05,bert-base-cased,wise-fog-1024,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.9406831860542296,0.0,,,,,,0.5356476902961731,0.5595238208770752
34,34,,,,,,,,,,,,,,,,,,,0.4509666562080383,0.3000000119209289,0.2290849834680557,0.1554647386074066,0.7275786399841309,0.8523438572883606,,,,,,,,269.8547613620758,50,1697201493.3543394,{'runtime': 269},artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.499413400888443,0.5,0.3401784896850586,,,,0.594759464263916,0.5,0.3461896479129791,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fns': ['cross_entropy'], 'revision': 'refs/pr/3', 'input_col': 'input', 'label_cols': ['label'], 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fns': ['mse_loss', 'cross_entropy'], 'revision': None, 'input_col': 'input', 'label_cols': ['label', 'behavior'], 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,1.772769743432258e-05,bert-base-cased,dauntless-vortex-1019,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,2.325981616973877,0.0,,,,,,0.5,0.5
35,35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5404.60195016861,168,1697051065.817655,{'runtime': 5404},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8312333226203918,,0.7154094576835632,,0.6913905143737793,,,,0.970566749572754,0.9126667976379396,0.91079843044281,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'RAW', 'path': 'data/ds000212/ds000212_raw', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,False,,,,,,,1e-05,1e-05,,,roberta-large,skilled-terrain-1006,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 2, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 7, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0040229544974863,0.0,,,,,,0.7154094576835632,0.9126667976379396
39,39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2085.7484085559845,59,1696609613.9296725,{'runtime': 2085},/workspace/brainbias/artifacts,,,0.2959424330565122,0.3190624224469908,0.2777876938481649,0.2679690757320042,0.2333917744825067,0.2054469652665392,0.2639484408585256,0.1360810123065997,0.1915934778638068,0.1040695773253727,0.042772118724419,0.1766858665263333,0.1424381407086285,0.0023781867999442,0.0706314990587121,0.1260027156250806,0.0455566727674297,0.162110532037925,0.1740158467679231,0.1414245415647379,0.2460174282151377,0.2065229808689641,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8399333953857422,,0.7414857149124146,,0.7256808280944824,,,,0.9554563164711,0.9003307819366455,0.8922266960144043,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,1e-05,1e-05,,,roberta-large,robust-star-1001,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 7, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.1694328337907791,0.0,,,,,,0.7414857149124146,0.9003307819366455
41,41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5468.635464429855,165,1696601082.4493434,{'runtime': 5468},/workspace/brainbias/artifacts,,,0.3267608084970115,0.2885356017165191,0.2836891353904222,0.2935907117986149,0.2506708461833131,0.3398903126493725,0.3262187267830805,0.3064575561090449,0.3321534763557535,0.3087247408017453,0.1107074010554466,0.1484748459863028,0.2825265623487368,0.2458787542697849,0.370071213184112,0.1079332274508031,0.0971723806520187,0.1617646429953355,0.1238251552760398,0.1692612500244689,0.2780039425475231,0.2069408205800203,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8282334208488464,,0.7071143388748169,,0.685374915599823,,,,0.958465576171875,0.9177249670028688,0.9107012152671814,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,7.936142836436552e-06,7.936142836436552e-06,,,roberta-large,stellar-pine-999,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 8, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0116498209536075,0.0,,,,,,0.7071143388748169,0.9177249670028688
42,42,0.1749057024717331,,0.271648108959198,,0.1201494485139846,,,,,,,,,,,,,,,,,,,,,,,,,,,2212.9589269161224,43,1696594957.5931609,{'runtime': 2212},/workspace/brainbias/artifacts,,,0.2963387962410311,0.306218407266386,0.3385540199861668,0.3306054826355948,0.3699867384096016,0.3150349662659547,0.3100114056812877,0.3343509131441051,0.3438555691042052,0.3461203391314674,0.098044570331895,0.3365535006128693,0.3382578900895894,0.3305566656134086,0.2079217652416968,0.0904383050724731,0.1096856431280409,0.1809960030132424,0.1396747030473747,0.2133487108314967,0.2625312171921344,0.2564223198880576,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4833285808563232,,0.5,,0.3387535810470581,,,,0.4959941506385803,0.5,0.3444494009017944,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,8.016305895390456e-06,8.016305895390456e-06,8.016305895390456e-06,,roberta-large,restful-sound-997,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.1319766640663147,0.0,,,,,,0.5,0.5
44,44,0.1751546114683151,,0.271703153848648,,0.1201904416084289,,,,,,,,,,,,,,,,,,,,,,,,,,,1965.5090670585632,42,1696590770.975379,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.497223824262619,,0.5043714046478271,,0.3806816041469574,,,,0.5025173425674438,0.4913079440593719,0.3689289391040802,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,8.016305895390456e-06,8.016305895390456e-06,8.016305895390456e-06,,roberta-large,golden-bush-995,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.1314861327409744,0.0,,,,,,0.5043714046478271,0.4913079440593719
45,45,,,,,,,,,,,,,0.1712346822023391,,0.7402589321136475,,0.8785380125045776,,,,,,,,,,,,,,,1892.7275557518003,42,1696588695.8756657,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4954047501087188,,0.4970000088214874,,0.3298275470733642,,,,0.4682038128376007,0.4955593645572662,0.3210894167423248,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,8.016305895390456e-06,8.016305895390456e-06,8.016305895390456e-06,,roberta-large,cool-glade-994,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.9808571934700012,0.0,,,,,,0.4970000088214874,0.4955593645572662
46,46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2360.17678141594,63,1696586761.2142663,{'runtime': 2359},/workspace/brainbias/artifacts,,,0.3382283675072421,0.322247003813126,0.329399054297205,0.3044147940572089,0.3263145761914081,0.3247020924448874,0.3020670408378793,0.3318312275772029,0.3467327519625064,0.3200939276231695,0.0861452022464763,0.2933137553583365,0.2773057406110295,0.1426194795932443,0.2732063332941228,0.1291595871551578,0.0409141868380347,0.2604177155912326,0.262745459108916,0.1877742413171337,0.2878915735753397,0.2329861158216615,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4914186298847198,,0.5,,0.325551301240921,,,,0.5355622172355652,0.5,0.3172438740730285,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,2.2301757365478138e-07,2.2301757365478138e-07,,,roberta-large,still-puddle-993,,,,0.0,,"{'adamw': {'lr': 2.754228703338167e-07, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.7309180498123169,0.0,,,,,,0.5,0.5
47,47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2096.3066215515137,63,1696582887.2947435,{'runtime': 2095},/workspace/brainbias/artifacts,,,0.3239755309657212,0.3692998355566265,0.3573745036637867,0.3296738922478242,0.3806043471073729,0.3616381784992513,0.3313528722980261,0.3943667300768533,0.3988046960815988,0.3953116658082224,0.0844948092938022,0.3704708011160635,0.3765410736222029,0.2632185085020519,0.3413442328267101,0.0831831842318277,0.0438847824949842,0.2248889155768547,0.2734221091709338,0.2257466650664121,0.3192763096677163,0.2378088696391221,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5167930126190186,,0.4968096315860748,,0.36232990026474,,,,0.4888904690742492,0.4950557351112366,0.3686170876026153,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,8.097278682212583e-06,8.097278682212583e-06,,,roberta-large,fallen-smoke-991,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.714719295501709,0.0,,,,,,0.4968096315860748,0.4950557351112366
48,48,0.1678082644939422,,0.2728704214096069,,0.1211522668600082,,,,,,,,,,,,,,,,,,,,,,,,,,,2228.957775115967,43,1696580733.4583762,{'runtime': 2228},/workspace/brainbias/artifacts,,,0.3239755309657212,0.3692998355566265,0.3573745036637867,0.3296738922478242,0.3806043471073729,0.3616381784992513,0.3313528722980261,0.3943667300768533,0.3988046960815988,0.3953116658082224,0.0844948092938022,0.3704708011160635,0.3765410736222029,0.2632185085020519,0.3413442328267101,0.0831831842318277,0.0438847824949842,0.2248889155768547,0.2734221091709338,0.2257466650664121,0.3192763096677163,0.2378088696391221,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5276714563369751,,0.5,,0.3245911002159118,,,,0.5038683414459229,0.5,0.3172438740730285,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,8.016305895390456e-06,8.016305895390456e-06,8.016305895390456e-06,,roberta-large,giddy-hill-990,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.1318253427743911,0.0,,,,,,0.5,0.5
49,49,,,,,,,,,,,,,0.1641417443752288,,0.7418690919876099,,0.8821674585342407,,,,,,,,,,,,,,,2185.7181589603424,43,1696578460.20206,{'runtime': 2185},/workspace/brainbias/artifacts,,,0.3665526331569999,0.3386283415030718,0.285838194652858,0.3135772294111345,0.2976196730774814,0.2452055642527575,0.2942843969127187,0.3061815217734468,0.2497085014137597,0.3012060817619921,0.1059875593305192,0.3050718669593882,0.3680742315707804,0.3495367031548565,0.3742400042155647,0.1462948842574434,0.1321960857574033,0.3154483033814802,0.3162109760278601,0.261494723877235,0.2356705122565108,0.2292200561550285,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/RoBERTa_3hours_on_ethics_only.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8206333518028259,,0.7023620009422302,,0.6800127029418945,,,,0.939642071723938,0.8662060499191284,0.8599669337272644,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,8.016305895390456e-06,8.016305895390456e-06,8.016305895390456e-06,,roberta-large,restful-feather-989,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 55, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.978830873966217,0.0,,,,,,0.7023620009422302,0.8662060499191284
50,50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4256.626519441605,184,1696534434.5756226,{'runtime': 4256},/workspace/brainbias/artifacts,,,0.3473043422454999,0.2967526012938548,0.3092003906104772,0.2480138127407254,0.181580413700863,0.2024923352644147,0.3126199315528093,0.2026862641359042,0.160973788684397,0.2391122223201802,0.1150914765167293,0.241944797210317,0.1859235061207848,0.282720802860154,0.1342629016806365,0.1155094531878395,0.0479694610079208,0.2711054722914587,0.2703136777095577,0.1397917574518335,0.1957311821800986,0.1915450361791642,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8096543550491333,,0.7204018831253052,,0.7092084884643555,,,,0.9650220274925232,0.901766836643219,0.9031136631965636,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,7.856781408072185e-06,7.856781408072185e-06,,,roberta-large,splendid-bird-988,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0071256328374147,0.0,,,,,,0.7204018831253052,0.901766836643219
51,51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10295.013877630234,184,1696530125.8126576,{'runtime': 10294},/workspace/brainbias/artifacts,,,0.3473043422454999,0.2967526012938548,0.3092003906104772,0.2480138127407254,0.181580413700863,0.2024923352644147,0.3126199315528093,0.2026862641359042,0.160973788684397,0.2391122223201802,0.1150914765167293,0.241944797210317,0.1859235061207848,0.282720802860154,0.1342629016806365,0.1155094531878395,0.0479694610079208,0.2711054722914587,0.2703136777095577,0.1397917574518335,0.1957311821800986,0.1915450361791642,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8067980408668518,,0.7215041518211365,,0.7104514837265015,,,,0.966066837310791,0.8968977332115173,0.8957172632217407,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,7.856781408072185e-06,7.856781408072185e-06,,,roberta-large,woven-night-987,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0031903174240142,0.0,,,,,,0.7215041518211365,0.8968977332115173
52,52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4225.638116598129,184,1696519799.6168425,{'runtime': 4225},/workspace/brainbias/artifacts,,,0.3382283885028153,0.322247023816653,0.3293990747446967,0.3044148129537992,0.3263145964474306,0.3247021126008144,0.3020670595887331,0.3318312481756724,0.3467327734859901,0.3200939474930443,0.0861452075939506,0.2933137735658315,0.2773057578248856,0.1426194884463064,0.2732063502534416,0.1291595951727603,0.0409141893777872,0.260417731756697,0.2627454754188759,0.1877742529732453,0.2878915914462499,0.2329861302843073,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.492218554019928,,0.5002519488334656,,0.3428170382976532,,,,0.4865575730800628,0.4990079402923584,0.3485328257083893,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,7.856781408072185e-06,7.856781408072185e-06,,,roberta-large,fresh-mountain-986,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.8289176821708679,0.0,,,,,,0.5002519488334656,0.4990079402923584
53,53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,917.357797384262,30,1696515509.9806144,{'runtime': 916},/workspace/brainbias/artifacts,,,0.3382283885028153,0.322247023816653,0.3293990747446967,0.3044148129537992,0.3263145964474306,0.3247021126008144,0.3020670595887331,0.3318312481756724,0.3467327734859901,0.3200939474930443,0.0861452075939506,0.2933137735658315,0.2773057578248856,0.1426194884463064,0.2732063502534416,0.1291595951727603,0.0409141893777872,0.260417731756697,0.2627454754188759,0.1877742529732453,0.2878915914462499,0.2329861302843073,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5052210092544556,,0.4998582899570465,,0.3324834406375885,,,,0.5532541275024414,0.4998582899570465,0.322857916355133,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/RoBERTa-Ethics-fmri.ckpt,,,,,2.3808428509309656e-05,2.3808428509309656e-05,,,roberta-large,laced-shape-985,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 15, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.8610630631446838,0.0,,,,,,0.4998582899570465,0.4998582899570465
54,54,,,,,,,,,,,,,0.1299133449792862,,0.7351474165916443,,0.870998203754425,,,,,,,,,,,,,,,2243.750134944916,60,1696448663.096457,{'runtime': 2243},/workspace/brainbias/artifacts,,,0.3126806229773663,0.3385966943558591,0.3502626769158152,0.3176381259404611,0.3361444678775614,0.3400949560685249,0.295280796908554,0.2815801808243228,0.2920276935881148,0.3100267528224743,0.0386142218736275,0.2849374368478668,0.3308680185371035,0.2439521246089871,-0.0137607236059599,0.1886357687159574,0.09700925224247,0.282343349578929,0.1528435788016726,0.1191639683560405,0.1940763113731122,0.2480422871292241,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.498478353023529,,0.5,,0.327863872051239,,,,0.4601143300533294,0.5,0.316671758890152,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,2.3808428509309656e-05,2.3808428509309656e-05,2.3808428509309656e-05,,roberta-large,earthy-wave-984,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.9170128703117372,0.0,,,,,,0.5,0.5
55,55,0.1699181050062179,,0.2774160504341125,,0.124864399433136,,,,,,,,,,,,,,,,,,,,,,,,,,,2243.6332664489746,60,1696446397.5908206,{'runtime': 2243},/workspace/brainbias/artifacts,,,0.3749453405400899,0.3503474697309174,0.366210042509004,0.3179936698606042,0.3391119715857305,0.3496581781153095,0.338086277683165,0.3628964085493228,0.3799770943385098,0.342760343743856,0.1113082345828962,0.3524193055027754,0.3914846604353941,0.3320115671251123,0.31227368341888,0.1373491990601795,0.0397676820244666,0.2522679331055685,0.2679820885735969,0.3108785625845842,0.3343887661520009,0.2068991694348285,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_roberta-large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5426263809204102,,0.5,,0.3334672152996063,,,,0.6863453388214111,0.5,0.3472205996513366,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,,2.3808428509309656e-05,2.3808428509309656e-05,2.3808428509309656e-05,,roberta-large,colorful-wind-983,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.1018923074007034,0.0,,,,,,0.5,0.5
56,56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4594.457991838455,12,1696444128.019307,{'runtime': 4594},/workspace/brainbias/artifacts,,,0.3382283885028153,0.322247023816653,0.3293990747446967,0.3044148129537992,0.3263145964474306,0.3247021126008144,0.3020670595887331,0.3318312481756724,0.3467327734859901,0.3200939474930443,0.0861452075939506,0.2933137735658315,0.2773057578248856,0.1426194884463064,0.2732063502534416,0.1291595951727603,0.0409141893777872,0.260417731756697,0.2627454754188759,0.1877742529732453,0.2878915914462499,0.2329861302843073,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5622308254241943,,0.4990706443786621,,0.3903071880340576,,,,0.7235457301139832,0.5,0.3482374250888824,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 512}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 512}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[500:2000]', 'batch_size': 512}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,True,,artifacts/train_head_on_ethics_roberta-large.ckpt,,,,,0.0018926429760766,0.0018926429760766,,,roberta-large,clear-star-982,,,,0.0,,"{'adamw': {'lr': 0.0022908676527677745, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'lr_base_model_factor': 1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.6627026200294495,0.0,,,,,,0.4990706443786621,0.5
59,59,,,,,,,,,,,,,0.1401772201061248,,0.7299309372901917,,0.8586615324020386,,,,,,,,,,,,,,,2226.0437762737274,60,1696430354.8933172,{'runtime': 2225},/workspace/brainbias/artifacts,,,0.3356068434300477,0.3354080194672112,0.3018440276506819,0.2915233931702566,0.2999658806248197,0.3249842498274803,0.3311252951824707,0.30737234259327,0.358775972916218,0.309773165029189,0.1212940347270256,0.3393505127126982,0.3054835750390689,0.3796122481363014,0.3521212743820887,0.1236534617826226,0.0534053084313851,0.2280153016071826,0.0467738913900243,0.219127164685945,0.257471607968822,0.2365359269930809,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4964118599891662,,0.5006285905838013,,,,,,0.4797245562076568,0.5010448694229126,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,7.105532272722918e-06,,,,,roberta-large,polar-planet-976,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.9302008152008056,0.0,,,,,,0.5006285905838013,0.5010448694229126
60,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7028.423704385757,127,1696428106.9987323,{'runtime': 7028},/workspace/brainbias/artifacts,,,0.3633453409332048,0.396574067069308,0.3402714568544661,0.3171869262434517,0.3133927454666199,0.3324968648506184,0.3474279851541144,0.1858913707835642,0.2057577681265116,0.2419665048440042,0.053306143937274,0.2048713920348113,0.2544697021225219,0.1076171707209396,0.2167064994112036,0.1041994805908133,0.029137106035159,0.1687186534919845,0.1542823699693249,0.1660551004612196,0.2808825991866245,0.2439226385552978,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8173218369483948,,0.7316893339157104,,,,,,0.9593263864517212,0.8956918716430664,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,7.105532272722918e-06,,,,,roberta-large,azure-dust-975,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.1075806766748428,0.0,,,,,,0.7316893339157104,0.8956918716430664
61,61,0.1709065735340118,,0.2773249745368957,,0.124763511121273,,,,,,,,,,,,,,,,,,,,,,,,,,,2215.8219594955444,60,1696421056.5767975,{'runtime': 2215},/workspace/brainbias/artifacts,,,0.2940426462071773,0.3010681456378557,0.2527664065233944,0.302096449115358,0.2819139757937158,0.3597311887632455,0.2717897745153098,0.3194927527040357,0.3251140214452113,0.3014204946508654,0.079390789052709,0.3548245667294022,0.3193857004796062,0.3060631965677405,0.2893203541854603,0.1211155130620079,0.1012067262014034,0.1444026645247293,0.0756773547130582,0.126380079375632,0.2536423474265903,0.248186703030375,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_roberta-large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.7502238154411316,,0.5,,,,,,0.8310742378234863,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,7.105532272722918e-06,,,,,roberta-large,super-planet-974,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.1116561666131019,0.0,,,,,,0.5,0.5
62,62,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7031.11376452446,127,1696418812.3009963,{'runtime': 7030},/workspace/brainbias/artifacts,,,0.3403408492309102,0.3330868683852706,0.3307725287854838,0.2635545226793689,0.196471986980213,0.2616025025526643,0.2461973094192097,0.2024461378323756,0.2699253797716014,0.1159434026542636,0.0819182114710191,0.1480532727232523,0.0869958711287039,-0.0028759760884771,0.1590093371208471,0.140983851291899,0.0777241212585338,0.2232193390347747,0.1834333282250665,0.1180952584398044,0.2514520595071947,0.2241878718130952,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.8267990946769714,,0.734114408493042,,,,,,0.9647541642189026,0.9020938277244568,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:80%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/train_head_on_ethics_roberta-large.ckpt,,,,7.105532272722918e-06,,,,,roberta-large,noble-smoke-973,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0004474793386179,0.0,,,,,,0.734114408493042,0.9020938277244568
63,63,0.172838807106018,,0.2771302759647369,,0.1246214658021926,,,,,,,,,,,,,,,,,,,,,,,,,,,2222.5290319919586,60,1696411760.196316,{'runtime': 2222},/workspace/brainbias/artifacts,,,0.3034749991550626,0.2954450308742321,0.2661656951439136,0.268037785983634,0.2920392604175698,0.2927688358055397,0.2870797197664294,0.2770957024420944,0.3313659839097765,0.3410581857129908,0.1319087094160743,0.3337793427892308,0.3168599505028585,0.2213855833308524,0.329209227395555,0.1312354579391534,0.0765839843967238,0.2514370403473745,0.2189451763545365,0.2139900425907412,0.1890499012344028,0.2109263086638237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4714189767837525,,0.5,,,,,,0.4455546438694,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,7.105532272722918e-06,,,,,roberta-large,expert-water-972,,,,0.0,,"{'adamw': {'lr': 1e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.1318855732679367,0.0,,,,,,0.5,0.5
65,65,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13518.855049610138,405,1696408727.4811575,{'runtime': 13518},/workspace/brainbias/artifacts,,,0.2051482539078122,0.2805008846670336,0.2514189102484104,0.3172963271021132,0.2150823802631315,0.3348086783298541,0.3385739205452974,0.336782196896906,0.3321632458214854,0.3350256968717293,0.0825429242031793,0.3316931606922085,0.3632626208115044,0.3636464835507143,0.3629412075590681,0.1126346371894183,0.2825612537296941,-0.0371883597874762,0.0602242259072669,0.1272471352312148,0.1174052107663247,0.2155956944943899,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5173333287239075,,0.4979999959468841,,,,,,0.5127777457237244,0.5013333559036255,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 4}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/train_head_on_ethics_DEBERTA.ckpt,,,,4.714068844843314e-06,,,,,microsoft/deberta-v2-xlarge,ancient-glitter-969,,,,0.0,,"{'adamw': {'lr': 6e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 4, 'token_location': 0, 'lr_warm_up_steps': 0.5, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 2, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.6854463219642639,0.0,,,,,,0.4979999959468841,0.5013333559036255
66,66,0.1742945462465286,,0.2773292362689972,,0.1247709318995475,,,,,,,,,,,,,,,,,,,,,,,,,,,6646.211628198624,220,1696377542.0770931,{'runtime': 6645},/workspace/brainbias/artifacts,,,0.0570697160646096,-0.1092020367445453,0.1519402804480636,0.1042063511528728,0.1137547245745434,0.0881360770753032,0.2018201565485832,0.2271927653831734,0.307082019630417,0.3329782742510823,-0.0002674415277525,0.3777265936889092,0.3715757486845242,0.376358065882721,0.3766978902489813,-0.0621432634037048,0.3075390453179245,0.0325542604211911,0.16237119135258,0.1942058982044178,0.1822284861042115,0.0134171586145732,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_DEBERTA.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.418500006198883,,0.484499990940094,,,,,,0.479333370923996,0.4986666738986969,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 2}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 4}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,4.263319363633753e-06,,,,,microsoft/deberta-v2-xlarge,devoted-puddle-968,,,,0.0,,"{'adamw': {'lr': 6e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 4, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 4, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.1072361916303634,0.0,,,,,,0.484499990940094,0.4986666738986969
67,67,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13131.000771045685,336,1696370837.9160771,{'runtime': 13130},/workspace/brainbias/artifacts,,,0.3094017343668406,0.1501878044289974,0.1986061298382053,0.3233033312572045,-0.0063282760317559,-0.1313759939611646,0.1838988293365642,0.2089123974459581,0.1642506802240236,0.116087819435008,-0.0106060508122733,0.1600793983860926,0.1769017541832257,0.1249529645582566,0.1857908322332964,-0.1276971976453607,0.2513508080221444,0.1337222350285536,0.2154528183437962,0.2377881377292907,0.2190342475942941,0.0685470436526927,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5046666860580444,,0.4990000128746032,,,,,,0.5132222175598145,0.4984444677829742,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 4}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 4}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/train_head_on_ethics_DEBERTA.ckpt,,,,4.263319363633753e-06,,,,,microsoft/deberta-v2-xlarge,icy-cosmos-967,,,,0.0,,"{'adamw': {'lr': 6e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 4, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 4, 'check_val_every_n_epoch': 2}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.7565827369689941,0.0,,,,,,0.4990000128746032,0.4984444677829742
68,68,,,,,,,,,,,,,0.0755182430148124,,0.7818760871887207,,0.9810325503349304,,,,,,,,,,,,,,,2958.903335094452,115,1696357329.0932171,{'runtime': 2958},/workspace/brainbias/artifacts,,,0.3501200209119214,0.2977970876263915,0.2673998678371528,0.2885522882321249,0.3063372810631916,0.2578811000259896,0.1244200927822454,0.3928667448979578,0.1891313493360032,0.1913196740276152,-0.0013779425263671,0.1420833212042895,0.0839179795652459,0.0419770206269587,0.3153586808620735,0.2018942098199412,0.2390526720792549,0.0984137624392532,0.2047477930157834,0.254687344858374,0.3347865226059612,0.3608287449889741,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4621666967868805,,0.4877333045005798,,,,,,0.4653632342815399,0.4975177347660064,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,2.131659681816876e-05,,,,,bert-large-cased,absurd-snowflake-964,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.7217020988464355,0.0,,,,,,0.4877333045005798,0.4975177347660064
69,69,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4717.643322706223,141,1696354349.5769117,{'runtime': 4717},/workspace/brainbias/artifacts,,,0.3412662703284027,0.3385376042561437,0.3228784315162716,0.3544957074213822,0.275891749017739,0.2916397578933971,0.2554567286435677,0.3583354989152301,0.1196652409629754,0.1136912885649252,0.0777109355811416,0.1060798808670257,0.1995916876972822,0.1564377413774746,0.2050112562179964,0.0785461994595448,0.1827845580440899,0.1255463796332972,0.1912432818932633,0.2331066023000398,0.3065106766259628,0.3254495269586635,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.628201961517334,,0.5872511267662048,,,,,,0.8822034597396851,0.7927094101905823,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,2.131659681816876e-05,,,,,bert-large-cased,curious-night-963,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0002602000313345,0.0,,,,,,0.5872511267662048,0.7927094101905823
70,70,0.0979049950838089,,0.2960107624530792,,0.1409006863832473,,,,,,,,,,,,,,,,,,,,,,,,,,,2956.4278090000157,115,1696349612.602773,{'runtime': 2955},/workspace/brainbias/artifacts,,,0.3509641146722456,0.3253619602435479,0.2904919925138455,0.2882629400111552,0.1761465240106126,0.1684878783121999,0.1760353786430844,0.3031323549111273,0.2659673449159256,0.3254476219900254,0.183525745562304,0.1508132002266125,0.106135458719898,0.0833316921596052,0.2581888699416717,0.2871908425102081,0.2643015896927895,0.1464678934517278,0.2000775603156185,0.2273522128140054,0.2944002570578465,0.2486431767198624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6858237981796265,,0.5959523916244507,,,,,,0.8713372349739075,0.7773470878601074,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,2.131659681816876e-05,,,,,bert-large-cased,hopeful-frog-962,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.1093315407633781,0.0,,,,,,0.5959523916244507,0.7773470878601074
71,71,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4719.336962938309,141,1696346628.482982,{'runtime': 4718},/workspace/brainbias/artifacts,,,0.3220507952193136,0.352178004574483,0.2901739962598569,0.2699095807354985,0.2999266883139463,0.2790225563473503,0.2784340062106725,0.1747282221939337,0.1444247294974371,0.1585719523936925,0.175464556795264,-0.0015882501317977,-0.0394914728402273,0.0839389876625224,0.0886070207129472,0.1731712396790446,0.1820815713783398,0.1071755621415719,0.1709976069296683,0.2157167334551117,0.2818484034540034,0.2261653712577898,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6522274613380432,,0.5882715582847595,,,,,,0.875968873500824,0.7802029848098755,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,2.131659681816876e-05,,,,,bert-large-cased,cosmic-glade-959,,,,0.0,,"{'adamw': {'lr': 3e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 16, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.0053082234226167,0.0,,,,,,0.5882715582847595,0.7802029848098755
73,73,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,960.6575095653534,16,1696339364.4777205,{'runtime': 960},artifacts,,,0.1042449089470374,0.1785573799555066,,,,,,,,,0.1134939972578861,,,,,0.2672098204632186,0.2324855959114379,0.2795970066083814,0.2721134115953262,0.3058449064804556,0.2333402474260197,0.1802243640974078,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5685949325561523,,0.5444515347480774,,,,,,0.8106157183647156,0.6678712368011475,,,,,data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 20}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 20}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 20}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,2e-05,,,,,bert-base-cased,true-snowball-951,,,,0.0,,"{'adamw': {'lr': 2e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 20, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,0.5583480000495911,0.0,,,,,,0.5444515347480774,0.6678712368011475
86,86,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2704.7518393993378,133,1696070725.9860003,{'runtime': 2986},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5,,0.5,,,,,,0.5014163255691528,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 16}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 16}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 16}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,True,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,0.2352862870197483,,,,,bert-large-cased,breezy-silence-906,,,,0.0,,"{'adamw': {'lr': 0.3311311214825908, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 10, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,high,,0.3713749051094055,0.0,,,,,,0.5,0.5
87,87,,,,,,,,,,,,,0.0830912142992019,,0.8017702102661133,,1.0300626754760742,,,,,,,,,,,,,,,687.6243345737457,21,1696067982.4763737,{'runtime': 686},/workspace/brainbias/artifacts,,,0.1826117850897664,0.195808343220449,,,,,,,,,0.2488039006743335,,,,,0.2664140111515024,0.3651099563846942,0.3042149957634867,0.3404052088352348,0.09781555565857,0.0691553862155876,0.2950620442591432,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5223536491394043,,0.5042048692703247,,,,,,0.5177371501922607,0.5025123953819275,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 128}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,True,,,,,,0.0002089296130854,,,,,bert-base-cased,sunny-monkey-905,,,,0.0,,"{'adamw': {'lr': 0.0002089296130854041, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,high,,0.7510178089141846,0.0,,,,,,0.5042048692703247,0.5025123953819275
88,88,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1130.2561600208282,36,1696067276.350118,{'runtime': 1129},/workspace/brainbias/artifacts,,,0.3572854661840421,0.3622768454495736,,,,,,,,,0.3211743741772153,,,,,0.3488703938517769,0.3360901339728745,0.3404096823486418,0.3659394658523781,0.3705621819972194,0.3662707920212426,0.3595955338201437,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5003742575645447,,0.5,,,,,,0.4905762374401092,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 64}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,True,,,,,,0.0003019951720402,,,,,bert-base-cased,fearless-plasma-904,,,,0.0,,"{'adamw': {'lr': 0.0003019951720402019, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,high,,0.7014466524124146,0.0,,,,,,0.5,0.5
89,89,0.1834177672863006,,0.2782868146896362,,0.1254891455173492,,,,,,,,,,,,,,,,,,,,,,,,,,,694.0684797763824,21,1696066127.1673317,{'runtime': 693},/workspace/brainbias/artifacts,,,0.3621582967443248,0.362192722628785,,,,,,,,,0.0522734221760872,,,,,0.1504470531882354,0.2595446304968526,0.3254220493158258,0.3356383072686813,0.3301227831671431,0.3351233765317793,0.3563839272535652,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert-base-cased.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.498903751373291,,0.5,,,,,,0.4921068847179413,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 128}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,True,,,,,,0.0002089296130854,,,,,bert-base-cased,exalted-darkness-903,,,,0.0,,"{'adamw': {'lr': 0.0002089296130854041, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,high,,0.1251417696475982,0.0,,,,,,0.5,0.5
90,90,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1131.6209411621094,36,1696065413.054603,{'runtime': 1130},/workspace/brainbias/artifacts,,,0.361944695910961,0.3622329219432014,,,,,,,,,0.3217022839447047,,,,,0.2688051093185034,0.3396581290350706,0.3619172434995795,0.364354638832055,0.3607066694416313,0.3609384490240858,0.3610111344364585,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4939426183700561,,0.5,,,,,,0.5192644000053406,0.5,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 64}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 64}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 64}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,True,,True,,artifacts/train_head_on_ethics_bert-base-cased.ckpt,,,,0.0003019951720402,,,,,bert-base-cased,dutiful-wind-902,,,,0.0,,"{'adamw': {'lr': 0.0003019951720402019, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.3, 'stepLR_step_size': None, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'auto', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,high,,0.714978039264679,0.0,,,,,,0.5,0.5
92,92,,,,,,,,,,,,,0.1227285787463188,,0.7341504693031311,,0.8669653534889221,,,,,,,,,,,,,,,1440.0082774162292,63,1696011613.8472843,{'runtime': 1439},/workspace/brainbias/artifacts,,,0.3158610516099412,0.3040783985422474,0.3345630559483963,0.3117920459457952,0.3211071804554736,0.2646845667180433,0.2233512919903557,0.093412422021969,0.1644008752852461,0.2834976607538661,-0.0336972298242318,0.0575112865425629,0.1299287041129057,0.0864237226235982,0.2430352589202022,0.0036523395773746,-0.0464146826065787,-0.0509041318419753,0.1045519306859992,0.126924178125067,0.3077643618426925,0.298164966004429,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4079999923706054,,0.4925000071525574,,,,,,0.4518055617809296,0.4740000069141388,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 2}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 3}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 3}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,3.311311214825911e-05,,,,,bert-large-cased,firm-firefly-891,,,,0.0,,"{'adamw': {'lr': 3.311311214825911e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 5, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 30, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.7531280517578125,0.0,,,,,,0.4925000071525574,0.4740000069141388
93,93,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2394.5915253162384,87,1696010149.6494863,{'runtime': 2393},/workspace/brainbias/artifacts,,,0.3522547526752202,0.3443323238705982,0.3534948883940038,0.3344616657115687,0.2825803566955351,0.3165426504862477,0.248795021584482,0.1989038920304545,0.1423307739874637,-0.0614721786700483,0.0284892166081164,-0.0042459441673883,-0.0965083073930848,-0.0514335200681629,0.1795032814709592,0.0629678239579189,0.0547447334232133,0.0651166067827588,0.1550333879011282,0.2376745810935803,0.3102357165358096,0.2652637615521834,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.458333283662796,,0.4879167675971985,,,,,,0.5205556154251099,0.517916738986969,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,7.585775750291837e-08,,,,,bert-large-cased,different-valley-890,,,,0.0,,"{'adamw': {'lr': 7.585775750291837e-08, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 5, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 30, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.6755181550979614,0.0,,,,,,0.4879167675971985,0.517916738986969
94,94,0.1747180223464965,,0.276954710483551,,0.1244314983487129,,,,,,,,,,,,,,,,,,,,,,,,,,,1424.4357006549835,63,1696007715.2175117,{'runtime': 1423},/workspace/brainbias/artifacts,,,0.3513770175487146,0.3672385898477857,0.321231195363426,0.3137051040952623,0.3283668039624416,0.3664323286541217,0.3277139126967679,0.2964723002565587,0.2408655766860769,0.2511333774018587,-0.0003070261900794,0.342678021697284,0.2510618530472403,0.0280035003169628,0.2973283387322515,0.0978250304065545,0.0513712746837536,0.01940278447022,0.1230307044253389,0.1556199437029654,0.3018735080175085,0.3085076837764128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4020000100135803,,0.4794999957084656,,,,,,0.4369444251060486,0.4939444661140442,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 2}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 3}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 3}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,3.311311214825911e-05,,,,,bert-large-cased,fluent-planet-889,,,,0.0,,"{'adamw': {'lr': 3.311311214825911e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 5, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 30, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.129416823387146,0.0,,,,,,0.4794999957084656,0.4939444661140442
95,95,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2275.6702258586884,87,1696006256.4599595,{'runtime': 2275},/workspace/brainbias/artifacts,,,0.3519743289575086,0.3450816725597757,0.3533663781999606,0.3347988303374156,0.2827122016893982,0.317057012557654,0.2480367882013365,0.200745610728555,0.1468878421858802,-0.0602662334656949,0.0284135385966622,-0.010039500653164,-0.0945365893665546,-0.058649793617358,0.1552406733996675,0.0647199376581991,0.0561002910838413,0.0659074536067881,0.1543496919109038,0.2397963264852351,0.311099015000709,0.2657317384120917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4681251645088196,,0.4979166984558105,,,,,,0.5106944441795349,0.5182222127914429,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,7.585775750291837e-08,,,,,bert-large-cased,prime-dust-888,,,,0.0,,"{'adamw': {'lr': 7.585775750291837e-08, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 5, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 30, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.656735360622406,0.0,,,,,,0.4979166984558105,0.5182222127914429
96,96,0.1535074114799499,,0.2781847715377807,,0.1253318190574646,,,,,,,,,,,,,,,,,,,,,,,,,,,3765.083158016205,61,1696002390.879321,{'runtime': 3764},/workspace/brainbias/artifacts,,,0.3802136853905397,0.3637951907320965,0.3462523177539016,0.3443812255160895,0.2788421857918403,0.348541727724944,0.2855519752903995,0.2474739340241455,0.1547175550569824,0.1884982432301489,0.0291565024156899,0.2076620472672309,0.2617978745752624,0.0462716637288373,0.1410671942549934,0.0678715173819958,0.0552051456713174,0.08865944387059,0.2033223840437636,0.2580624745345838,0.3348602292896254,0.3328380046833936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert-large-cased.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5404167771339417,,0.5272916555404663,,,,,,0.5173822641372681,0.5298148393630981,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 10}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 5}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,3.0199517204020163e-06,,,,,bert-large-cased,iconic-lion-882,,,,0.0,,"{'adamw': {'lr': 3.0199517204020163e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 10, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.1096498519182205,0.0,,,,,,0.5272916555404663,0.5298148393630981
97,97,,,,,,,,,,,,,-0.0010731964139267,,0.7606505155563354,,0.9253008961677552,,,,,,,,,,,,,,,1384.9263854026794,47,1695998488.7736604,{'runtime': 1384},/workspace/brainbias/artifacts,,,0.1347092293020806,0.1548339515680707,,,,,,,,,0.1632720930763743,,,,,0.3066490575835245,0.1779412525535204,0.2832839792336974,0.2982680268620956,0.3318781509305676,0.2311514454380097,0.1311145345868732,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4994832277297973,,0.4944084584712982,,,,,,0.4704594612121582,0.5057781338691711,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 7}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 7}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,7.585775750291837e-08,,,,,bert-base-cased,exalted-universe-880,,,,0.0,,"{'adamw': {'lr': 7.585775750291837e-08, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 3, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.9907834529876708,0.0,,,,,,0.4944084584712982,0.5057781338691711
98,98,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1787.7064092159271,57,1695997079.0674062,{'runtime': 1787},/workspace/brainbias/artifacts,,,0.3455593438282973,0.3541369948407915,,,,,,,,,0.2565984991113409,,,,,0.3245418803699066,0.3236306711946995,0.3629231323489557,0.3631662922096049,0.3626866920571588,0.365442162252031,0.358748172601355,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.502358615398407,,0.5,,,,,,0.4512643218040466,0.5149999856948853,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 15}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 15}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,0.0363078054770101,,,,,bert-base-cased,winter-cosmos-879,,,,0.0,,"{'adamw': {'lr': 0.036307805477010104, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 3, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.6900049448013306,0.0,,,,,,0.5,0.5149999856948853
99,99,-0.0057416358031332,,0.3692806363105774,,0.2164458483457565,,,,,,,,,,,,,,,,,,,,,,,,,,,1265.169862985611,47,1695995271.244234,{'runtime': 1264},/workspace/brainbias/artifacts,,,0.2226377525449291,0.0748216094370995,,,,,,,,,0.1827431432145559,,,,,0.178304411293519,0.1375817683367249,0.2885089148791047,0.2610382495115196,0.3182458124716205,0.1315510841218395,0.0588904240929061,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert-base-cased.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.604866623878479,,0.5554250478744507,,,,,,0.7814788222312927,0.736579418182373,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 7}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 7}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 8}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 8}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,7.585775750291837e-08,,,,,bert-base-cased,fancy-pond-878,,,,0.0,,"{'adamw': {'lr': 7.585775750291837e-08, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 3, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.2226415723562241,0.0,,,,,,0.5554250478744507,0.736579418182373
100,100,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1625.167465686798,57,1695993977.9726996,{'runtime': 1624},/workspace/brainbias/artifacts,,,0.202870712685991,0.112684830813595,,,,,,,,,0.1817576225622072,,,,,0.1787336738482402,0.1381546997966806,0.2859636804605893,0.2610484191144472,0.3175165919071954,0.136018743116631,0.0601296827771859,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.6034502983093262,,0.5539613366127014,,,,,,0.770356297492981,0.7173922657966614,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 15}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 15}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1500]', 'batch_size': 15}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,artifacts/train_head_on_ethics_bert-base-cased.ckpt,,,,5.7543993733715664e-05,,,,,bert-base-cased,logical-hill-877,,,,0.0,,"{'adamw': {'lr': 5.7543993733715664e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 15, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 3, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.0036636777222156,0.0,,,,,,0.5539613366127014,0.7173922657966614
103,103,,,,,,,,,,,,,0.0601556450128555,,0.9034355878829956,,1.2954225540161133,,,,,,,,,,,,,,,293.5558943748474,30,1695976803.3028843,{'runtime': 465},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5474438071250916,,0.5151380300521851,,,,,,0.7380544543266296,0.6642106771469116,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,0.05,,,,,bert-base-cased,fearless-bush-856,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,1.298783302307129,0.0,,,,,,0.5151380300521851,0.6642106771469116
104,104,0.1592092812061309,,0.2743273675441742,,0.1224894598126411,,,,,,,,,,,,,,,,,,,,,,,,,,,2630.904547929764,52,1695973703.189195,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5840162038803101,,0.5525725483894348,,,,,,0.7428044676780701,0.6499056816101074,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 11}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 11}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 12}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 12}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,1.9054607179632464e-05,,,,,bert-large-cased,lemon-firebrand-851,,,,0.0,,"{'adamw': {'lr': 1.9054607179632464e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.1044418066740036,0.0,,,,,,0.5525725483894348,0.6499056816101074
106,106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3083.5333409309387,57,1695940734.9976878,{'runtime': 3630},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5004764199256897,,0.5,,,,,,0.500124454498291,0.4996379315853119,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 23}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 23}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,0.0039810717055349,,,,,bert-large-cased,serene-breeze-848,,,,0.0,,"{'adamw': {'lr': 0.003981071705534969, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.6848539113998413,0.0,,,,,,0.5,0.4996379315853119
108,108,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1337.416684627533,57,1695933523.6130016,,/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5692427158355713,,0.5298568606376648,,,,,,0.8016980886459351,0.6453668475151062,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 23}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 23}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,0.0251188643150958,,,,,bert-large-cased,ruby-wood-846,,,,0.0,,"{'adamw': {'lr': 0.025118864315095826, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.9489009380340576,0.0,,,,,,0.5298568606376648,0.6453668475151062
109,109,0.1839167475700378,,0.2717154026031494,,0.1203122287988662,,,,,,,,,,,,,,,,,,,,,,,,,,,3954.974606990814,93,1695931567.244118,{'runtime': 4533},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4773915410041809,,0.4997403919696808,,,,,,0.4869998097419739,0.5003620386123657,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 11}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 11}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 12}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 12}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,2.959854681166016e-06,,,,,bert-large-cased,dashing-bush-845,,,,0.0,,"{'adamw': {'lr': 3.0199517204020163e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.1197503358125686,0.0,,,,,,0.4997403919696808,0.5003620386123657
112,112,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1314.2497627735138,57,1695917901.7924058,{'runtime': 2982},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5655314922332764,,0.5423464775085449,,,,,,0.7988473773002625,0.6546140909194946,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 23}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 23}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,0.0208929613085404,,,,,bert-large-cased,comic-pond-842,,,,0.0,,"{'adamw': {'lr': 0.02089296130854041, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 1, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.9426332712173462,0.0,,,,,,0.5423464775085449,0.6546140909194946
113,113,,,,,,,,,,,,,0.0632298588752746,,0.8346121907234192,,1.109493374824524,,,,,,,,,,,,,,,148.3109631538391,5,1695916257.2009692,{'runtime': 399},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5384550094604492,,0.529077410697937,,,,,,0.697196364402771,0.612122654914856,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,False,,,,,,0.05,,,,,bert-base-cased,worthy-dew-840,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,1.0988130569458008,0.0,,,,,,0.529077410697937,0.612122654914856
115,115,0.183466762304306,,0.2716775238513946,,0.1203036308288574,,,,,,,,,,,,,,,,,,,,,,,,,,,1582.567322254181,27,1695915007.5696852,{'runtime': 1582},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4893579483032226,,0.5002595782279968,,,,,,0.5229164361953735,0.4996379315853119,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 11}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 11}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 23}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 12}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 12}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,0.0,False,,True,,,,,,1.9054607179632464e-05,,,,,bert-large-cased,brisk-night-832,,,,0.0,,"{'adamw': {'lr': 1.9054607179632464e-05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': 23, 'token_location': 0, 'lr_warm_up_steps': 0.75, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.1424389481544494,0.0,,,,,,0.5002595782279968,0.4996379315853119
118,118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,903.5377411842346,4,1695907911.376351,{'runtime': 903},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5653040409088135,,0.5338348746299744,,,,,,0.7707415223121643,0.6738196611404419,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 500}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 500}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 500}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,8.0,False,,False,,,,,,0.0069183097091893,,,,,bert-large-cased,zesty-butterfly-824,,,,0.0,,"{'adamw': {'lr': 0.006918309709189364, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 500, 'token_location': 0, 'lr_warm_up_steps': 0.8, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,high,,0.6976571083068848,97.0,,,,,,0.5338348746299744,0.6738196611404419
120,120,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1650.5821251869202,22,1695904842.9935572,{'runtime': 1650},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5709611773490906,,0.5400350093841553,,,,,,0.746946394443512,0.665782630443573,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 100}, 'train': {'shuffle': True, 'slicing': '[:2000]', 'batch_size': 100}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:3000]', 'batch_size': 100}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,30.0,False,,True,,,,,,0.0069183097091893,,,,,bert-large-cased,stoic-bird-816,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 100, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,0.3938957750797272,300.0,,,,,,0.5400350093841553,0.665782630443573
121,121,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,676.3929615020752,8,1695903140.9158094,{'runtime': 675},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5700711607933044,,0.5406560301780701,,,,,,0.7241909503936768,0.5758435130119324,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:2000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': True, 'slicing': '[:3000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,8.0,False,,True,,,,,,0.0301995172040201,,,,,bert-large-cased,fanciful-dream-815,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'batch_size_all': 50, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,0.7060462236404419,173.0,,,,,,0.5406560301780701,0.5758435130119324
122,122,,,,,,,,,,,,,,0.0504433736205101,,0.8687589168548584,,1.196888446807861,,,,,,,,,,,,,,627.6940457820892,25,1695900015.9641268,{'runtime': 627},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5167199969291687,,0.5205366015434265,,,,,0.6034426093101501,0.5473920702934265,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,24.0,False,,True,,,,,,0.0001737800828749,,,,,bert-base-cased,morning-sky-813,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'batch_size_all': None, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'strategy': 'ddp_find_unused_parameters_true', 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,,,,,,49.0,,,,,,0.5205366015434265,0.5473920702934265
127,127,,0.1310995817184448,,0.279828280210495,,0.1272051483392715,,,,,,,,,,,,,,,,,,,,,,,,,,31771.094570159912,1545,1695712748.4413905,{'runtime': 31772},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5764083862304688,,0.5119072198867798,,,,,0.6168159246444702,0.5186913013458252,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': True, 'slicing': '[:3000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,279.0,,True,,,artifacts/train_head_on_ethics_bert_large_end.ckpt,,,,9.120108393559096e-06,,,,,bert-large-cased,sparkling-cherry-787,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 300, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 50, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,0.1189542710781097,1674.0,,,,,,0.5119072198867798,0.5186913013458252
128,128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14880.22339630127,1283,1695680949.2685442,{'runtime': 14880},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5666467547416687,,0.5451452732086182,,,,,,0.6822502017021179,0.6563328504562378,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': True, 'slicing': '[:3000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,100.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,0.2754228703338169,,,,,bert-large-cased,dulcet-durian-786,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 50, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,0.0006769987521693,1500.0,,,,,,0.5451452732086182,0.6563328504562378
129,129,,0.181114912033081,,0.2719180881977081,,0.1204804927110672,,,,,,,,,,,,,,,,,,,,,,,,,,11291.633395910265,645,1695664746.915395,{'runtime': 11292},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5621246695518494,,0.5304581522941589,,,,,0.6690197587013245,0.5695752501487732,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': True, 'slicing': '[:3000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,100.0,,True,,,artifacts/train_head_on_ethics_bert_large_end.ckpt,,,,1.947360525085198e-06,,,,,bert-large-cased,elated-glitter-785,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,0.1187428832054138,2800.0,,,,,,0.5304581522941589,0.5695752501487732
130,130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5881.73924446106,651,1695653431.9753654,{'runtime': 5881},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5680338740348816,,0.527925431728363,,,,,,0.7191347479820251,0.5907252430915833,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': True, 'slicing': '[:3000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,40.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,0.0076750560680234,,,,,bert-large-cased,curious-night-784,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 40, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,0.5061703324317932,2920.0,,,,,,0.527925431728363,0.5907252430915833
131,131,,0.1658026874065399,,0.2736184597015381,,0.1219108328223228,,,,,,,,,,,,,,,,,,,,,,,,,,10514.687022686005,701,1695644760.0831127,{'runtime': 10515},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5824858546257019,,0.5400397181510925,,,,,0.8027081489562988,0.6545830965042114,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:100%]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,100.0,,True,,,,,,,5.2932430267575966e-08,,,,,bert-large-cased,lyric-monkey-782,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,0.0827648043632507,5600.0,,,,,,0.5400397181510925,0.6545830965042114
132,132,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4170.205071687698,523,1695634215.1426027,{'runtime': 4169},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.579323410987854,,0.529625415802002,,,,,,0.8337500095367432,0.6786456108093262,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,30.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,8.115635054534986e-05,,,,,bert-large-cased,usual-dream-781,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 30, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 5, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,0.0564930848777294,4350.0,,,,,,0.529625415802002,0.6786456108093262
133,133,,,,,,,,,,,,,,-0.0217792969197034,,1.1638058423995972,,2.147585153579712,,,,,,,,,,,,,,132.76133966445923,20,1695629915.0566278,{'runtime': 132},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5098560452461243,,0.49983611702919,,,,,0.5234035849571228,0.4979130327701568,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:0]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,4.0,,False,,,,,,,0.05,,,,,bert-base-cased,winter-bee-780,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,1.964114785194397,439.0,,,,,,0.49983611702919,0.4979130327701568
134,134,,,,,,,,,,,,,,0.0362970232963562,,1.092170476913452,,1.896653771400452,,,,,,,,,,,,,,144.43823981285095,15,1695626934.6623738,{'runtime': 143},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5349443554878235,,0.5168223977088928,,,,,0.7317860722541809,0.5325327515602112,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,3.0,,False,,,,,,,0.05,,,,,bert-base-cased,floral-shape-777,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,1.741474986076355,303.0,,,,,,0.5168223977088928,0.5325327515602112
135,135,,,,,,,,,,,,,,,,1.0751209259033203,,1.8467315435409544,,,,,,,,,,,,,,180.44248795509336,20,1695626584.307257,{'runtime': 169},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5379254221916199,,0.5046797394752502,,,,,0.7342519164085388,0.6491926312446594,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,4.0,,False,,,,,,,0.05,,,,,bert-base-cased,generous-gorge-776,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,1.8797569274902344,407.0,,,,,,0.5046797394752502,0.6491926312446594
136,136,,,,,,,,,,,,,,,,,,1.912004351615905,,,,,,,,,,,,,,199.76384830474856,27,1695623454.6324582,{'runtime': 199},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,0.5199999809265137,,,,,,0.5929999947547913,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,5.0,,False,,,,,,,0.05,,,,,bert-base-cased,dauntless-breeze-774,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,1.9747997522354128,576.0,,,,,,0.0,0.5929999947547913
137,137,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,154.69078946113586,19,1695623132.3601174,{'runtime': 153},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,3.0,,False,,,,,,,0.05,,,,,bert-base-cased,splendid-wind-773,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,1.6387945413589478,0.0,0.5419999957084656,,,,,,,1.9276965856552124,399.0,,,,0.6060000061988831,,0.0,0.6060000061988831
138,138,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,222.38172507286072,32,1695622923.4622371,{'runtime': 221},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,6.0,,False,,,,,,,0.05,,,,,bert-base-cased,fancy-dust-772,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,1.822555899620056,0.0,0.5339999794960022,,,,,,,1.780839920043945,672.0,,,,0.6800000071525574,,0.0,0.6800000071525574
140,140,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2843.2788376808167,325,1695460721.0735986,{'runtime': 2842},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4970000088214874,,,,,,,0.4704999923706054,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,20.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,1.799351772668518e-07,,,,,bert-large-cased,easy-sunset-763,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,0.6846752166748047,1460.0,,,,,,0.4970000088214874,0.4704999923706054
142,142,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2676.0028533935547,325,1695455018.1165605,{'runtime': 2675},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.4790000021457672,,,,,,,0.4194999933242798,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,20.0,,False,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,3.126915001754998e-07,,,,,bert-large-cased,balmy-silence-761,,,,0.0,,"{'adamw': {'lr': 1.20226443461741e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,0.7651711106300354,1460.0,,,,,,0.4790000021457672,0.4194999933242798
143,143,,,,,,,,,,,,,,,,,,1.780733585357666,,,,,,,,,,,,,,165.69549560546875,20,1695452113.0425267,{'runtime': 165},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5220000147819519,,,,,,0.6060000061988831,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,4.0,,False,,,,,,,0.05,,,,,bert-base-cased,solar-oath-760,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,,,,,1.9197285175323489,409.0,,,,,,0.5220000147819519,0.6060000061988831
147,147,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13471.061417102814,248,1695359959.047482,{'runtime': 13470},/Users/ajmeek/PycharmProjects/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/Users/ajmeek/PycharmProjects/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,False,,True,input,label,cross_entropy,commonsense,hendrycks/ethics,refs/pr/3,50.0,False,[:1000],50.0,True,[:1000],50.0,False,[:1000],,True,input,label,mse_loss,LFB-LAST,data/ds000212/ds000212_lfb,,,,2.0,False,,,,10.0,,False,,,,,,,0.05,,,,,bert-base-cased,comfy-firefly-747,,,,,,,"[0.9, 0.999]",1e-08,0.05,0.01,10000.0,,True,,,,,,,,,0.1,False,0.99,500.0,0.0,False,,1.0,1.0,False,1.0,1.0,1.0,,10.0,-1.0,,,,,0.0,32-true,1.0,,,,,,,[],,,,,,,,,0.531000018119812,False,,,5.9371538162231445,5950.0,,0.5960000157356262,,,,0.531000018119812,0.5960000157356262
149,149,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3041.7673218250275,325,1695313528.302616,{'runtime': 3041},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,20.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,0.0595821370425414,,,,,bert-large-cased,zesty-voice-745,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,0.5490000247955322,,,,,,1.5244423151016235,1460.0,,,,,0.6840000152587891,0.5490000247955322,0.6840000152587891
151,151,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,927.5932185649872,78,1695309019.4552195,{'runtime': 925},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,5.0,,True,,,artifacts/train_head_on_ethics_bert_large.ckpt,,,,2.601624838898515e-07,,,,,bert-large-cased,efficient-shadow-743,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 5, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,0.515999972820282,,,,,,0.5858181118965149,365.0,,,,,0.534500002861023,0.515999972820282,0.534500002861023
153,153,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,317.1926975250244,78,1695306645.5492623,{'runtime': 316},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,5.0,,True,,,artifacts/train_head_on_ethics.ckpt,,,,0.0065349861327074,,,,,bert-base-cased,lemon-planet-741,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 5, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,0.5289999842643738,,,,,,0.5807023048400879,365.0,,,,,0.6294999718666077,0.5289999842643738,0.6294999718666077
154,154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,319.08488965034485,78,1695306114.2577317,{'runtime': 318},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,5.0,,True,,,artifacts/train_head_on_ethics.ckpt,,,,0.0716546980661182,,,,,bert-base-cased,jolly-durian-739,,,,0.0,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 5, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 10, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,0.5320000052452087,,,,,,1.7917693853378296,365.0,,,,,0.5504999756813049,0.5320000052452087,0.5504999756813049
155,155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,118.01417231559752,12,1695304525.0364134,{'runtime': 117},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,2.0,,False,,,artifacts/checkpoint.ckpt,,,,0.05,,,,,bert-base-cased,breezy-moon-735,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,1.6752569675445557,,,,,,0.5199999809265137,,,,,1.7462605237960815,296.0,,,,,0.609000027179718,0.5199999809265137,0.609000027179718
157,157,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,79.23234558105469,23,1695303579.3034346,{'runtime': 78},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,1.0,,False,,,,,,,0.05,,,,,bert-base-cased,royal-voice-733,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,0.5299999713897705,,,False,,,5.07367467880249,574.0,,,,,0.6290000081062317,0.5299999713897705,0.6290000081062317
158,158,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,106.0059552192688,50,1695298921.7891562,{'runtime': 105},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,2.0,,False,,,,,,,0.05,,,,,bert-base-cased,polished-bush-732,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,0.515999972820282,,,False,,,7.864725112915039,1229.0,0.6349999904632568,,,,,0.515999972820282,0.6349999904632568
159,159,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,113.37168598175047,23,1695226221.102116,{'runtime': 112},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,1.0,,False,,,,,,,0.05,,,,,bert-base-cased,floral-capybara-720,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5230000019073486,True,,,6.108151912689209,595.0,,0.5360000133514404,,,,0.5230000019073486,0.5360000133514404
160,160,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12975.603532075882,410,1695146075.702577,{'runtime': 12975},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:6000]', 'batch_size': 6}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:2000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-SENTENCES', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:4000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,20.0,,False,,,,,,,0.0009135172474836,,,,,bert-large-cased,kind-voice-719,,,,,,"{'adamw': {'lr': 0.001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 100}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 20, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 100, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,0.515999972820282,False,,,0.8469237089157104,200.0,,0.5339999794960022,,,,0.515999972820282,0.5339999794960022
161,161,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,983.3234541416168,296,1695131975.8865912,{'runtime': 982},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,274.0,,True,,,,,,,0.0001445439770745,,,,,bert-base-cased,resilient-wind-716,,,,,,"{'adamw': {'lr': 0.0005, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,simple,,,,,,[],,,,,,,,,0.5189999938011169,False,,,0.0075565502047538,549.0,,0.5299999713897705,,,,0.5189999938011169,0.5299999713897705
162,162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16023.36880993843,585,1695128322.472525,{'runtime': 16023},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,100.0,,False,,,,,,,1.0764333400476943e-06,,,,,bert-large-cased,bumbling-waterfall-715,,,,,,"{'adamw': {'lr': 1.20226443461741e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 8, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5730000138282776,False,,,0.1220652163028717,3200.0,,0.7540000081062317,,,,0.5730000138282776,0.7540000081062317
163,163,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2231.163437604904,133,1695103131.6274097,{'runtime': 2230},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': False, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,0.1584893192461114,,,,,bert-large-cased,serene-rain-713,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 4, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.515999972820282,False,,,0.3919190168380738,1250.0,,0.5299999713897705,,,,0.515999972820282,0.5299999713897705
164,164,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4563.24374127388,146,1695100723.9822452,{'runtime': 4563},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,1.2022644346174132e-06,,,,,bert-large-cased,cosmic-cosmos-709,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 8, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5600000023841858,False,,,0.0893582627177238,800.0,,0.7350000143051147,,,,0.5600000023841858,0.7350000143051147
165,165,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4317.194122552872,163,1695096142.310567,{'runtime': 4316},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,0.0001202264434617,,,,,bert-large-cased,rose-silence-708,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 4, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.515999972820282,False,,,0.818365216255188,1575.0,,0.5299999713897705,,,,0.515999972820282,0.5299999713897705
166,166,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4264.367608785629,194,1695091804.8772118,{'runtime': 4264},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-AVG', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,2.2908676527677725e-05,,,,,bert-large-cased,youthful-deluge-707,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 2, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5649999976158142,False,,,0.1129550710320472,3125.0,,0.7360000014305115,,,,0.5649999976158142,0.7360000014305115
167,167,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4564.374855518341,146,1695087519.7458274,{'runtime': 4564},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-MIDDLE', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,3.0199517204020163e-06,,,,,bert-large-cased,worldly-capybara-706,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 8, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5699999928474426,False,,,0.7993297576904297,800.0,,0.7450000047683716,,,,0.5699999928474426,0.7450000047683716
168,168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4325.295979976654,163,1695082938.052607,{'runtime': 4325},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-MIDDLE', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,0.0001202264434617,,,,,bert-large-cased,expert-plasma-705,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 4, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.515999972820282,False,,,1.7454555034637451,1575.0,,0.5299999713897705,,,,0.515999972820282,0.5299999713897705
169,169,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4266.319978713989,194,1695078594.1973836,{'runtime': 4266},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-MIDDLE', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,3.311311214825911e-05,,,,,bert-large-cased,dazzling-plasma-704,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 2, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.515999972820282,False,,,1.6050455570220947,3125.0,,0.5299999713897705,,,,0.515999972820282,0.5299999713897705
170,170,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4579.343895196915,146,1695074303.6230133,{'runtime': 4578},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,1.2022644346174132e-06,,,,,bert-large-cased,brisk-voice-703,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 8, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,['evidence'],,,,,,,,,0.5759999752044678,False,,,0.7256357073783875,800.0,,0.7400000095367432,,,,0.5759999752044678,0.7400000095367432
171,171,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4325.839015722275,163,1695069702.9339907,{'runtime': 4325},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,0.0001737800828749,,,,,bert-large-cased,breezy-sunset-702,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 4, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.515999972820282,False,,,1.4152942895889282,1575.0,,0.5299999713897705,,,,0.515999972820282,0.5299999713897705
172,172,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4295.264527082443,194,1695065355.8873382,{'runtime': 4294},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 4}, 'enable': True, 'loss_fn': 'mse_loss', 'revision': None, 'input_col': 'input', 'label_col': 'label', 'validation': None}",,,,,,,,,,,,,,,25.0,,True,,,,,,,2.2908676527677725e-05,,,,,bert-large-cased,unique-aardvark-701,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 25, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 2, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5559999942779541,False,,,0.9122304916381836,3125.0,,0.7120000123977661,,,,0.5559999942779541,0.7120000123977661
176,176,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4329.159880876541,127,1694870863.72655,{'runtime': 4329},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,22.0,,True,,,,,,,2.089296130854039e-06,,,,,bert-large-cased,blooming-moon-689,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 7500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 7, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,"['Higher val_acc', 'evidence']",,,,,,,,,0.5709999799728394,False,,,0.8467081189155579,776.0,,0.7459999918937683,,,,0.5709999799728394,0.7459999918937683
178,178,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4626.079259395599,181,1694861872.3068354,{'runtime': 4627},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[-500:]', 'batch_size': 1}, 'train': {'shuffle': True, 'slicing': '[-1000:]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[-500:]', 'batch_size': 1}}",,,,,,,,,,,,,,,,,"{'name': 'LFB-LAST', 'path': 'data/ds000212/ds000212_lfb', 'test': None, 'train': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,10.0,,True,,,,,,,1.0964781961431852e-05,,,,,microsoft/deberta-v2-xlarge,breezy-eon-683,,,,,,"{'adamw': {'lr': 0.0002, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 15, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,"['Higher val_acc', 'evidence']",,,,,,,,,0.484375,False,,,0.8154250383377075,600.0,,0.6997318863868713,,,,0.484375,0.6997318863868713
180,180,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6599.947950363159,408,1694797106.1375422,{'runtime': 6600},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': False, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,83.0,,True,,,,,,,0.0301995172040201,,,,,bert-large-cased,deft-terrain-668,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 7500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 7, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5199999809265137,False,,,0.2414695024490356,2435.0,,0.4659999907016754,,,,0.5199999809265137,0.4659999907016754
182,182,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13768.85400533676,494,1694774810.4557483,{'runtime': 13769},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[-500:]', 'batch_size': 1}, 'train': {'shuffle': True, 'slicing': '[-1000:]', 'batch_size': 1}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[-500:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,10.0,,True,,,,,,,0.0015848931924611,,,,,microsoft/deberta-v2-xlarge,radiant-leaf-665,,,,,,"{'adamw': {'lr': 0.0002, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 10, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 15, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.453125,False,,,0.8152824640274048,1550.0,,0.5335120558738708,,,,0.453125,0.5335120558738708
183,183,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33561.041081905365,1095,1694790483.013324,{'runtime': 33560},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,100.0,,True,,,,,,,7.585775750291836e-06,,,,,bert-large-cased,pretty-dragon-661,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 100, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 7500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'accumulate_grad_batches': 7, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,"['Higher val_acc', 'evidence']",,,,,,,,,0.550000011920929,False,,,0.8401982188224792,6700.0,,0.7519999742507935,,,,0.550000011920929,0.7519999742507935
187,187,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3476.682554244995,353,1694734937.6528363,{'runtime': 3476},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:400]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:400]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'enable': False, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,100.0,,True,,,,,,,0.0010349590933006,,,,,bert-large-cased,comfy-pine-656,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5199999809265137,False,,,0.5493258237838745,8000.0,,0.5325000286102295,,,,0.5199999809265137,0.5325000286102295
191,191,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29650.626119852062,1889,1694731437.9504218,{'runtime': 29650},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,False,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:400]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:400]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,100.0,,True,,,,,,,4.1121641186780494e-25,,,,,bert-large-cased,serene-armadillo-647,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 100, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,"['Higher val_acc', 'evidence']",,,,,,,,,0.5099999904632568,False,,,0.8650360107421875,46400.0,,0.6800000071525574,,,,0.5099999904632568,0.6800000071525574
193,193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,393.3537392616272,26,1694700038.389501,{'runtime': 393},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,True,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,2.0,,,,,,,,,0.0005,,,,,bert-base-cased,stoic-haze-641,,,,,,"{'adamw': {'lr': 0.0005, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 0, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.515999972820282,False,,,0.9388046264648438,625.0,,0.5299999713897705,,,,0.515999972820282,0.5299999713897705
194,194,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,528.6731519699097,37,1694699548.392306,{'runtime': 528},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,True,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,3.0,,,,,,,,,0.0005,,,,,bert-base-cased,glamorous-disco-640,,,,,,"{'adamw': {'lr': 0.0005, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 0, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.515999972820282,False,,,0.4430127143859863,890.0,,0.5299999713897705,,,,0.515999972820282,0.5299999713897705
195,195,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,610.4560539722443,96,1694696301.306927,{'runtime': 610},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,True,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,90.0,,,,,,,,0.0005000000237487,0.0005,,,,,bert-base-cased,colorful-sponge-638,,,,,,"{'adamw': {'lr': 0.0005, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 0, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.515999972820282,False,,,1.4157085418701172,181.0,,0.5099999904632568,,,,0.515999972820282,0.5099999904632568
196,196,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,630.4593250751495,62,1694695440.963737,{'runtime': 630},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,True,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,58.0,,,,,,,,,0.0005,,,,,bert-base-cased,flowing-shape-635,,,,,,"{'adamw': {'lr': 0.0005, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 0, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.515999972820282,False,,,1.4192135334014893,116.0,,0.5099999904632568,,,,0.515999972820282,0.5099999904632568
198,198,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,450.29399609565735,40,1694694713.2914531,{'runtime': 449},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,38.0,,,,,,,,,0.0005,,,,,bert-base-cased,rosy-silence-632,,,,,,"{'adamw': {'lr': 0.0005, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 0, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.515999972820282,False,,,1.4307936429977417,76.0,,0.5099999904632568,,,,0.515999972820282,0.5099999904632568
199,199,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,475.38592004776,71,1694694092.432484,{'runtime': 475},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:1000]', 'batch_size': 10}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:1000]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,67.0,,,,,,,,,0.05,,,,,bert-base-cased,lucky-forest-631,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': -1, 'min_epochs': None, 'overfit_batches': 2, 'limit_val_batches': 0, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': 2, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.515999972820282,False,,,1.6207704544067385,134.0,,0.5099999904632568,,,,0.515999972820282,0.5099999904632568
201,201,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7609.680545568466,445,1694689838.737609,{'runtime': 7611},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:500]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,23.0,,,,,,,,,0.0092651009442592,,,,,bert-large-cased,quiet-salad-626,,,,,,"{'adamw': {'lr': 0.05, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.9, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 2500}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 75, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5199999809265137,False,,,0.8246591687202454,10959.0,,0.5339999794960022,,,,0.5199999809265137,0.5339999794960022
202,202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5999.709806442261,348,1694682074.1177003,{'runtime': 6001},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:500]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 10}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,18.0,,,,,,,,,0.0009320653479069,,,,,bert-large-cased,solar-paper-624,,,,,,"{'adamw': {'lr': 0.001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 500, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 75, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5199999809265137,False,,,1.0207709074020386,8597.0,,0.5339999794960022,,,,0.5199999809265137,0.5339999794960022
204,204,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,47400.09772133827,2811,1694673132.1682844,{'runtime': 47401},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:500]', 'batch_size': 5}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:500]', 'batch_size': 5}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 5}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,148.0,,,,,,,,,1.1163135664444592e-32,,,,,bert-large-cased,resilient-snowflake-620,,,,,,"{'adamw': {'lr': 0.0001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'stepLR_gamma': 0.99, 'token_location': 0, 'stepLR_step_size': 10, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 5000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': True, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5199999809265137,False,,,0.8919395804405212,69083.0,,0.5339999794960022,,,,0.5199999809265137,0.5339999794960022
215,215,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,588.2305908203125,1,1694590875.2967918,{'runtime': 586},/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,,True,input,label,cross_entropy,commonsense,hendrycks/ethics,refs/pr/3,50.0,False,[:1000],50.0,True,[:1000],50.0,False,[:1000],,True,input,label,mse_loss,learning_from_brains,data/ds000212,,LAST,,2.0,False,,,,1.0,,,,,,,,,,,,,,bert-base-cased,different-dust-591,,,,,,,"[0.9, 0.999]",1e-08,0.001,0.01,10000.0,,True,,,,,,,,,0.1,False,,,0.0,False,,,1.0,False,1.0,1.0,1.0,,1.0,-1.0,,,,,,32-true,1.0,,,,,,,[],,,,,,,,,0.5180000066757202,False,,,,2.0,,0.5,,,,0.5180000066757202,0.5
217,217,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,78.70291304588318,24,1694588658.584622,{'runtime': 77},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}, 'train': {'shuffle': True, 'slicing': '[:100]', 'batch_size': 50}, 'enable': True, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:100]', 'batch_size': 50}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 2}, 'enable': True, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,bert-base-cased,efficient-frost-586,,,,,,"{'adamw': {'lr': 0.001, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': False, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_learning_rate_decay': True, 'before_lr_decay_warm_up_steps': 10000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '32-true', 'max_epochs': 1, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 1, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 1}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5199999809265137,False,,,0.8132522106170654,1160.0,,0.5099999904632568,,,,0.5199999809265137,0.5099999904632568
227,227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1483.1271092891693,30,1694189916.885991,{'runtime': 1481},/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/home/art/mydir/dev/aisc_2023/Inducing-human-like-biases-in-moral-reasoning-LLMs/data,,,True,input,label,cross_entropy,commonsense,hendrycks/ethics,refs/pr/3,50.0,False,[:100],50.0,True,[:100],50.0,False,[:100],,True,input,label,mse_loss,learning_from_brains,data/ds000212,,LAST,,2.0,False,,,,30.0,,,,,,,,,,,,,,bert-base-cased,sparkling-galaxy-536,,,,,,,"[0.9, 0.999]",1e-08,0.001,0.01,,True,,10.0,epoch,0.0,1e-08,0.1,0.0,10.0,True,0.1,False,,,0.0,False,,,1.0,False,1.0,15.0,1.0,,-1.0,-1.0,,,,,1.0,32-true,1.0,,,,,,,[],,,,,,,,,0.5099999904632568,False,,,,30.0,,0.5600000023841858,,,,0.5099999904632568,0.5600000023841858
232,232,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,825.7204818725586,9,1694020707.3379838,{'runtime': 829},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:40%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[50%:]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[60%:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,0.8,15.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,rosy-deluge-521,,,,,,"{'adamw': {'lr': 0.0006538379548447884, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False, 'has_ReduceLROnPlateau': True, 'reduceLROnPlateau_config': {'eps': 1e-08, 'factor': 0.1, 'min_lr': 1e-10, 'verbose': True, 'cooldown': 100, 'patience': 10}, 'lr_scheduler_steps_frequency': 3000}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'precision': '16-mixed', 'max_epochs': 3000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.4842370748519897,False,,,3.016477108001709,225.0,,0.5004270076751709,,,,0.4842370748519897,0.5004270076751709
233,233,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17693.451994419098,162,1694019094.1085374,{'runtime': 17693},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:40%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[50%:]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[60%:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,,257.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,feasible-fog-514,,,,,,"{'adamw': {'lr': 1e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 3000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.7080706357955933,False,,,0.7475665211677551,3869.0,,0.5286080241203308,,,,0.7080706357955933,0.5286080241203308
234,234,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,528.9845213890076,4,1693999750.2290585,{'runtime': 531},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:40%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[50%:]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[60%:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,0.8,12.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,worthy-sea-513,,,,,,"{'adamw': {'lr': 1e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 3000, 'min_epochs': None, 'overfit_batches': 1, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.4810844957828522,False,,,,12.0,,0.5,,,,0.4810844957828522,0.5
235,235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1285.1543684005735,9,1693998298.2271843,{'runtime': 1288},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:40%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[50%:]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[60%:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,0.8,15.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,swift-bee-511,,,,,,"{'adamw': {'lr': 1e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 3000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5353089570999146,False,,,2.2010416984558105,225.0,,0.5004270076751709,,,,0.5353089570999146,0.5004270076751709
236,236,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1077.2237186431885,7,1693993960.9093397,{'runtime': 1080},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:40%]', 'batch_size': 2}, 'train': {'shuffle': True, 'slicing': '[50%:]', 'batch_size': 1}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'input_col': 'input', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[60%:]', 'batch_size': 2}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 1}, 'loss_fn': 'mse_loss', 'input_col': 'input', 'label_col': 'label', 'validation': None, 'sampling_method': 'SENTENCES'}",,,,,,,,,,,,,,0.8,12.0,,,,,,,,,,,,,,microsoft/deberta-v2-xlarge,crisp-shape-510,,,,,,"{'adamw': {'lr': 3e-06, 'eps': 1e-08, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 1000, 'min_epochs': None, 'overfit_batches': 0, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 3}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.5157629251480103,False,,,1.6423921585083008,180.0,,0.4995730221271515,,,,0.5157629251480103,0.4995730221271515
241,241,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,489.81043100357056,12,1693912386.875182,{'runtime': 489},/workspace/brainbias/artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,/workspace/brainbias/data,,"{'name': 'commonsense', 'path': 'hendrycks/ethics', 'test': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}, 'train': {'shuffle': True, 'slicing': '[:50%]', 'batch_size': 10}, 'loss_fn': 'cross_entropy', 'revision': 'refs/pr/3', 'label_col': 'label', 'validation': {'shuffle': False, 'slicing': '[:50%]', 'batch_size': 10}}",,,,,,,,,,,,,,,,,"{'name': 'learning_from_brains', 'path': 'data/ds000212', 'test': None, 'train': {'shuffle': False, 'slicing': None, 'batch_size': 10}, 'loss_fn': 'mse_loss', 'label_col': 'label', 'validation': None, 'sampling_method': 'LAST'}",,,,,,,,,,,,,,,26.0,,,,,,,,,,,,,,bert-base-cased,frosty-wildflower-491,,,,,,"{'adamw': {}, 'train_all': True, 'token_location': 0, 'regularization_coef': 0.1, 'regularize_from_init': False}",,,,,,,,,,,,,,,,,,,,,,"{'max_time': None, 'max_steps': -1, 'min_steps': None, 'max_epochs': 100, 'min_epochs': None, 'limit_val_batches': 1, 'log_every_n_steps': None, 'limit_test_batches': 1, 'val_check_interval': 1, 'limit_train_batches': 15, 'enable_checkpointing': False, 'num_sanity_val_steps': None, 'check_val_every_n_epoch': 5}",,,,,,,,,,,,,,,,,,,,,,,[],,,,,,,,,0.4884937107563019,False,,,1.5648138523101809,390.0,,0.5339855551719666,,,,0.4884937107563019,0.5339855551719666
