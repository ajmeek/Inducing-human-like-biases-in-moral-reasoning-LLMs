import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils import *
import numpy as np
import argparse
from itertools import product
from torch.utils.data import DataLoader

def main(args):
    test_hard_accs = []
    test_accs = []
    test_hard_ems = []
    test_ems = []
    with open("runs.txt", "a") as f:
        f.write('{}\n'.format(args))
    data_dir = os.path.abspath(".")
    for run in range(args.nruns):
        model, optimizer = load_model(args)

        # data for normal training + evaluation
        train_data = load_process_data(args, data_dir, "deontology", "train")
        test_hard_data = load_process_data(args, data_dir, "deontology", "test_hard")
        test_data = load_process_data(args, data_dir, "deontology", "test")
        print(len(train_data), len(test_hard_data), len(test_data))

        train_dataloader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)
        test_hard_dataloader = DataLoader(test_hard_data, batch_size=args.batch_size, shuffle=False)
        test_dataloader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False)

        for epoch in range(1, args.nepochs + 1):
            print()
            train(model, optimizer, train_dataloader, epoch)
            print('\ntest hard set:')
            test_hard_acc, test_hard_em  = evaluate(model, test_hard_dataloader)
            print('\ntest set:')
            test_acc, test_em = evaluate(model, test_dataloader)

        test_hard_accs.append(test_hard_acc)
        test_accs.append(test_acc)
        test_hard_ems.append(test_hard_em)
        test_ems.append(test_em)

        if args.save:
            save_path = "deontology_{}_{}_{}_{}.pkl".format(args.model, args.learning_rate, args.batch_size, args.nepochs)
            print("SAVING to", save_path)
            torch.save(model.module.state_dict(), save_path)

        with open("runs.txt", "a") as f:
            f.write("Run {} test hard acc: {:.3f} test acc: {:.3f} test hard em: {:3f} test em: {:.3f}\n".format(run, test_hard_acc, test_acc, test_hard_em, test_em))

    return np.max(test_hard_accs), np.max(test_accs), np.max(test_hard_ems), np.max(test_ems)

def train(model, optimizer, train_dataloader, epoch, log_interval=10):
    # Set model to training mode
    model.train()
    criterion = torch.nn.BCEWithLogitsLoss()

    # Loop over each batch from the training set
    for step, batch in enumerate(train_dataloader):

        # Copy data to GPU if needed
        batch = tuple(t.cuda() for t in batch)

        # Unpack the inputs from our dataloader
        b_input_ids, b_input_mask, b_labels = batch

        # Zero gradient buffers
        optimizer.zero_grad()

        # Forward pass
        output = model(b_input_ids, attention_mask=b_input_mask)[0].squeeze()

        loss = criterion(output, b_labels.float())

        # Backward pass
        loss.backward()

        # Update weights
        optimizer.step()

        if step % log_interval == 0 and step > 0 and args.verbose:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, step * len(b_input_ids),
                len(train_dataloader.dataset),
                       100. * step / len(train_dataloader), loss))


def evaluate(model, dataloader):
    model.eval()
    cors = []

    for batch in dataloader:
        batch = tuple(t.cuda() for t in batch)
        b_input_ids, b_input_mask, b_labels = batch

        with torch.no_grad():
            logits = model(b_input_ids, attention_mask=b_input_mask)[0]
        output = logits.squeeze().detach().cpu().numpy()
        predictions = (output > 0).astype(int)

        b_labels = b_labels.detach().cpu().numpy()
        cors += list(predictions == b_labels)

    acc = np.mean(cors)
    em_sums = [int(cors[4*i]) + int(cors[4*i+1]) + int(cors[4*i+2]) + int(cors[4*i+3]) for i in range(len(cors) // 4)]
    em_cors = [em_sums[i] == 4 for i in range(len(em_sums))]
    em = np.mean(em_cors)
    print('Accuracy: {:.4f}, Exact match: {:.4f}'.format(acc, em))
    return acc, em

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", "-m", type=str, default="bert-base-uncased")
    parser.add_argument("--ngpus", "-n", type=int, default=2)
    parser.add_argument("--nepochs", "-e", type=int, default=2)
    parser.add_argument("--batch_size", "-b", type=int, default=16)
    parser.add_argument("--max_length", "-t", type=int, default=64)
    parser.add_argument("--weight_decay", "-w", type=float, default=0.01)
    parser.add_argument("--learning_rate", "-l", type=float, default=2e-5)
    parser.add_argument("--verbose", "-v", action="store_true")
    parser.add_argument("--nruns", "-r", type=int, default=1)
    parser.add_argument("--grid_search", "-g", action="store_true")
    parser.add_argument("--save", "-s", action="store_true")
    args = parser.parse_args()

    if args.grid_search:
        file = "grid_search_results.txt"
        args.nruns = 1
        models = ["bert-base-uncased", "bert-large-uncased", "roberta-large", "albert-xxlarge-v2"]
        lrs = [1e-5, 3e-5]
        batch_sizes = [8, 16]
        epochs = [2,4]

        with open(file, "a") as f:
            f.write("{}\n".format(args))
            f.write("models: {}, lrs: {}, batch_sizes: {}, epochs: {}\n".format(models, lrs, batch_sizes, epochs))

        for model, lr, bs, nepoch in product(models, lrs, batch_sizes, epochs):
            args.model = model
            args.learning_rate = lr
            args.batch_size = bs
            args.nepochs = nepoch

            test_hard_acc, test_acc, test_hard_em, test_em = main(args)

            with open(file, "a") as f:
                f.write("model: {}, lr: {}, batch_size: {}, nepoch: {}.\n test hard accuracy: {:.3f}, test accuracy: {:.3f}, test hard em: {:.3f}, test em: {:.3f}\n".format(model, lr, bs, nepoch, test_hard_acc, test_acc, test_hard_em, test_em))
    else:
        main(args)


