{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds   \n",
    "from typing import Dict, Tuple, Generator\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _pad_seq_right_to_n(\n",
    "    seq: np.ndarray,\n",
    "    n: int,\n",
    "    pad_value: float = 0.\n",
    "    ) -> np.ndarray:\n",
    "    if n == seq.shape[0]:\n",
    "        return seq\n",
    "    return np.concatenate(\n",
    "        [\n",
    "            seq,\n",
    "            np.ones(\n",
    "                (\n",
    "                    n-seq.shape[0],\n",
    "                    *seq.shape[1:]\n",
    "                )\n",
    "            ) * pad_value,  \n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataloader,\n",
    "        length,\n",
    "        sample_keys\n",
    "        ) -> None:\n",
    "        self.name = 'BaseDataset'\n",
    "        self._length = length\n",
    "        self.dataloader = iter(dataloader)\n",
    "        self.sample_keys = sample_keys\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = next(self.dataloader)\n",
    "            \n",
    "        if self.sample_keys is not None:\n",
    "            sample = {\n",
    "                key: sample[key] \n",
    "                for key in self.sample_keys\n",
    "                if key in sample\n",
    "            }\n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "class BaseBatcher:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_random_seq: bool = True,\n",
    "        seq_min: int = 10,\n",
    "        seq_max: int = 50,\n",
    "        sample_keys: Tuple[str] = None,\n",
    "        decoding_target: str = None,\n",
    "        seed: int =  None,\n",
    "        bold_dummy_mode: bool = False,\n",
    "        **kwargs\n",
    "        ) -> None:\n",
    "        assert seq_min > 0, \"seq_min must be greater than 0\"\n",
    "        assert seq_min < seq_max, 'seq_min must be less than seq_max'\n",
    "        self.sample_random_seq = sample_random_seq\n",
    "        self.seq_min = seq_min\n",
    "        self.seq_max = seq_max\n",
    "        self.decoding_target = decoding_target\n",
    "        self.sample_keys = sample_keys\n",
    "        self.bold_dummy_mode = bold_dummy_mode\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "    def _make_dataloader(\n",
    "        self,\n",
    "        files,\n",
    "        repeat: bool = True,\n",
    "        n_shuffle_shards: int = 1000,\n",
    "        n_shuffle_samples: int = 1000,\n",
    "        batch_size: int = 1,\n",
    "        num_workers: int = 0\n",
    "        ) -> Generator[Dict[str, torch.tensor], None, None]:\n",
    "        dataset = wds.WebDataset(files)\n",
    "\n",
    "        if n_shuffle_shards is not None:\n",
    "            dataset = dataset.shuffle(n_shuffle_shards)\n",
    "\n",
    "        dataset = dataset.decode(\"pil\").map(self.preprocess_sample)\n",
    "\n",
    "        if repeat:\n",
    "            dataset = dataset.repeat()\n",
    "        \n",
    "        if n_shuffle_samples is not None:\n",
    "            dataset = dataset.shuffle(n_shuffle_samples)\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    def dataset(\n",
    "        self,\n",
    "        tarfiles: list,\n",
    "        repeat: bool=True,\n",
    "        length: int = 400000,\n",
    "        n_shuffle_shards: int = 1000,\n",
    "        n_shuffle_samples: int = 1000,\n",
    "        num_workers: int = 0\n",
    "        ) -> torch.utils.data.Dataset:\n",
    "        \"\"\"Create Pytorch dataset that can be used for training.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            tarfiles: list\n",
    "                List of paths to data files (ie., fMRI runs) used for training.\n",
    "            repeat: bool\n",
    "                If True, repeat the dataset indefinitely.\n",
    "            length: int\n",
    "                Maximum number of samples to yield from the dataset.\n",
    "            n_shuffle_shards: int\n",
    "                Buffer for shuffling of tarfiles during training.\n",
    "            n_shuffle_samples: int\n",
    "                Buffer for shuffling of samples during training.\n",
    "            num_workers: int\n",
    "                Number of workers to use for data loading.\n",
    "\n",
    "        Returns:\n",
    "        -----\n",
    "            torch.utils.data.Dataset: Pytorch dataset.\n",
    "        \"\"\"\n",
    "        dataloader = self._make_dataloader(\n",
    "            files=tarfiles,\n",
    "            repeat=repeat,\n",
    "            n_shuffle_shards=n_shuffle_shards,\n",
    "            n_shuffle_samples=n_shuffle_samples,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "        return BaseDataset(\n",
    "            dataloader=dataloader,\n",
    "            length=length,\n",
    "            sample_keys=self.sample_keys\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def _pad_seq_right_to_n(\n",
    "        seq: np.ndarray,\n",
    "        n: int,\n",
    "        pad_value: float = 0\n",
    "        ) -> np.ndarray:\n",
    "        return _pad_seq_right_to_n(\n",
    "            seq=seq,\n",
    "            n=n,\n",
    "            pad_value=pad_value\n",
    "        )\n",
    "    \n",
    "    def _sample_seq_on_and_len(\n",
    "        self,\n",
    "        bold_len: int\n",
    "        ) -> Tuple[int, int]:\n",
    "        \n",
    "        seq_on = 0\n",
    "        if self.sample_random_seq and self.seq_min < bold_len:            \n",
    "            seq_min = min(int(self.seq_min), bold_len)\n",
    "            seq_max = min(int(self.seq_max), bold_len)\n",
    "\n",
    "            if seq_min < seq_max:\n",
    "                seq_len = np.random.randint(\n",
    "                    low=seq_min,\n",
    "                    high=seq_max,\n",
    "                    size=1\n",
    "                )[0]\n",
    "            \n",
    "            else:\n",
    "                seq_len = seq_max\n",
    "            \n",
    "            if seq_len < bold_len:\n",
    "                seq_on = np.random.choice(\n",
    "                    np.arange(\n",
    "                        0,\n",
    "                        bold_len-seq_len,\n",
    "                        seq_len\n",
    "                    ),\n",
    "                    size=1\n",
    "                )[0]\n",
    "        \n",
    "        elif not self.sample_random_seq and self.seq_max < bold_len:\n",
    "            seq_len = self.seq_max\n",
    "\n",
    "        else:\n",
    "            seq_len = bold_len\n",
    "\n",
    "        return seq_on, seq_len\n",
    "\n",
    "    def make_bold_dummy(\n",
    "        self,\n",
    "        bold_shape: Tuple[int, int],\n",
    "        t_r: float, # in seconds\n",
    "        f_s: Tuple[float]=None, # sine frequencies in seconds\n",
    "        ) -> np.ndarray:\n",
    "        f_s = np.array([4, 8, 12]) if f_s is None else np.array(f_s).flatten()\n",
    "        np.random.shuffle(f_s)\n",
    "        f = np.zeros((1, bold_shape[-1]))\n",
    "        for i, f_i in enumerate(f_s):\n",
    "            f[:,i::len(f_s)] = f_i\n",
    "        t_offsets = np.random.choice(\n",
    "            a=np.arange(0,3), # in TRs\n",
    "            size=bold_shape[1],\n",
    "            replace=True\n",
    "        )\n",
    "        t = np.concatenate(\n",
    "            [\n",
    "                np.arange(\n",
    "                    t_offsets[i],\n",
    "                    bold_shape[0]+t_offsets[i]\n",
    "                ).reshape(-1,1)\n",
    "                for i in range(bold_shape[1])\n",
    "            ],\n",
    "            axis=-1\n",
    "        ) * t_r\n",
    "        return np.sin((1. / f) * t)\n",
    "\n",
    "    def preprocess_sample(\n",
    "        self,\n",
    "        sample\n",
    "        ) -> Dict[str, torch.Tensor]:\n",
    "        out = dict(__key__=sample[\"__key__\"])\n",
    "        t_r = sample[\"t_r.pyd\"]\n",
    "\n",
    "        label = None\n",
    "        f_s = None\n",
    "        if self.bold_dummy_mode and self.decoding_target is not None:\n",
    "            label = np.random.choice([0, 1])\n",
    "            f_s = np.array([1, 2, 4]) if label == 0 else np.array([6, 8, 10])                \n",
    "\n",
    "        for key, value in sample.items():\n",
    "            if key == \"bold.pyd\":\n",
    "\n",
    "                bold = np.array(value).astype(np.float)\n",
    "                \n",
    "                if self.bold_dummy_mode:\n",
    "                    bold = self.make_bold_dummy(\n",
    "                        bold_shape=bold.shape,\n",
    "                        t_r=t_r,\n",
    "                        f_s=f_s\n",
    "                    )\n",
    "\n",
    "                seq_on, seq_len = self._sample_seq_on_and_len(bold_len=len(bold))\n",
    "                bold = bold[seq_on:seq_on+seq_len]\n",
    "                t_rs = np.arange(seq_len) * t_r\n",
    "                attention_mask = np.ones(seq_len)\n",
    "                bold = self._pad_seq_right_to_n(\n",
    "                    seq=bold,\n",
    "                    n=self.seq_max,\n",
    "                    pad_value=0\n",
    "                )\n",
    "                t_rs = self._pad_seq_right_to_n(\n",
    "                    seq=t_rs,\n",
    "                    n=self.seq_max,\n",
    "                    pad_value=0\n",
    "                )\n",
    "                attention_mask = self._pad_seq_right_to_n(\n",
    "                    seq=attention_mask,\n",
    "                    n=self.seq_max,\n",
    "                    pad_value=0\n",
    "                )\n",
    "                out[\"inputs\"] = torch.from_numpy(bold).to(torch.float)\n",
    "                out['t_rs'] = torch.from_numpy(t_rs).to(torch.float)\n",
    "                out[\"attention_mask\"] = torch.from_numpy(attention_mask).to(torch.long)\n",
    "                out['seq_on'] = seq_on\n",
    "                out['seq_len'] = seq_len\n",
    "\n",
    "            elif key in {\n",
    "                f\"{self.decoding_target}.pyd\",\n",
    "                self.decoding_target\n",
    "                }:\n",
    "                out[\"labels\"] = value\n",
    "\n",
    "            else:\n",
    "                out[key] = value\n",
    "        \n",
    "        if self.sample_keys is not None:\n",
    "            out = {\n",
    "                key: out[key] \n",
    "                for key in self.sample_keys\n",
    "                if key in out\n",
    "            }\n",
    "\n",
    "        if label is not None:\n",
    "            out['labels'] = label\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://github.com/athms/learning-from-brains/blob/master/data/upstream/ds000212/ds-ds000212_sub-{03..47}_task-dis_run-{1..6}.tar\"\n",
    "dataset = BaseBatcher().dataset(tarfiles=[url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "('((\"curl -f -s -L \\'https://github.com/athms/learning-from-brains/blob/master/data/upstream/ds000212/ds-ds000212_sub-{03..47}_task-dis_run-{1..6}.tar\\'\",), {\\'shell\\': True, \\'bufsize\\': 8192}): exit 22 (read) {}', <webdataset.gopen.Pipe object at 0x7f6a51eeeec0>, 'https://github.com/athms/learning-from-brains/blob/master/data/upstream/ds000212/ds-ds000212_sub-{03..47}_task-dis_run-{1..6}.tar')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset[\u001b[39m0\u001b[39;49m]\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mBaseDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m---> 39\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader)\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_keys \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         sample \u001b[39m=\u001b[39m {\n\u001b[1;32m     43\u001b[0m             key: sample[key] \n\u001b[1;32m     44\u001b[0m             \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_keys\n\u001b[1;32m     45\u001b[0m             \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m sample\n\u001b[1;32m     46\u001b[0m         }\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:34\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     33\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m         data\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_iter))\n\u001b[1;32m     35\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/pipeline.py:64\u001b[0m, in \u001b[0;36mDataPipeline.iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create an iterator through the entire dataset, using the given number of repetitions.\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepetitions):\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator1():\n\u001b[1;32m     65\u001b[0m         \u001b[39myield\u001b[39;00m sample\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/filters.py:208\u001b[0m, in \u001b[0;36m_shuffle\u001b[0;34m(data, bufsize, initial, rng, handler)\u001b[0m\n\u001b[1;32m    206\u001b[0m initial \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(initial, bufsize)\n\u001b[1;32m    207\u001b[0m buf \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 208\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data:\n\u001b[1;32m    209\u001b[0m     buf\u001b[39m.\u001b[39mappend(sample)\n\u001b[1;32m    210\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buf) \u001b[39m<\u001b[39m bufsize:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/filters.py:297\u001b[0m, in \u001b[0;36m_map\u001b[0;34m(data, f, handler)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_map\u001b[39m(data, f, handler\u001b[39m=\u001b[39mreraise_exception):\n\u001b[1;32m    296\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Map samples.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data:\n\u001b[1;32m    298\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m             result \u001b[39m=\u001b[39m f(sample)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/filters.py:297\u001b[0m, in \u001b[0;36m_map\u001b[0;34m(data, f, handler)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_map\u001b[39m(data, f, handler\u001b[39m=\u001b[39mreraise_exception):\n\u001b[1;32m    296\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Map samples.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data:\n\u001b[1;32m    298\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m             result \u001b[39m=\u001b[39m f(sample)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/filters.py:208\u001b[0m, in \u001b[0;36m_shuffle\u001b[0;34m(data, bufsize, initial, rng, handler)\u001b[0m\n\u001b[1;32m    206\u001b[0m initial \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(initial, bufsize)\n\u001b[1;32m    207\u001b[0m buf \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 208\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data:\n\u001b[1;32m    209\u001b[0m     buf\u001b[39m.\u001b[39mappend(sample)\n\u001b[1;32m    210\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buf) \u001b[39m<\u001b[39m bufsize:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/tariterators.py:218\u001b[0m, in \u001b[0;36mgroup_by_keys\u001b[0;34m(data, keys, lcase, suffixes, handler)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Group tarfile contents by keys and yield samples.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[1;32m    204\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39m    iterator over samples.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    217\u001b[0m current_sample \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m \u001b[39mfor\u001b[39;00m filesample \u001b[39min\u001b[39;00m data:\n\u001b[1;32m    219\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(filesample, \u001b[39mdict\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/tariterators.py:189\u001b[0m, in \u001b[0;36mtar_file_expander\u001b[0;34m(data, handler, select_files, rename_files)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exn:\n\u001b[1;32m    188\u001b[0m     exn\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m exn\u001b[39m.\u001b[39margs \u001b[39m+\u001b[39m (source\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m), source\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mif\u001b[39;00m handler(exn):\n\u001b[1;32m    190\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/filters.py:80\u001b[0m, in \u001b[0;36mreraise_exception\u001b[0;34m(exn)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreraise_exception\u001b[39m(exn):\n\u001b[1;32m     76\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Reraises the given exception; used as a handler.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[39m    :param exn: exception\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[39mraise\u001b[39;00m exn\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/tariterators.py:176\u001b[0m, in \u001b[0;36mtar_file_expander\u001b[0;34m(data, handler, select_files, rename_files)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(source, \u001b[39mdict\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m source\n\u001b[0;32m--> 176\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m tar_file_iterator(\n\u001b[1;32m    177\u001b[0m     source[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    178\u001b[0m     handler\u001b[39m=\u001b[39mhandler,\n\u001b[1;32m    179\u001b[0m     select_files\u001b[39m=\u001b[39mselect_files,\n\u001b[1;32m    180\u001b[0m     rename_files\u001b[39m=\u001b[39mrename_files,\n\u001b[1;32m    181\u001b[0m ):\n\u001b[1;32m    182\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    183\u001b[0m         \u001b[39misinstance\u001b[39m(sample, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sample \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mfname\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sample\n\u001b[1;32m    184\u001b[0m     )\n\u001b[1;32m    185\u001b[0m     sample[\u001b[39m\"\u001b[39m\u001b[39m__url__\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m url\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/tariterators.py:120\u001b[0m, in \u001b[0;36mtar_file_iterator\u001b[0;34m(fileobj, skip_meta, handler, select_files, rename_files)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtar_file_iterator\u001b[39m(\n\u001b[1;32m    103\u001b[0m     fileobj: tarfile\u001b[39m.\u001b[39mTarFile,\n\u001b[1;32m    104\u001b[0m     skip_meta: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__[^/]*__($|/)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     rename_files: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    108\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Dict[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[1;32m    109\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Iterate over tar file, yielding filename, content pairs for the given tar stream.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39m        a stream of samples.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     stream \u001b[39m=\u001b[39m tarfile\u001b[39m.\u001b[39;49mopen(fileobj\u001b[39m=\u001b[39;49mfileobj, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mr|*\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m tarinfo \u001b[39min\u001b[39;00m stream:\n\u001b[1;32m    122\u001b[0m         fname \u001b[39m=\u001b[39m tarinfo\u001b[39m.\u001b[39mname\n",
      "File \u001b[0;32m/usr/lib/python3.10/tarfile.py:1662\u001b[0m, in \u001b[0;36mTarFile.open\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[39mif\u001b[39;00m filemode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1660\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmode must be \u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1662\u001b[0m stream \u001b[39m=\u001b[39m _Stream(name, filemode, comptype, fileobj, bufsize)\n\u001b[1;32m   1663\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1664\u001b[0m     t \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(name, filemode, stream, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/lib/python3.10/tarfile.py:350\u001b[0m, in \u001b[0;36m_Stream.__init__\u001b[0;34m(self, name, mode, comptype, fileobj, bufsize)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extfileobj \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m comptype \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    348\u001b[0m     \u001b[39m# Enable transparent compression detection for the\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[39m# stream interface\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     fileobj \u001b[39m=\u001b[39m _StreamProxy(fileobj)\n\u001b[1;32m    351\u001b[0m     comptype \u001b[39m=\u001b[39m fileobj\u001b[39m.\u001b[39mgetcomptype()\n\u001b[1;32m    353\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname     \u001b[39m=\u001b[39m name \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/tarfile.py:577\u001b[0m, in \u001b[0;36m_StreamProxy.__init__\u001b[0;34m(self, fileobj)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, fileobj):\n\u001b[1;32m    576\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfileobj \u001b[39m=\u001b[39m fileobj\n\u001b[0;32m--> 577\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfileobj\u001b[39m.\u001b[39;49mread(BLOCKSIZE)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/gopen.py:88\u001b[0m, in \u001b[0;36mPipe.read\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrap stream.read and checks status.\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mread(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m---> 88\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_status()\n\u001b[1;32m     89\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/gopen.py:68\u001b[0m, in \u001b[0;36mPipe.check_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m status \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproc\u001b[39m.\u001b[39mpoll()\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m status \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait_for_child()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/webdataset/gopen.py:83\u001b[0m, in \u001b[0;36mPipe.wait_for_child\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m     79\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpipe exit [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mgetpid()\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproc\u001b[39m.\u001b[39mpid\u001b[39m}\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00minfo\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     80\u001b[0m         file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m     81\u001b[0m     )\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_status \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_errors:\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m}\u001b[39;00m\u001b[39m: exit \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus\u001b[39m}\u001b[39;00m\u001b[39m (read) \u001b[39m\u001b[39m{\u001b[39;00minfo\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: ('((\"curl -f -s -L \\'https://github.com/athms/learning-from-brains/blob/master/data/upstream/ds000212/ds-ds000212_sub-{03..47}_task-dis_run-{1..6}.tar\\'\",), {\\'shell\\': True, \\'bufsize\\': 8192}): exit 22 (read) {}', <webdataset.gopen.Pipe object at 0x7f6a51eeeec0>, 'https://github.com/athms/learning-from-brains/blob/master/data/upstream/ds000212/ds-ds000212_sub-{03..47}_task-dis_run-{1..6}.tar')"
     ]
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Inducing-human-like-biases-in-moral-reason-eYvEhHxD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
