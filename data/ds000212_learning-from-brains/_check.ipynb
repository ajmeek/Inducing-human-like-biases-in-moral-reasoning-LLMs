{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/art/.local/share/virtualenvs/Inducing-human-like-biases-in-moral-reason-eYvEhHxD/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import webdataset as wds   \n",
    "from typing import Dict, Tuple, Generator\n",
    "import numpy as np\n",
    "import torch\n",
    "from pprint import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _pad_seq_right_to_n(\n",
    "    seq: np.ndarray,\n",
    "    n: int,\n",
    "    pad_value: float = 0.\n",
    "    ) -> np.ndarray:\n",
    "    if n == seq.shape[0]:\n",
    "        return seq\n",
    "    return np.concatenate(\n",
    "        [\n",
    "            seq,\n",
    "            np.ones(\n",
    "                (\n",
    "                    n-seq.shape[0],\n",
    "                    *seq.shape[1:]\n",
    "                )\n",
    "            ) * pad_value,  \n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataloader,\n",
    "        length,\n",
    "        sample_keys\n",
    "        ) -> None:\n",
    "        self.name = 'BaseDataset'\n",
    "        self._length = length\n",
    "        self.dataloader = iter(dataloader)\n",
    "        self.sample_keys = sample_keys\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = next(self.dataloader)\n",
    "            \n",
    "        if self.sample_keys is not None:\n",
    "            sample = {\n",
    "                key: sample[key] \n",
    "                for key in self.sample_keys\n",
    "                if key in sample\n",
    "            }\n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "class BaseBatcher:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_random_seq: bool = True,\n",
    "        seq_min: int = 10,\n",
    "        seq_max: int = 50,\n",
    "        sample_keys: Tuple[str] = None,\n",
    "        decoding_target: str = None,\n",
    "        seed: int =  None,\n",
    "        bold_dummy_mode: bool = False,\n",
    "        **kwargs\n",
    "        ) -> None:\n",
    "        assert seq_min > 0, \"seq_min must be greater than 0\"\n",
    "        assert seq_min < seq_max, 'seq_min must be less than seq_max'\n",
    "        self.sample_random_seq = sample_random_seq\n",
    "        self.seq_min = seq_min\n",
    "        self.seq_max = seq_max\n",
    "        self.decoding_target = decoding_target\n",
    "        self.sample_keys = sample_keys\n",
    "        self.bold_dummy_mode = bold_dummy_mode\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "        \n",
    "    def _make_dataloader(\n",
    "        self,\n",
    "        files,\n",
    "        repeat: bool = True,\n",
    "        n_shuffle_shards: int = 1000,\n",
    "        n_shuffle_samples: int = 1000,\n",
    "        batch_size: int = 1,\n",
    "        num_workers: int = 0\n",
    "        ) -> Generator[Dict[str, torch.tensor], None, None]:\n",
    "        dataset = wds.WebDataset(files)\n",
    "\n",
    "        if n_shuffle_shards is not None:\n",
    "            dataset = dataset.shuffle(n_shuffle_shards)\n",
    "\n",
    "        dataset = dataset.decode(\"pil\").map(self.preprocess_sample)\n",
    "\n",
    "        if repeat:\n",
    "            dataset = dataset.repeat()\n",
    "        \n",
    "        if n_shuffle_samples is not None:\n",
    "            dataset = dataset.shuffle(n_shuffle_samples)\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    def dataset(\n",
    "        self,\n",
    "        tarfiles: list,\n",
    "        repeat: bool=True,\n",
    "        length: int = 400000,\n",
    "        n_shuffle_shards: int = 1000,\n",
    "        n_shuffle_samples: int = 1000,\n",
    "        num_workers: int = 0\n",
    "        ) -> torch.utils.data.Dataset:\n",
    "        \"\"\"Create Pytorch dataset that can be used for training.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            tarfiles: list\n",
    "                List of paths to data files (ie., fMRI runs) used for training.\n",
    "            repeat: bool\n",
    "                If True, repeat the dataset indefinitely.\n",
    "            length: int\n",
    "                Maximum number of samples to yield from the dataset.\n",
    "            n_shuffle_shards: int\n",
    "                Buffer for shuffling of tarfiles during training.\n",
    "            n_shuffle_samples: int\n",
    "                Buffer for shuffling of samples during training.\n",
    "            num_workers: int\n",
    "                Number of workers to use for data loading.\n",
    "\n",
    "        Returns:\n",
    "        -----\n",
    "            torch.utils.data.Dataset: Pytorch dataset.\n",
    "        \"\"\"\n",
    "        dataloader = self._make_dataloader(\n",
    "            files=tarfiles,\n",
    "            repeat=repeat,\n",
    "            n_shuffle_shards=n_shuffle_shards,\n",
    "            n_shuffle_samples=n_shuffle_samples,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "        return BaseDataset(\n",
    "            dataloader=dataloader,\n",
    "            length=length,\n",
    "            sample_keys=self.sample_keys\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def _pad_seq_right_to_n(\n",
    "        seq: np.ndarray,\n",
    "        n: int,\n",
    "        pad_value: float = 0\n",
    "        ) -> np.ndarray:\n",
    "        return _pad_seq_right_to_n(\n",
    "            seq=seq,\n",
    "            n=n,\n",
    "            pad_value=pad_value\n",
    "        )\n",
    "    \n",
    "    def _sample_seq_on_and_len(\n",
    "        self,\n",
    "        bold_len: int\n",
    "        ) -> Tuple[int, int]:\n",
    "        \n",
    "        seq_on = 0\n",
    "        if self.sample_random_seq and self.seq_min < bold_len:            \n",
    "            seq_min = min(int(self.seq_min), bold_len)\n",
    "            seq_max = min(int(self.seq_max), bold_len)\n",
    "\n",
    "            if seq_min < seq_max:\n",
    "                seq_len = np.random.randint(\n",
    "                    low=seq_min,\n",
    "                    high=seq_max,\n",
    "                    size=1\n",
    "                )[0]\n",
    "            \n",
    "            else:\n",
    "                seq_len = seq_max\n",
    "            \n",
    "            if seq_len < bold_len:\n",
    "                seq_on = np.random.choice(\n",
    "                    np.arange(\n",
    "                        0,\n",
    "                        bold_len-seq_len,\n",
    "                        seq_len\n",
    "                    ),\n",
    "                    size=1\n",
    "                )[0]\n",
    "        \n",
    "        elif not self.sample_random_seq and self.seq_max < bold_len:\n",
    "            seq_len = self.seq_max\n",
    "\n",
    "        else:\n",
    "            seq_len = bold_len\n",
    "\n",
    "        return seq_on, seq_len\n",
    "\n",
    "    def make_bold_dummy(\n",
    "        self,\n",
    "        bold_shape: Tuple[int, int],\n",
    "        t_r: float, # in seconds\n",
    "        f_s: Tuple[float]=None, # sine frequencies in seconds\n",
    "        ) -> np.ndarray:\n",
    "        f_s = np.array([4, 8, 12]) if f_s is None else np.array(f_s).flatten()\n",
    "        np.random.shuffle(f_s)\n",
    "        f = np.zeros((1, bold_shape[-1]))\n",
    "        for i, f_i in enumerate(f_s):\n",
    "            f[:,i::len(f_s)] = f_i\n",
    "        t_offsets = np.random.choice(\n",
    "            a=np.arange(0,3), # in TRs\n",
    "            size=bold_shape[1],\n",
    "            replace=True\n",
    "        )\n",
    "        t = np.concatenate(\n",
    "            [\n",
    "                np.arange(\n",
    "                    t_offsets[i],\n",
    "                    bold_shape[0]+t_offsets[i]\n",
    "                ).reshape(-1,1)\n",
    "                for i in range(bold_shape[1])\n",
    "            ],\n",
    "            axis=-1\n",
    "        ) * t_r\n",
    "        return np.sin((1. / f) * t)\n",
    "\n",
    "    def preprocess_sample(\n",
    "        self,\n",
    "        sample\n",
    "        ) -> Dict[str, torch.Tensor]:\n",
    "        out = dict(__key__=sample[\"__key__\"])\n",
    "        t_r = sample[\"t_r.pyd\"]\n",
    "\n",
    "        label = None\n",
    "        f_s = None\n",
    "        if self.bold_dummy_mode and self.decoding_target is not None:\n",
    "            label = np.random.choice([0, 1])\n",
    "            f_s = np.array([1, 2, 4]) if label == 0 else np.array([6, 8, 10])                \n",
    "\n",
    "        for key, value in sample.items():\n",
    "            if key == \"bold.pyd\":\n",
    "\n",
    "                bold = np.array(value).astype(float)\n",
    "                \n",
    "                if self.bold_dummy_mode:\n",
    "                    bold = self.make_bold_dummy(\n",
    "                        bold_shape=bold.shape,\n",
    "                        t_r=t_r,\n",
    "                        f_s=f_s\n",
    "                    )\n",
    "\n",
    "                seq_on, seq_len = self._sample_seq_on_and_len(bold_len=len(bold))\n",
    "                bold = bold[seq_on:seq_on+seq_len]\n",
    "                t_rs = np.arange(seq_len) * t_r\n",
    "                attention_mask = np.ones(seq_len)\n",
    "                bold = self._pad_seq_right_to_n(\n",
    "                    seq=bold,\n",
    "                    n=self.seq_max,\n",
    "                    pad_value=0\n",
    "                )\n",
    "                t_rs = self._pad_seq_right_to_n(\n",
    "                    seq=t_rs,\n",
    "                    n=self.seq_max,\n",
    "                    pad_value=0\n",
    "                )\n",
    "                attention_mask = self._pad_seq_right_to_n(\n",
    "                    seq=attention_mask,\n",
    "                    n=self.seq_max,\n",
    "                    pad_value=0\n",
    "                )\n",
    "                out[\"inputs\"] = torch.from_numpy(bold).to(torch.float)\n",
    "                out['t_rs'] = torch.from_numpy(t_rs).to(torch.float)\n",
    "                out[\"attention_mask\"] = torch.from_numpy(attention_mask).to(torch.long)\n",
    "                out['seq_on'] = seq_on\n",
    "                out['seq_len'] = seq_len\n",
    "\n",
    "            elif key in {\n",
    "                f\"{self.decoding_target}.pyd\",\n",
    "                self.decoding_target\n",
    "                }:\n",
    "                out[\"labels\"] = value\n",
    "\n",
    "            else:\n",
    "                out[key] = value\n",
    "        \n",
    "        if self.sample_keys is not None:\n",
    "            out = {\n",
    "                key: out[key] \n",
    "                for key in self.sample_keys\n",
    "                if key in out\n",
    "            }\n",
    "\n",
    "        if label is not None:\n",
    "            out['labels'] = label\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://github.com/athms/learning-from-brains/raw/b061ef97680365a074671e0c95581af426773f9e/data/upstream/ds000212/ds-ds000212_sub-[03-47]_task-dis_run-[1-6].tar\"\n",
    "dataset = BaseBatcher(seq_max=300).dataset(tarfiles=[url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = [dataset[i] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({128: 1,\n",
      "         162: 1,\n",
      "         87: 1,\n",
      "         125: 1,\n",
      "         139: 1,\n",
      "         61: 1,\n",
      "         76: 1,\n",
      "         147: 1,\n",
      "         161: 1,\n",
      "         48: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "pp(Counter(l['seq_len'].item() for l in loaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__key__', ['ds-ds000212_sub-03_task-dis_run-1']]\n",
      "['__url__',\n",
      " ['https://github.com/athms/learning-from-brains/raw/b061ef97680365a074671e0c95581af426773f9e/data/upstream/ds000212/ds-ds000212_sub-[03-47]_task-dis_run-[1-6].tar']]\n",
      "inputs torch.Size([1, 300, 1024]) torch.float32\n",
      "t_rs torch.Size([1, 300]) torch.float32\n",
      "attention_mask torch.Size([1, 300]) torch.int64\n",
      "seq_on tensor([0])\n",
      "seq_len tensor([128])\n",
      "t_r.pyd tensor([2.])\n",
      "['__key__', ['ds-ds000212_sub-03_task-dis_run-1']]\n",
      "['__url__',\n",
      " ['https://github.com/athms/learning-from-brains/raw/b061ef97680365a074671e0c95581af426773f9e/data/upstream/ds000212/ds-ds000212_sub-[03-47]_task-dis_run-[1-6].tar']]\n",
      "inputs torch.Size([1, 300, 1024]) torch.float32\n",
      "t_rs torch.Size([1, 300]) torch.float32\n",
      "attention_mask torch.Size([1, 300]) torch.int64\n",
      "seq_on tensor([0])\n",
      "seq_len tensor([162])\n",
      "t_r.pyd tensor([2.])\n",
      "['__key__', ['ds-ds000212_sub-03_task-dis_run-1']]\n",
      "['__url__',\n",
      " ['https://github.com/athms/learning-from-brains/raw/b061ef97680365a074671e0c95581af426773f9e/data/upstream/ds000212/ds-ds000212_sub-[03-47]_task-dis_run-[1-6].tar']]\n",
      "inputs torch.Size([1, 300, 1024]) torch.float32\n",
      "t_rs torch.Size([1, 300]) torch.float32\n",
      "attention_mask torch.Size([1, 300]) torch.int64\n",
      "seq_on tensor([0])\n",
      "seq_len tensor([87])\n",
      "t_r.pyd tensor([2.])\n",
      "['__key__', ['ds-ds000212_sub-03_task-dis_run-1']]\n",
      "['__url__',\n",
      " ['https://github.com/athms/learning-from-brains/raw/b061ef97680365a074671e0c95581af426773f9e/data/upstream/ds000212/ds-ds000212_sub-[03-47]_task-dis_run-[1-6].tar']]\n",
      "inputs torch.Size([1, 300, 1024]) torch.float32\n",
      "t_rs torch.Size([1, 300]) torch.float32\n",
      "attention_mask torch.Size([1, 300]) torch.int64\n",
      "seq_on tensor([0])\n",
      "seq_len tensor([125])\n",
      "t_r.pyd tensor([2.])\n",
      "['__key__', ['ds-ds000212_sub-03_task-dis_run-1']]\n",
      "['__url__',\n",
      " ['https://github.com/athms/learning-from-brains/raw/b061ef97680365a074671e0c95581af426773f9e/data/upstream/ds000212/ds-ds000212_sub-[03-47]_task-dis_run-[1-6].tar']]\n",
      "inputs torch.Size([1, 300, 1024]) torch.float32\n",
      "t_rs torch.Size([1, 300]) torch.float32\n",
      "attention_mask torch.Size([1, 300]) torch.int64\n",
      "seq_on tensor([0])\n",
      "seq_len tensor([139])\n",
      "t_r.pyd tensor([2.])\n",
      "['__key__', ['ds-ds000212_sub-03_task-dis_run-1']]\n",
      "['__url__',\n",
      " ['https://github.com/athms/learning-from-brains/raw/b061ef97680365a074671e0c95581af426773f9e/data/upstream/ds000212/ds-ds000212_sub-[03-47]_task-dis_run-[1-6].tar']]\n",
      "inputs torch.Size([1, 300, 1024]) torch.float32\n",
      "t_rs torch.Size([1, 300]) torch.float32\n",
      "attention_mask torch.Size([1, 300]) torch.int64\n",
      "seq_on tensor([61])\n",
      "seq_len tensor([61])\n",
      "t_r.pyd tensor([2.])\n",
      "['__key__', ['ds-ds000212_sub-03_task-dis_run-1']]\n",
      "['__url__',\n",
      " ['https://github.com/athms/learning-from-brains/raw/b061ef97680365a074671e0c95581af426773f9e/data/upstream/ds000212/ds-ds000212_sub-[03-47]_task-dis_run-[1-6].tar']]\n",
      "inputs torch.Size([1, 300, 1024]) torch.float32\n",
      "t_rs torch.Size([1, 300]) torch.float32\n",
      "attention_mask torch.Size([1, 300]) torch.int64\n",
      "seq_on tensor([76])\n",
      "seq_len tensor([76])\n",
      "t_r.pyd tensor([2.])\n",
      "['__key__', ['ds-ds000212_sub-03_task-dis_run-1']]\n",
      "['__url__',\n",
      " ['https://github.com/athms/learning-from-brains/raw/b061ef97680365a074671e0c95581af426773f9e/data/upstream/ds000212/ds-ds000212_sub-[03-47]_task-dis_run-[1-6].tar']]\n",
      "inputs torch.Size([1, 300, 1024]) torch.float32\n",
      "t_rs torch.Size([1, 300]) torch.float32\n",
      "attention_mask torch.Size([1, 300]) torch.int64\n",
      "seq_on tensor([0])\n",
      "seq_len tensor([147])\n",
      "t_r.pyd tensor([2.])\n",
      "['__key__', ['ds-ds000212_sub-03_task-dis_run-1']]\n",
      "['__url__',\n",
      " ['https://github.com/athms/learning-from-brains/raw/b061ef97680365a074671e0c95581af426773f9e/data/upstream/ds000212/ds-ds000212_sub-[03-47]_task-dis_run-[1-6].tar']]\n",
      "inputs torch.Size([1, 300, 1024]) torch.float32\n",
      "t_rs torch.Size([1, 300]) torch.float32\n",
      "attention_mask torch.Size([1, 300]) torch.int64\n",
      "seq_on tensor([0])\n",
      "seq_len tensor([161])\n",
      "t_r.pyd tensor([2.])\n",
      "['__key__', ['ds-ds000212_sub-03_task-dis_run-1']]\n",
      "['__url__',\n",
      " ['https://github.com/athms/learning-from-brains/raw/b061ef97680365a074671e0c95581af426773f9e/data/upstream/ds000212/ds-ds000212_sub-[03-47]_task-dis_run-[1-6].tar']]\n",
      "inputs torch.Size([1, 300, 1024]) torch.float32\n",
      "t_rs torch.Size([1, 300]) torch.float32\n",
      "attention_mask torch.Size([1, 300]) torch.int64\n",
      "seq_on tensor([96])\n",
      "seq_len tensor([48])\n",
      "t_r.pyd tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "for l in loaded:\n",
    "    for f in l:\n",
    "        if isinstance(l[f], torch.Tensor):\n",
    "            if len(l[f].shape) == 1:\n",
    "                print(f, l[f])\n",
    "            else:\n",
    "                print(f, l[f].shape, l[f].dtype)\n",
    "        else:\n",
    "            pp([f, l[f]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Inducing-human-like-biases-in-moral-reason-eYvEhHxD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
